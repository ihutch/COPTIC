SOR NOTES

Problems with constructing the MPI red/black boundary communicators
___________________________________________________________________

In general, the construction of the multidimensional odd/even
communicator can be thought of as follows:

odd(0)=this element
even(0)=null element
for dimension n=1,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

For n>0, the number of non-null elements is the same for odd and even.
But for n=0, the even element is of length 0 which is troublesome.

Therefore it might be better to start at level 1:

odd(1)=odd elements in this dimension
even(1)=even elements in this dimension
for dimension n=2,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

This construction must be done nd times, for each one, omitting the 
direction of the face normal nn from the dimension iteration. So really
it is

for dimension n=1,nd
	if(n.ne.nn) then
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
	endif
endfor

or maybe better:

for dimension nc=nn,nn+nd-2
	n=mod(nc,nd)+1
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

Because we are alternating types, we have to use
MPI_TYPE_CREATE_STRUCT, which is the only constructor that allows
different types in the creation.  In that case, we need arrays of
lengths,displacements,types. The length of these arrays is (1/2 times)
the block-side-length, which won't be known. We would either have to
choose a long length or allocate working storage.

Also, it is not clear that there exists a null element in MPI.

If we have to provide working storage, then maybe we ought just to
construct our own indexes and create the communicator directly by
indexing. Then we only have to provide an index array. But it might
have to be substantially bigger.

If we do the lamination directly, what does it consist of?

The even or odd type can be considered to consist of an array of
indexes, referring to the addresses of the values, plus a total length.
A lamination then consists of taking an odd type, taking an even type
shifted by appropriate amount, and concatenating them, then taking
a shifted odd concatenating that, ...

oddnm1=start of odd
evennm1=start of even
odd=start of new odd
do i=1,nsidei/2
   do j=1,nsidej/2
      index(jc)=index(j+odd)
      jc=jc+1
   enddo
   do j=1,nsidej/2
      index(jc)=index(j+even)
      jc=jc+1
   enddo
enddo
ditto for even

-------------------------------------------------------------------
7 Jan 06

Got the block-divided SOR to work, running on MPI processes (on same
machine). Then realized that the gather, to get the individual
solutions back into one big matrix is quite tricky.

One problem is that the order of the blocks generated by the MPI_CART
creation is row major, not column major like fortran, i.e. it goes
in the order 1 (0,0); 2 (0,1); 3 (0,2); 4 (1,0); 5 (1,1); ... etc.
Consequently, when you do a gather, the blocks come back not in the
order you want to store them in in fortran, which would be column
major. That makes it much more difficult to specify the gather, because
a gather puts the data from process-j, in the j-th storage position,
relative to the starting point. It would all be trivial if the blocks
were all the same size, and the order was right. 

It seems that this ought to be fixable by reversing the order of the
suffices. It would be too confusing to do this in the outer fortran
call and specification, I think. But presumably it could be done inside
the block-specification program. The problem can presumably be fixed
by reversing the order of dimensions in the calls to MPI_CARTs.
Yes that seems to work fixing all the MPI_CART calls (create, get, shift).

Now in principle one ought for equal sized blocks to be able to do a
simple gather. There are still major problems with unequal sized blocks.
I suppose that one can construct a generalized call that will interleave
the larger blocks. The iorig pointers to block start are now in the order
of mycartid. This enables iorig-1 to be used for the displacement array.
If all blockstypes were equal we could do
call MPI_[ALL]GATHERV(u,1,blocktype,u,[1],[iorig-1],blocktype,
	icommcart,ierr)
but the problem is that [iorig-1] means an array with values equal
to iorig(j)-1, because of the zero-based C offsets used in the MPI
routines, compared with the 1-based fortran pointers. It would be
possible to change my convention to zero-based. Then a few modifications
elsewhere would be necessary.

Alternatively, one could presumably not bother with all this reordering
of the mycartid, and instead manually reorder the blocks for the
purposes of calls. Do this by creating an array that translates 
mycartid into fortran order. In other words ifortorder(mycartid) =
fortran column major ordered indices, then iorig(ifortorder(j))-1
is the displacement of the jth block. 
	do j=1,nproc
	   idisp(j)=iorig(ifortorder(j))-1
	   irecvcounts(j)=1
	enddo
call MPI_[ALL]GATHERV(u,1,blocktype,u,irecvcounts,idisp,blocktype,
	icommcart,ierr)
Then none of the fiddling around with dimension reversal ought to be 
necessary. There might be a slight memory access hit because the blocks
will come in a strange order. For that reason, the dimension reversal
might still be worth it. 

But wait, it says that the equivalent receive is 
MPI_Recv(recvbuf+disp[i].extent(recvtype),recvtype,i...)
whereas I am taking the displacement to be in units of the underlying
REAL, not the extent of the structure: A problem.

On p207 there seems to be a thing called MPI_UB which is the equivalent
of a null send, but takes up space in the MPI Type. I guess this is 
within a struct. Apparently this is for setting the Upper Bound of a
structure, and there is a type MPI_LB for lower bound. They are not
listed in the type map but affect the value of extent. Apparently
"MPI_TYPE_STRUCT is the only constructor that allows MPI pseudo types
MPI_UB and LB"

-----------------------------------------------------------------------
16 Jan 06
Now have a 2d specific sor2dmpi and a general-dimension sormpi version
going that includes the MPI communications. The general dimension
version is not really fully tested at this stage. Also its Jacobi
radius has a rather different optimum for convergence.

After a day of work we seem to have a multidimensional version of sormpi
working. 

There is a bug somewhere because although different block divisions
of the first two dimensions give identical results, different block
numbers in the third dimension give subtly different results. This 
should not happen.

18 Jan 06
Found the bug, it was in the choice of parity when using the blocks
with the boundary excluded. The parity must be changed by ndims.


_______________________________________________________________________

Turning SORMPI into a solver for embedded boundaries.
_______________________________________________________________________

29 Dec 06

Implemented a test routine comparing solution with smt.out. This is
to discover any errors that I might accidentally introduce. smt.fixed is the
output with which to compare. Comes from the charge-ball case. 

Plan is to introduce boundary handling into this solver code.
For memory access optimization, the plan is to make the pointer to 
the boundary information part of cij, namely the element 2*ndims+1. 
For this I need to fix some addressing in the routines, but not very much.

Added the extra cij slot as cij(2*ndims+1,i,j,...). Works. 
Document minimally. 

Passed the object data obj(idata,jpointer) through to sorelaxgen, which
is where it will be needed for the treatment. Also passed the length
nobj of the obj data (leading dimension). Demonstrated calling it.

30 Dec 06

Now we need a way to implement the objects. I have decided to separate
the object implementation from the rest of the code. That way the instability
of geometric object representation is separated. One can use whatever 
representation one likes provided one has a way to translate it into the
object data that we are going to use. However, we need an object-data
api or specification for how the object data is to be represented. This
needs to be fairly compact, and also not to require lots of processing
during the iterations. There are questions about different numbers of 
dimensions. 

If we make the handling of object data through a function or subroutine
call, we can make the api a matter for the user to specify. However, if
it is too general, then lots of things have to be passed. That would
be cumbersome. It is probably best to make sure that the cij do really 
contain all of the weight from adjacent points that we wish to put into
the numerator. Then the effect of the object code will be to add some
constant quantities. It seems as if this is all it really needs to be able
to do. In fact the main thing appears to be to have the quantity (bdy in
ES2D) to add to numerator, and maybe diag, since this is not now reliably
calculated by adding cijs. For the purposes of potential solving, this may
be all one needs for any given point. 2 quantities. For field interpolation
other information is almost certainly needed.

1 Jan 07

Slow progress. Implemented a cijroutine that is called by mditerate to
set the cij. The cijset.f was an explicit iteration, that I don't really
need since I use mditerate.

Fiddled with getting it to avoid the boundaries properly. Now ok.

Set up series of smt.out to do checking with. smt.1 is the output
from the original, Jan 2006 verified version. smtround.1 is an alternative
output with reduced significant figures. The idea is that if we
get things nearly right, they will match.
i
smt.2 and smtround.2 are the versions that implement the cij in terms
of dxs. They differ by rounding arising from the redefinition. The
difference between smtround.1 .2 are all in the final place. Still 
some differences at the 10^-4 level. Move the test to smt.2 so we don't
have to retain the nint that prevents rounding errors.

Implemented much of the infrastructure for handling the potential
intersections. What remains is to construct potlsect which returns
for any point, dimension, and direction the fraction and potential
value for any intersection between it and its neighbour. It returns
fraction 1 if there's no intersection.

There's a trap with the offsetting of indi that needs to be fixed.

2 Jan 07

Fixed the trap. Rationalized the routines. Wrote a plotting routine
to display the results of the cij setting. There's a problem with that
to some extent that myid is not known prior to the sormpi call, so all
the nodes do plotting, unless you do it after calling. If you do it before,
then all nodes call. One approach is to just call ./sormpitest then. 
That crashes after the plot as it tries to sormpi. Never mind.

Test obj setting seems to work for one point.

Got a sphere apparently working in cij and its plotting, but potential
solution is crazy. Saved its output as smt.b1 for now. 

That problem was that I was not doing the obj treatment in the solver.
Implemented it. Seems to work. 

Switched to pure Poisson (not shielded Poisson) solver. Looks good.
Have to be careful with the external declaration.
Turned off charge density for the test saved as smtsphere.1. Had to turn
off the hand-set object data too. Done.

We seem to have a working solver. Ran a 80x80x36 case in about 10s
on laptop. Took 321 iterations.

3 Jan 07

I realize that there might be good reason to include additional data
into obj. A couple of ideas: 

1. the object number(s) that the intersections correspond to. This
would be useful for calculating the total flux out of a given
object. However, it would have to be specified for each of the
directions. So that would be a very cumbersome number of additional
elements, almost doubling the storage, if done directly. Needs thought.

2. the address of the cij (or ijk pointer) of the mesh node that this
object belongs to. This would enable one to scan the obj data, and refer
back to the mesh, rather than having to scan the mesh and refer to the 
obj data. 

To store the latter in a real is somewhat problematic, since the mesh 
might become larger than the integer available. Probably one way to 
solve that is to use an equivalence of obj to an integer.

Another issue is the namespace that obj is in needs to be observed now
that it is in common. Start by fixing that. Done

Separated the specific objects into 3dobjects.f
Took out the save from circleset and created smtsphere.2 to match.

Object information: probably the best way to handle this is NOT to put
the additional information we might want to retain into the sor_obj
data. Instead we can optionally create and populate additional object
data structures that parallel the sor_obj and give the information.
This would be implemented in routine potlsect, where the intersections
and fractions are calculated. There's a slight problem in that isor_oi
is incremented only after potlsect has returned, so syncronizing the 
additional data with isor_oi is a little tricky. One way to syncronize
would be to pass iset to potsect, telling it thereby whether we have
already incremented the isor_oi or not. Then one just writes the additional
data into isor_oi-1+iset. If we are storing the pointer to cij, we
may also have to pass that to potsect. The parallel information could
be integer.

11 Jan 07
Implementing general boundary conditions. 
Converted the fixed boundary to general form. Some thoughts about
how to pass the a,b,c. 

Also need to pass back more information for the continuity condition.
Do that by changing from passing in dpm the fractional distance. Instead
pass the total distance, so that dp = dpm(i)*fraction(i) etc.
Unfortunately this changes the rounding. So store this result as
smtsphere.3

The general condition does not seem to be working correctly. There's a 
problem with the inactive side, it seems. Need better diagnostics.
Implemented surfmark. This seems to show that the mismatch is really 
occurring at the surface, not on adjacent points. It also shows the directions
of the connections are not what one might intuit from the surface plot.
So you have to be careful.

13 Jan 07

Finally found my problem. It was that I was storing Cd in the object instead
of the DIFFERENCE Cd-Cij, which is what I should have been storing. 

Adjusted the namespace control by changing to a suffix rather than a prefix
_sor. This enables variables to determine their type in the usual way.
objcom.f fixed. ctl_sor also fixed.

Change to using a proper 0-1 range for the variables. Replace smt.out
to smtsphere.4 smtround.3.

Doing numerical checks, I find that the logarithmic derivative setting
is not giving correct solutions. I find that adjusting the coefficients
inside cijroutine can correct. It seems that the problem is with my
implementation of the form for the general BC. I make d+ equal to 
apb+b/apb times dxp1, but then for the opposite direction, I have not used
d- equal to this same value. I have used just dxp1. That is incorrect.

Rebuilt the cij routine using the proper values for dminus (and the full
continuity expression). Gives correct answers 40x40x16 has maximum
error for the 1/r case of .0032 and SD .001. 
For 40x40x40
 Max error=  0.00121030153  Standard Deviation=  0.000311983633
This seems to be second order small.
For 80x80x80 the oi_sor overflowed. Doubling it solves. then
 Max error= -0.000307714043  Standard Deviation=  9.30291717E-05
Reduced by faster than dx. Not quite factor of 4 in SD, but yes in max.
Looks as if we are getting second order accuracy.
For 20x20x20
 Max error= -0.00420894753  Standard Deviation=  0.00136070442
Definitely looks like dx^2.
For 10x10x10
 Max error= -0.0172496308  Standard Deviation=  0.00511154439
For 100x100x100
 Max error= -0.000233839804  Standard Deviation=  7.77521054E-05
This is getting closer to the eps convergence:
 mi_sor,k_sor,xjac_sor,del_sor 1210 401  0.999440014  1.9293886E-05 0
Wrote plotconv to plot the convergence of these cases. Very clearly
quadratic except for the 100 case where we seem to be getting less than
quadratic reduction, which is not surprising since we are not necessarily
converging the iterations to significantly better accuracy.

Running quantitative test case again finds rounding differences, but
zero rounded differences. So consolidate in smtsphere.5, smtround.5

Summary: We have the scheme working and tested with both fixed potential
and logarithmic gradient boundary conditions. 

To do: media. Rethinking stored data (for charge assignment, object
identification, reverse reference to cij, etc).

Longer term ideas: curvilinear coordinates.

What to store in the object data? Prior idea was fraction, potential. 
This works for a fixed potential boundary and gives enough information
to generate an interpolated position. With other types of boundary it
is not so clear what one should store. 

18 Jan 07

Begun adjustment of the object data. First generalize the code so
that ndata_sor is a parameter that describes the amount of data per
direction. Ensure that it works for ndata_sor=3, even if we are only
using the prior two spaces. Of course, at present the data is only 
being used by the plotting routines for a graphic display.

Then I can add 1 to the start of each of the object data descriptor
pointers:
      parameter (ndims_sor=3,ndata_sor=3)
      parameter (nobj_sor=1+2*ndims_sor*ndata_sor+3)
      parameter (idgs_sor=1+2*ndims_sor*ndata_sor+1)
      parameter (ibdy_sor=1+2*ndims_sor*ndata_sor+2)
      parameter (iflag_sor=1+2*ndims_sor*ndata_sor+3)

Need to correct things all the way through sormpi.f. Probably it is better
to remove the process of passing the object data through and rely on 
external calls. So implement ddn_sor(ip,dden,dnum) that adjusts the numerator
and denominator. Unfortunately this gives rounding error changes.
Make smt.6

Remove the passing of nobj, and obj from the sormpi and sorrelaxgen calls.
This is now entirely in the common and its effects within the sormpi call
are purely the dden and dnum adjustment via ddn_sor(ip,dden,dnum). There is
negligible hit on the speed.

27 Jan 07

Planning and development of box handling for incorporation of surface
descriptions through box analysis.

boxedge iterates over the edges of a box in levels going out from the
	base node 000... 

gaussij is a modification of the numerical recipes gauss-jordan elimination
	to make it robust to singular matrices. 

Before incorporating these into the sor, it is probably best to implement
the c/a, b/a aspects of the code. Did that.

Now got a call to boxedge going after the cijs have been set. It seems
to be giving sensible results. For example the replacement of intersections
shows that when replacement of a real intersection takes place (more often
than is nice) it is because of rounding in late digits. 

Developed wireframe plotting diagnostics in cijplot to tell if I am
doing the right thing. Plotted the true intersections. They look fine,
although there are box recuts that I don't understand being
tried. Perhaps that is a sign that I need a better points-chosing
scheme.  This does not validate the rest of the fractions,
though. Need some inspiration on how to verify the other fractions in
a graphic way.

31 Jan 07

Changed additional fraction calculations to use all the points on levels 
up to that being examined, and svdsol to fit a plane to them. This gives
results the same within rounding as the gaussij when the number of points
is 3, and similar results otherwise. 

There is still an issue about recutting the cell. 
I still do not seem to have a clear verification that I have got the
additional fractions correct. 

An idea. The plane equation is \sum fn_i^-1 x_i = \sum a_i x_i= 1. 

For a sphere this ought to be a tangential plane (approximately). We
could test whether it is by evaluating the distance of the sphere
center from the plane, which is (\sum a_i.xc_i -1)/|a|, and seeing if
this is equal to the sphere radius.
 
Note that any box that has one of its fractions unset (=1.) should be
considered to have no plane crossing it. Thus each unset fraction 
rules out all boxes in that direction. If a node has three unset fractions
then there is only one of its boxes that is cut. There are some of these
among the Added. But there are others that have more than 3 fractions set.

3 Feb 07

There seem to be persistent erroneous boxes among the vast majority of 
correct boxes. I am pursuing the idea that these arise because of the
overloading of the fraction settings. If all three of the fractions
for a particular box happen to get set by the adjacent boxes, then this
box will erroneously be intepreted as having a plane intersecting it.

The only way out of this that I see is to use the flags to indicate whether
a box is in fact cut by a plane or not. There are 2**dims possible boxes.
Thus if we use all of the 32 bits of a 4-byte word, we can cope with 
2**5, i.e. 5-dimensions. 3-D requires only 8 bits. The intrinsic 
btest(n,ipos) yields true if bit pos of integer n is set. This extension
is in the mil-spec and available on practically all compilers. There is 
also an intrinsic ibset(n,ipos) and ibclr(n,ipos) (I think I got those right)
Their availability is less certain. They are not mentioned in current
gfortran documentation. 

After substantial additional work, I found that the main problem was that
the handling was in the wrong place in boxedge and hence incorrect.
Moving it to the proper end of level gives good results. 
If we use the test that we don't account for planes that fail to intersect
the cell, we get only about 40 additional pointers for the correct planes.
We get no recuts. We get no errors above .01. Also we find that only 
6-intersection cases are found. This is the effect of the cut criterion.
If we remove the cut criterion, we get many more additional points which
have 3,4, or 5 intersections.

I think we have the thing working now. Flags signal whether a box is
relevant or not. They are set only according to the boxedge call.

The next thing to do is to implement the field (gradient) interpolation.

6 Feb 07

Further cases with different radii shows there are still some problems
to sort out. I had a test that replaced the fraction only if b/a,c/a
are zero. If you have a zero potential setting. That is the case even
for previously found fractions. Then problems arise. Therefore, I ought
to reset the fraction in the box code only if it has not been already
set, i.e. not adjust it smaller in succeeding calls, as had been the 
previous algorithm. I think that algorithm was supposed to handle
cases where it had been set by previous box calls (not direct fractions). 
Now choose only to permit fraction setting if fraction =1., i.e. it has
not previously been set at all.


2 Jun 07
Converted the object code to depend on reading an input file in which
different types of object can be specified. Also coded the ability
to detect whether we are inside (or outside) such objects.

9 June 07
Got the gradient interpolation working with correction of the node about
which interpolation is done to use the fraction information to determine
when we need to use something other than the nearest node. 
Packaged this into gradinterpcorrect. So the call is packaged.

15 Jun 07

Added ipointer to the idob_sor data: a reverse pointer back to the
u/c arrays. Created routine:  indexexpand(ndims,ifull,index,ix)
to obtain the multi-D index ix, from the pointer index.
Then added routine iregioninit(ndims,ifull) to initialize the 
iregion flags for the idob_sor to the insideall value telling what
region they are in. It cycles over all the object data that is
created by the cijroutine, uses indexexpand.

Thus we now have ipoint_sor and iregion_sor set to sensible values
after calling cijroutine and iregioninit. Then at any interpolation,
we can access the region of any point that has object-boundary data.
A point that does not have such data is not adjacent to any boundary.
Therefore, it is safe to assume that it is in the same region as
a nearby point that we might wish to compare with. 

20 Jun 07
Attempting to compare the gradinterpregion call with the other, we find
that the iregion values are apparently incorrect. Misaligned by 1 in
all dimensions. This appears to be because when the cijroutine is called,
it is called by mditerate, and this is relative to the origin
(2,2,2) in the 3-d cij array. Consequently the reverse pointer is 
set incorrectly. Or, to put it another way, the reverse pointer is
relative to the (2,2,2) origin. Fix this in the reverse lookup during
the regioninit. Now gradinterpregion gives identical results.

22 Jun 07
Got getfield working. It gives the same value as the raw calls. Also
seems to give plausible looking interpolations.

23 Jun07
Constructed fieldtest which plots a field profile along a radius of the
sphere and a chosen angle. It shows there's a bug in getfield. 
Made get3simple to do totally simple box interpolation with no 
gradient extrapolation and neglect of boundaries. 
Found the bug in getfield logic. Incorrectly handling the construction
of the inputs to boxinterp for general dimensions. Temporarily use
the 3-D version from get3simple for iteration. That works and now things
agree away from the boundaries.

16 July 07

Getting back to this. The fieldtest code is close to working, but it
currently does not do interpolation correctly, because it produces
cases where iflags(1)=0, contradicting the assumptions of the
box2interp version. 

Extended box2interp to cope with f00 absent. Then tests show that the 
errors are reduced by a factor of 2 in the prior worst cases. So this
really helps (getting the interpolation right). 

18 July 07

Implemented an extrapolation scheme that is used for a box in which
more than two nodes are absent. If an absent node has precisely one
(of two) neighbors present then extrapolate through the present node
using the value one step further away to the desired (absent) node.
Otherwise do nothing because if only one node is absent (i.e. a node
has two present neighbors) we are already doing the sensible extrapolation.

This code has the effect of reducing errors in E-field by about a
factor of 2 when it kicks in. But there are fairly significant errors
in which it does not. Generally we find now that the field error is
less than about 10% for the 16x16x20 mesh near the sphere
boundary. Since the boundary is at r=0.2, which defines the
scale-length of the phi-solution there, and the node spacing is 1/16 or 1/20.
which is 0.05 (optimistically), the ratio of node spacing to characteristic 
scale-length is 0.25. Therefore if we get 10% errors we aren't doing
badly. Going to twice the mesh numbers (half the spacing) the errors drop to 
about 2-3% max. Which seems to be falling like (s/l)^2 (or at least faster
than linearly). There was a strange result with 
 fieldtest -p -p1 -t2.4
some kind of error inside the sphere but not too bad.

Looks like the difference between using extrapolation or not is still only
about a factor of 2 at smaller s. The extrapolation does not change the 
order of interpolation (as expected). That order is presumably that we
are correct to first order, since we are doing linear E-interpolation.
The error being order (s/l)^2 thus makes sense.


23 Aug 07

Started padvnc.f for advancing.
Updated getfield to use general number of dimensions.
Removed extrapolation from it, but saved old version.
Removed passing of iLsc, just multiply iLs by (2*ndims+1).


24 Aug 07

Further cleaning and confirmation that padvnc is working.
Changed getfield so that it is permissible to pass the full mesh
position, not just the fractional mesh position. This gives a version
that can either be used by passing the local origin of arrays, or
be used with the global origin and full mesh position.

There is a puzzle about the following. It seem to be using incorrect 
offset (ought to be (ix-1)*iLs ) but it works as is.

      do id=1,ndims_mesh
c Offset to start of dimension-id-position array.
         ioff=ixnp(id)
c xn is the position array for each dimension arranged linearly.
c Find the index of xprime in the array xn:
         ix=interp(xn(ioff+1),ixnp(id+1)-ioff,x_part(id,i),xm)
         xfrac(id)=xm-ix
         x_part(ndimsx2+id,i)=xm
         ixp(id)=ix
         iu=iu+ix*iLs(id)
      enddo

Interp returns a value >=1. But if we have ix=1, that is the first 
value in the array, so the offset should be zero and iu zero.
In the interpolate.f file (ix-1) is used, correctly.
In padvnc.f it is not. The getfield calculation in fieldtest appears
to treat things correctly, because it uses u(ix,ix,ix) explicitly.

I think this is bound up with the fact that I've assumed that we can
treat the shifted/fractional and full position cases the same, but
perhaps we can't since the fractional treatment passes 0.5 but the
full position value at 0.5 is 1.5 reading from a lowest index of 1.

Actually I think the previous result was wrong and the present one is
right. Played around with the padvnc printing out the force and comparing
with analytic. The corrected results are within about 2%, but the orbit
closure stinks.

Yes this is the correct alignment. The orbit closure improves lots
as one goes to finer mesh. This agrees quite well with analytic approx.
Fixed some little problems. I think we can declare padvnc working 
using (ix-1) everywhere.

25 Aug 07

Completed the initial chargetomesh assignment code.

Thinking about how to handle changes to the boundary potential between
steps, which would be needed for floating cases. Introduced a new
iinter flag in dob_sor to indicate which object was intersected. (Not 
yet populated.)

There seem to be three levels of cij update that would make sense, in
increasing level of computational cost.

1. Directly scale the C/A and ibdy components by the changed potential
for a specific object whose potential is varying. Assumes that objects
are not moving and that the changing BC is fixed potential. And that
each node that intersects the changing object intersects no others.
Involves searching the existing dob_sor data.

2. Rerun cijroutine for those nodes which intersect a changed object.
Assumes objects not moving, but would work for arbitrary changes to 
BC(s). Just operate on the existing dob_sor data.

3. Do 2, but in addition, search nodes in the neighborhood of a moving
object to determine nodes that become boundary which weren't before
(and presumably those that stop being boundary but were before). 
This requires substantially more effort to be sure you don't miss new
nodes. But is still short of re-searching the entire mesh, which would
be very costly.

A further possibility would be to limit the intersection investigation to
only the object(s) that have changed. This might be a significant time
saving. In other words, ignore the other objects in calculating intersections.

Implemented writing of the object number into iinter_sor. Test is
that we use cijplot to plot wireframe with color equal to the number.
Works.

27 Aug 07

Looking into MPI id's and communication, with a view being able 
conveniently to call the setup before starting the sormpi solution.
This enables me to know what my process is etc.
There are currently two separate initializations. 

1. bbdydefine just sets the iorig vector which tells the block sizes
for the specified arrays and process arrangement

2. first call to bbdy sets up the communications and initializes MPI.

It hardly seems necessary to have these separate calls.
There's an issue about how iorig is saved. Currently it is only
saved during the operation of sormpi (lost on return). 
Certainly there's no need to do what is currently done which is
to call bbdydefine anew each time sormpi is called. It really ought
only to be needed the first time.

There is a general save in bbdy which is where most things are saved
(although apparently not iorig because it's an argument). I see no
real reason why iorig should not be saved. Indeed there does not seem
to be any reason why it normally should be accessed outside of 
bbdy, so it might simply be defined in a common (in case access is needed)
within bbdy, and not passed to it. Then bbdydefine would be called by
bbdy itself, not separately. If we keep the arguments to bbdy the same,
then iorig has to be saved in sormpi. That's a problem because if we
call bbdy outside of sormpi, and then think we've done setup, it won't
then be given to sormpi for subsequent storage, unless we reinitialize,
which would be a bad thing.

Better to delete iorig from the bbdy arguments. Places:
mpibbdy.f mpibbdytest.f sormpi.f bbdydecl.f
However, we also then need to add ifull to the bbdy arguments
otherwise the initialization doesn't know what they are.

Done all that. mpibbdytest works (2 processes). fieldtest works.
Fieldtest had no modifications to it to handle this change. 
The whole interface is unchanged in respect of anything outside sormpi.

Added the code for special case kc=-2 purely (re)initialization.
A prior call with kc=-2 will set up the communicator but not try
to actually do any communication.

This all works, but there's still a problem with calling the bbdy
directly, that is that all its (other) arguments don't end up in the
sormpi places, which is where they are needed. Probably the best
thing is to make the bbdy call through sormpi somehow. The integer
switch ictl is the right place to do this. 

Implemented that using the 3rd bit of ictl.

Found a big problem with the initial call causing sorrelaxgen errors
and segfaults. Traced one issue to new usage of iLs which is not 
correct now because in bbdy bbdydecl of integer iLs(ndims+1) has ndims
as a parameter but iLs not passed. That's ok in bbdytest, but not in
sormpi. The error is not detected by the compiler and then shows
up later only when the save is required. In general there's an issue
with using bbdydecl in sormpi because a number of the bbdydecl parameters
are not passed in to sormpi. Therefore they are really local definitions
and can't be sized by anything other than a parameter. 

Fixed that for now with a hack to make it a parameter. Annoying compiler
bug!

28 Aug 07

Initialization of 32x32x40 now takes about 4 seconds.  When thinking
about the initialization, and possible reinitialization if there are
boundary changes, it is clear that for a multiprocessing environment,
we ought to make each processor do its own cij initialization.  This
potentially makes a big time saving.

However, the way things are currently set up, the cartesian layout
is hidden from the main routine, but the main routine calls mditerate
of cijroutine. Thus the information is not available to divide the 
work between different processors.

There's another issue which is that cijroutine uses an indexed array
of object data. As it generates the array, the presumption is that
there is a 1-1 correspondence. If the cijroutine work were divided
between processors, then the would be a 1-many correspondence. The
object data corresponding to a particular pointer would refer to
different mesh- locations in different processors. That would have a
possible benefit in that the object data storage would be
distributed. But it would cause a problem in that we could not simply
gather the data together.  That is a MAJOR problem because the
interpolation routines currently assume that there's a cij with
corresponding pointers covering the whole of the volume. It would
perhaps be possible to segment the object data by offsetting the
pointer for each processor so as to keep the data separate. Then we
could gather the cij back to central location.  Unless we did this, I
shudder to think how we'd proceed. The cost is the segmentation of the 
object data storage, which might make it less efficient. The benefit
is presumably speed of recalculating cij.

Basically we need access to the iorig information in order to do much
with other processors, such as mditerate or iregioninit. Actually that
can be accessed through iorigcom. We might also need information about
our place in the cartesian communicator; this would be in the form of
iobindex or myorig. 

Looking into the partreduce and other mpi code. There's some awkward 
shuffling in sceptic. This is not necessary because using the argument
MPI_IN_PLACE instead of the send buffer causes in-place reduction, 
which avoids all the problems of shuffling. Much more elegant.

Implemented a basic mditerarg of psumtoq to calculate the charge
density. But we still need a way to calculate rhoinfinity which is passed.

Put a new solve of sormpi in the stepping loop. It shows a blip of
charge giving rise to a potential peak. (And also causes the particle
to veer off in a different direction. Probably that is some sort of
image-charge effect). 

On a single processor, a 32x32x40 solve of about 100 iterations takes ~1sec.
Not bad, although the resolution is not that great yet.

Working on horace. Found several compatibility problems. (gfortran)
1. Makefile does not use implied patterns correctly. Seems to be something
   I don't understand about the match-anything rule.
2. I made incorrect assumption about o implying integer. Gfortran detects
   that error and one with pwr2nd, because of checking for real arguments.
   Fixed.

Succeeded in making. Got some complaints about blanket saves. Seems to be
caused by saves in mpif.h.
Also got diagnostics about unused argument variables.

29 Aug 07

Corrected the sign of field to make attraction of the particle to image
charges correct. Things seem fine with last night's changes.
Code will compile and run on unity. There it is back to g77 not gfortran,
I think.

Reinjection. We need to have some sort of approach on this. There needs
to be some reinjection boundary identified, and some way to determine
where on that boundary and with what velocity the particles are going
to be reinjected. 
A spherical boundary is basically the sceptic situation.
A rectangular boundary might also be of interest.
Probably these should simply be provided as subroutines.

Incorporated a somewhat modified version of the old reinject.f routine
from sceptic. Sphere. Seems to work but not quantitatively
verified. The advancing by a partial step is to be incorporated into
the padvnc code. 

Next thing: Loading particles. I'm not wonderfully happy with sceptic
on this. But I don't know if there's a better way. 

30 Aug 07
Implemented pinit. But there's some kind of bug that causes it to hang
in some situations. That was incorrect iregion_part value.

31 Aug 07
Idea about different particle species. We could use if_part to identify
different species 0: no particle 1: species 1, ... They padvnc could be
adjusted to advance each species differently, and reinjection could likewise
reinject different species. This would make it essentially trivial to
generalize the code to make it PIC electrons (for example) or multiple
ion species.

Branched to ccpic as main. Then began packaging the diagnostics so
that we can clean up the code, and use consistent mesh sizes etc.
I realize we need to worry about the wrong field we are getting at the
edge. This is an error that needs to be understood and fixed.

1 Sep 07
Trying to fix the edge incorrect field. First, try to set the iregion
flags in all the edge object data. This sort of works, but I find that
there's a problem with the fact that the reverse pointer is in various
places assumed to be addressed relative to (1,2,2,2). This was because
of the problems with mditerate. 

Therefore change the way that mditerate is called with cijroutine.
This needs us to fix the passing of the ipin to mditerate to do the 
right thing, and to calculate the right indi and save the indinp
in mditerate.

Then there are two places where the 2,2,2 assumption must be removed.
One is in 3dobjects.f iregioninit, called by objstart.
The other is in cijroutine itself. Both fixed.

Now the edge iregion is initialized with iregion as well as the
intersections.  This improves the field diagnostic plot a great deal,
leaving only a tiny region actually outside the outer object where the
field is incorrect. I still don't quite understand why that's there.
I guess there's no reason it shouldn't be, since the presumption is that
getfield returns only the field corresponding to the iregion specified.
Actually, no, I understand this, inside solu3plot the iregion is set
to be the insideall of the actual position being plotted (and without
that one gets rubbish). So we are telling getfield that we want the 
value as if one were in the region in question. Improved solu3plot
to show the region.

Trying out non-uniform mesh. There are big problems. The autocolorcont
is now incorrect. But more important, the getfield is plainly incorrect
and it looks as if there might be a bug in getting the spacing. At least
the solution looks plausible!

3 Sep 07

Fixed the contouring.
Fixed getfield. It was indexing the position array slightly wrong.
Now things seem to be correct. However, I think a numerically 
analytic orbit ought to be examined to prove that we are getting the
correct orbit.

4 Sep 07

Did an installation of an exact circular orbit. Seems to give rather
good accuracy after 100 steps with dt from .1 to 1. (0.7 to 7
turns). In the vicinity of 1-2 % accuracy in the r over that
evolution. About 5% deviations with 400 steps (28 revolutions), but
still only 1-2% for 400 steps 3 revolutions (dt=.1). Thus there's some
effective collisionality but it is not terribly great. Allows many tens
of revolutions. This all with 32x32x40 mesh r=1-5.

Tried 100,000 particles with dt=1. Goes at about 2 iterations per second.
200,000 goes at about 1s per step. No significant difference with dt=.1
This is without -ffortran-bounds-check.

Using profiling and gprof we get

 22.81      3.17     3.17 23890368     0.00     0.00  gradinterp_
 15.11      5.27     2.10 24594336     0.00     0.00  gradlocalregion_
 14.75      7.32     2.05  6148584     0.00     0.00  getfield_
 10.54      8.79     1.47  6763776     0.00     0.00  circlesect_
  6.73      9.72     0.94  6148584     0.00     0.00  interp_

Most of the time is being spent getting the field and in its interpolation.
If we put a short-cut into gradlocalregion to do the calculation directly
if the icp0 is zero (not a boundary), then we get:
 time   seconds   seconds    calls   s/call   s/call  name    
 31.21      4.38     4.38 24594336     0.00     0.00  gradlocalregion_
 16.33      6.67     2.29  6148584     0.00     0.00  getfield_
 12.02      8.35     1.69  6763776     0.00     0.00  circlesect_
  7.49      9.40     1.05  8993020     0.00     0.00  inside_geom__
  6.78     10.35     0.95  6148584     0.00     0.00  interp_
  6.06     11.20     0.85      314     0.00     0.00  sorrelaxgen_
  4.49     11.83     0.63  3739985     0.00     0.00  gradinterp_

There's not much savings of time. The shortcut has added about 16%
(2.3s) to gradlocalregion, which came out of gradinterp. There's no
substantial total savings (<3%). This shows that it is really the
calculations that are costing us:

         uprime= (2.*x+dx0)/(dx0+dx1) * (up-u0)/dx1
     $        +(dx1-2.*x)/(dx0+dx1) * (u0-um)/dx0

If we reorganize that expression to
         uprime= ((2.*x+dx0) * (up-u0)/dx1
     $        +(dx1-2.*x) * (u0-um)/dx0)/(dx0+dx1)
We get
 time   seconds   seconds    calls   s/call   s/call  name    
 30.70      3.91     3.91 24594336     0.00     0.00  gradlocalregion_
 16.20      5.97     2.06  6148584     0.00     0.00  getfield_
  9.79      7.21     1.25  6763776     0.00     0.00  circlesect_
  8.22      8.26     1.05  8993020     0.00     0.00  inside_geom__
  7.47      9.21     0.95  6148584     0.00     0.00  interp_
  6.29     10.01     0.80      314     0.00     0.00  sorrelaxgen_
  3.85     10.50     0.49  3739985     0.00     0.00  gradinterp_
A small saving.
Similarly, we get a small saving from using icp0=cij(1) instead of cij(ix).
Reorganizing test gets time to 3.74s. Another small saving. But we have
not done much. Probably only a 10% saving.

The profiling conclusion is that the main cost is the parabolic
interpolation of the field-gradient, which is called four times for
each dimension for each particle at each step. By fiddling around 
inside that (gradlocalregion) implementing two conditional short-cuts,
I shaved about 30% off the routine for uniform scaling. Everything else
seems pretty marginal, and is not worth effort at this time.

time   seconds   seconds    calls   s/call   s/call  name    
 23.07      2.85     2.85 24594336     0.00     0.00  gradlocalregion_
 17.28      4.98     2.13  6148584     0.00     0.00  getfield_
 10.83      6.31     1.34  6763776     0.00     0.00  circlesect_
  7.38      7.22     0.91      314     0.00     0.00  sorrelaxgen_
  7.22      8.11     0.89  6148584     0.00     0.00  interp_
  7.18      9.00     0.89  8993020     0.00     0.00  inside_geom__
  6.12      9.75     0.76  3739985     0.00     0.00  gradinterp_

5 Sep 07

Reorganized some files to clean up the main program and put plotting
and some other stuff elsewhere. 

Checked the cvs that you really can build from it. 

Now we need to get the volumes correct for calculating the charge
density.  This is trivial for the core nodes. Only difficulty is for
those on object boundaries. In cijroutine and boxedge we do some
elaborate calculation of fractions, which are supposed to represent
the points that define a plane approximation to the bounding surface
cutting a particular box, in the case that the plane actually cuts the
cell surrounding the box (which it can do even if fractions are >1).
However for the CIC particle assignment, a particle contributes
partially to a node if it is in the _box_ (not just in the node-cell,
which extends to the half-node-spacing position). Therefore, it is not
clear that I have done the relevant calculation. There will be some
degree of volume reduction for every node that lies in the center of a
2x2x2 box which is intersected by the boundary.

Things would be different for NGP assignment. Then particles contribute
only if they are in the node-cell, in which case the fractions might have
some value. 

In either case, there is a problem in that currently particles are 
removed based on the iregion. That is not a planar approximation. 
If volumes are calculated on a different basis from the particle removal,
the charge-density calculation will not be done correctly. It would
therefore be inconsistent to use direct iregion evaluations for particles
but fraction-approximations for volumes.

One alternative approach would be a Monte-Carlo integration of the
volume by multiple calls to insideall. If the weighting were uniform
(which it is not) then this amounts to a random choice of region in or
out. So n points are distributed according to a binomial. The average
of a sample therefore has a mean number: np (where p is the
probability of acceptance) and variance np(1-p), so that S.D./mean =
sqrt((1-p)/np), and the uncertainty as a fraction of the total box
volume is sqrt(p(1-p)/n)<=sqrt(1/n)/2.  Consequently, to get 1%
accuracy requires n=50^2=2500. Rather a lot of random points (3 ran0
calls per point). If we used a uniform mesh in each direction, with
this total number of points, we would have 2500^1/3= 14 per direction.
That would give a worse result in terms of uncertainty (probably). 
Monte-Carlo is better in dimensions greater than 2.


11 Sep 07

Started volintegrate and volnode for node volume calculations.
Transitioned active code to using mditerarg instead of mditerate.
However, haven't pruned all mditerate because the calls of mditerarg
are a bit hokey, since they use inconsistent number of arguments.
This should not matter, since the arguments are at the end. However
it probably violates fortran standards. So probably ought to think
about making the calls consistent.

Calls to volintegrate amount to 2152, according to gprof, which is a
major hit on initiation: about 8 seconds. This is for the 16x16x20
mesh and 10000 points for the montecarlo integration. Total pointers
used is 2800, but that includes the edge. As mesh size increases
the boundary points scales like n^2. So a 100x100 mesh would 
probably be about 6^2=36 times longer. One approach might be to 
do this work shared between the nodes. That would reduce it back to
about 8 seconds. Of the used time, about half is calls to inside_geom
which is the routine that is called for each object to determine if
the point is inside it. 

It might well be worth saving the volume data in a file, to be read
back in, in most cases. Done. File storedgeom.dat and seems to work.
There are numerous cross-checks to try to make sure this is valid
data for the case being run. 

12 Sep 07

Implemented a text plot of volume percentage. Seems to show things are
working ok (except for the external volumes). 

Increased the npoints parameter for monte-carlo to 10^5. This reduces the
random errors to <1% as expected. 

Made the volume for nodes external to active region 100%. But perhaps
the more appropriate thing would be to use their region for their
volume, or possibly a very large number so that the charge density
becomes negligible.

Discovered that if the network is in a certain type of unconnected state
then program takes for ever to initialize. This appears to be an MPI
problem.

How accurate does the volume have to be?

Well, the accuracy of the charge density is the real question. But
that has statistical fluctuations at a fractional level of 1/sqrt(N),
where N is the number of particles per cell. Thus, from a purely
statistical viewpoint, all we really need is to calculate the volume
from a monte-carlo sample of points much larger than the number of
particles in the cell. If we have a million cells (the upper range of
what might currently be managable) then 7 million particles would give
only 7 per cell.  That would make the use of 10000 points per cell
overkill.  By contrast, a 20x20x20 mesh has 8000 cells, and so we
would have a maximum of about 1000 particles per cell for this very
small mesh. Therefore, 10000 points is certainly enough to make the
statistical noise from volume calculation less than that from
particles.

There's something of a compensating factor: the particle number might
be averaged over many different realizations, while the volume remains
fixed and does not improve with averaging. Consequently, one might 
find that such quantities as the flux distribution, when averaged over
many steps, might show significant effects from cell volume errors.

14 Sep 07

Implemented a rhoinfcalc based on smax flux. Run with 1000000 particles
and zero sphere potential, it gives about 4000 reinjections per step
when it should give about 2400 or so. Thus the density inside is 
considerably less than rhoinf. Played around with this. Eventually found
that the problem is we don't have electrons turned on and so this gives
a strongly positive potential even with zero on the sphere.

We need to introduce debyelen and set it to some sensible value.
The solution of L\phi = q means that we either have to put the debyelen
scaling into the difference stencil or into q. That is we either have
to consider the equation to be \lambda^2 L\phi =\rho, 
or L\phi = \rho/\lambda^2.
The latter means that the "density" is stored as density/lambda^2.
The former means that the cij coefficients are scaled by \lambda^2.
From a computational viewpoint either is probably fine.
Actually I notice that the sceptic solver adjusts the overrelaxation
according as lambda is large or small. Probably this is not necessary
for ccpic, but might speed up things. In sceptic, I multiplied the 
poisson coefficients by debyelen^2. Perhaps that's what I should do here.
It's slightly awkward because dpm is passed up from the geometry routines.
I don't think the debyelen should be put into them. 

So put it into cijroutine. Then we can get small potential by making
the debyelen very long (with zero potential on sphere). Then we get
correct numbers of reinjections and sensible rho approximately one.
The sor convergence is a bit slower like this. We are committed to 
using mditerarg by this choice. Found that there's a problem, namely
that when setting derivative, there also needs to be a debylen 
factor in this approach. Seems to be fixed by scaling b as well.

Now we have a segfault from the -gt case. OK fixed that it was the 
adjustment I had made to slice3plot. But we still don't have the 
interpolation correct now. Ok one needed to fix another place in
cijroutine.

Need now to turn on the electrons. Doing this things are very broken
for few particles, but seem to work for many. Also there's a problem
outside the active region, where faddu is being added, though it 
shouldn't be. It might be possible to put the q equal to 1 in the 
outside region. That would cancel out the electron function.
But it looks like being a major bother.

18 Sep 07
Actually it is not a problem because volumes is set to a very large 
number outside the active region. This is easily used in psumtoq to
set q=1. Seems to work.

20 Sep 07

Now we'd like to get the code going with constant number of particles being
injected per unit time. This might be a more satisfactory solution than
calculating rhoinfinity each step based on the number that happens to be
reinjected. 

Done that. The total number of particles shows a decay over about 100 steps
of .1, by about 10%. This is presumably the readjustment of the initially
uniform density to reflect the proper final spatial distribution. 
Now we have the choice of specifying -ni the number of ions or specifying
-ri the rhoinfinity (density of ions at infinity). May not have the 
multiprocess numbers set correctly yet.

I wonder if there's a way to speed up the convergence to the final density
other than what we've always used in sceptic, which is large initial steps.
Once we vary the dt, we will need to vary the ninjcomp accordingly. 

Observed that there's an error in u inside the sphere. This arises because
the electron density is not being correctly compensated there. Changing
the setting of rho to be faddu(u) for an external region helps, but does
not fix this. The error mostly goes away if rho is set to zero. This is
a serious puzzle. Seems as if the solver is incorrect in the inner
sphere. It is possible there's an error in the algorithm for the case
when faddu is present. 

It's really bizarre, but putting the rho equal to minus faddu in the
inner region gives a perfectly flat profile. But in the outer region
you need +faddu. Explain that! Actually it seems to be an accident.
It does not work for other sphere potentials.

Found a bug in the faddu part of sorrelaxgen.f. Fixed it. But it's a
bit worrying for sceptic. It's ok. Sceptic uses a hard-wired 2-D sor,
not the sorrelaxgen. I think we now have a correct compensation for
electron density in the external regions.

Fixed some sliceplot bugs in rotation.

21 Sep 07

Things to do next maybe.

Reading and Writing the code state, fields and particles. Probably we
should keep the fields and particles in separate files since the fields
only need to be written by master, while each node must write its own
particles. How do we ensure that random numbers are the same thereafter
for a restart?

Tracking and saving particle exits (and entrances?). This depends to
some extent on the type of object. So we need a uniform way of
handling the documentation of flux distributions. This requires an api
for the object.

MPI communication of particle parameters. 

22 Sep 07

One thing slowing down my decisions is the problem of the overwhelming
amount of data. Even to save the fluxes at each step is going to take
a substantial amount if we do it for every object of 32. Of course we
will rarely or never want to do it for so many objects. Probably we
should not use a structured obj_flux(n_fluxmax,ngeomobjmax) data store
for the object storage, because this is going to be very inefficient.
Instead we should probably use a big storage space accessed by a dynamic
structure. E.g. only providing for the number of objects that actually
exists, or even the number for which we care about the flux. If that's
the case, then the main thing to specify is how that data is structured.
It could be structured the same in a binary output file.

27 Sep 07

Flux data might be structured as follows:

Level	Description					Name	No of values
0	Number of objects for which data is stored.	nof		1
0	Address of object header starts.		iof(nof)	nof

1	Number of quantities stored for this object.	nqf		1
1	Address of quantity starts.			iqf(nqf)	nqf

2	Number of positions for this quantity.		npf		1
2	Quantity descriptors/positions			qdf		npf

3	Timesteps							1

----- End of header----

3	Flux Data of quantity (nqf)					npf

When addressing a place to write flux data it is:

buff(time0+(time-1)*sum_{nof}(nqf*npf)+sum_{iof}(nqf*npf)+sum_{iqf}(npf)+ipf)

Thought again about using HDF5. However, this is really a library. There
does not seem to be any intention for it to be a data structure API for
simple programming. Therefore the only real reason for using HDF5 is to
make the data portable to someone else's calling of the HDF5 libraries.
That's not really what I am looking for. The principles of HDF5 do
embody some of what I am looking for: self-describing data strutures etc.

Perhaps we could reasonably choose the number of quantities to be fixed
so that we simply only need to specify the objects for which we are storing.
Then, the positions could be variable. 


29 Sep 07

Implemented structures in 3dcom.f and initializations in fluxdata.f.
Found that it was block data operating on big common blocks that caused
the very large executable. So removed that and put explicit initialization.

Implemented tallyexit in padvnc, with rudimentary particle flux counting.
It seems to work. Can subsequently obtain the fluxes.

Implement outputflux routine and corresponding readfluxfile routine to
write and read-back the flux data. 

Implement fluxdatatest to see that we really can read it back independently.

13 Dec 07

Over several days working on fedora8 have got rid of warnings. The
main actions are to turn off unused argument warnings and to edit
/usr/local/include/mpif.h to remove the save statements on the common blocks.
It ought not to be necessary to use save statements on them I think.
However, that is a peculiar usage that might be really needed, and I'll
have to watch out for it. In any case, that change did not change the
answers.

[Got extace compiled and working after installing some devel rpms for fftw
and other. Got rununfullread working.]

16 Feb O8
Fiddled a bit with the flux reading and writing.

12 Jun 09
Added documentation to 3dcom.f
Added additional possible dimensions to the position descriptor data,
because a single number might not describe the position sufficiently.
More dimensions (nf_posdim>1) does not affect the actual data averaging
since it is stored as a linear array. But the linear array might really
refer to two angles (or more parameters). 

Fix segfault bug with undefined cworka in orbit3plot.

13 Jun 09

Fixed some other little bugs.
Improved some accis routines to make the rotation of the slice plot work
more smoothly and fixed some bugs in it.

Now want to overplot the 3-D orbit on an existing perspective plot.
Issue is that a mesh has the third axis scale set to something to do
with the function amplitude rather than position. Need the ability to
rescale that axis, plot orbits, then scale back to prior. Also need
a poly3mark routine.

Instead, first modified the cijplot routine to enable choice of objects
to plot and overplot the orbits on that. This avoids any need to rescale
etc. Also made a polymark routine. 

2 July 09

Replaced the ran() function with a purely fortran one ran1 from NR. 
The reason is that we can't get at the internal state of any of the
decent built-in random generators, so we can't do proper restarts.
The time for ccpic -s20 with the old one is 
real    0m33.843s
user    0m16.358s
sys     0m0.378s
And with the new one it is
real    0m33.437s
user    0m16.306s
sys     0m0.304s
There is no detectable time difference for this single-processor case.

In partwriteread we save the random number state ranstate, and read it back
so that we can restart and get exactly what we would have got if we had
not saved the state.

At present, we do not have the mpi infrastructure for gathering particle
data from different processes or broadcasting back the field data. This
is eventually going to be needed.

Ok. Created writing of potential too. 
I think that ought to be enough to be able to restart. We have to have
identical parameters, which is currently assumed rather than written,
although iuds is stored with potential. 

Trying to get the restart going. I find that there is disagreement.
To test the rereading, I do an internal restart by jumping back into the
code after 5 steps. This gives identical results as not restarting.
This shows that everything (relevant) that is read back is being read
back correctly. However, the difference with a true restart means
that there's something in the restart that is not being set correctly 
by the reading process.

Tests based on ccpic, ccpic --restart, ccpic -s10,
with a sample psum, q, and random number show:

The psum and q difference does not start till the 8th iteration.
This is already remarkable, since it means there's no difference
for the first two iterations after restart. 

The psum/q differences start before there are any differences in the
random sample. This strongly suggests there's not a problem with the
random number generator restart.

I find that the u-differences start immediately after the first solve
of the restart. So that is where things are changing.

Probing inside of sormpi with output from individual iterations. This
shows that the differences start to show up on the second return from
sorrelaxgen0 after the restart:

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
 Return from sorrelaxgen0           4 -3.5938688E-02  -2.000000      6.5017469E-02


Return from sorrelaxgen0          56  2.6678543E-05  -2.000000      6.2136307E-02
 Return from sorrelaxgen0          57 -2.2355829E-05  -2.000000      5.4357897E-02
 Return from sorrelaxgen0          58 -1.8731193E-05  -2.000000      6.2133629E-02
 q    sample   1.502413     u    sample -6.0202956E-02
0006 iterations:  58 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499379     u    sample -6.0202956E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3089679E-02  -2.000000      5.8978312E-02
 Return from sorrelaxgen0           2  7.8395963E-02  -2.000000      6.1556634E-02
 Return from sorrelaxgen0           3  4.7830440E-02  -2.000000      6.8785459E-02

 q    sample   1.059864     u    sample -5.5481944E-02
0005 iterations:  58 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
 Return from sorrelaxgen0           3 -3.9642353E-02  -2.000000      6.9339350E-02

Printing out in addition, relax, omega, xjac_sor shows difference in relax
occuring on the same return that there is a delta difference (but not omega).

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
   1.890094       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
   1.876438       1.967066      0.9957700    

 q    sample   1.059864     u    sample -5.5481944E-02
0005 iterations:  58 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
   1.891335       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9642353E-02  -2.000000      6.9339350E-02
   1.876438       1.967066      0.9957700    

Yes, there seems to be a difference in oaddu. Perhaps this is arising from
the previous solution because there does not seem to be an initialization of
dden. Consequently the dden is what is left over from before. That will make
the prior test solution give a difference, since oaddu is 0 for it. This seems
to be the cause:

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
  7.3466018E-02   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
  7.2402194E-02   1.890094       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
  7.2402194E-02   1.876438       1.967066      0.9957700    

 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
  7.2402194E-02   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
  7.2402194E-02   1.891335       1.983257      0.9957700    

Yes, changing to ensure that it is properly initialized fixes this:


 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197598     u    sample -5.5441111E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9148662E-02  0.1710546       1.000000    
 Return from sorrelaxgen0           2 -5.9492927E-02  0.1805826       1.787417    
 Return from sorrelaxgen0           3 -3.5247445E-02  0.1702435       1.765143    


 q    sample   1.059865     u    sample -5.5441111E-02
0005 iterations:  43 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197598     u    sample -5.5441111E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9148662E-02  0.1710546       1.000000    
 Return from sorrelaxgen0           2 -5.9492927E-02  0.1805826       1.787417    
 Return from sorrelaxgen0           3 -3.5247445E-02  0.1702435       1.765143    

It also seems to lead to smaller number of iterations. Which is also good.

However, it has not solved the later divergences. They start after the 6th
iteration, which is completed without divergence. How can that be?
The difference first observed is in delta.

 Return from sorrelaxgen0          44 -1.9401210E-05  0.1755715       1.662254    
 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3093132E-02  0.1704593       1.000000    
 Return from sorrelaxgen0           2  7.4114323E-02  0.1755701       1.787982    

 Return from sorrelaxgen0          44 -1.9401210E-05  0.1755715       1.662254    
 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3690698E-02  0.1704593       1.000000    
 Return from sorrelaxgen0           2  6.2475450E-02  0.1755701       1.787982    
 Return from sorrelaxgen0           3  4.0570423E-02  0.1725774       1.769773    


 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -3.3093132E-02  0.1704593       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.787982    
 Return from sorrelaxgen0           2  7.4114323E-02  0.1755701       1.787982    

 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -3.3690698E-02  0.1704593       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.787982    
 Return from sorrelaxgen0           2  6.2475450E-02  0.1755701       1.787982    


I think it pretty much has to be an internal state of sorelaxgen. Because
all the parameters are set to be the same going into it. Unless something
very bad has happened with u or q.

3 Jul 09

It is the seventh iteration first call to sorrelaxgen that goes awry.
Perhaps the red-black iteration is the problem? It is controlled by the
value of k_sor which is the same. Hard to see it. We need a better way
of detecting the difference.


 psum sample   279.4092     u    sample   0.000000      ran1  0.2056652    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -7.8246323E-03   2.635117       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.070949    
 Return from sorrelaxgen0           2 -8.1036519E-03   2.636601       1.070949    
 q    sample   1.000000     u    sample   0.000000    
0005 iterations:  14 Total flux number   51.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51906  52359  99.973   0.100

Created a checkdelta routine inside sorrelaxgen to read an old file of
deltas and check the current value against it. The files have to be swapped
after the run: mv checknew checkdelta.

I find that even subsequent identical runs do not agree. There are
small differences in the low order bits of the delta. This seems to show
either that there are truly random bit-level errors or that there's
something wrong with the initialization that prevents exact repetition.
The differences arise even in the initial test solution before any
particles have been added. Strangely, there's zero difference in the particle
solutions etc. So I don't understand how there can be delta differences.

Called ran1(-1) earlier to make sure the volumes are the same on
different machines. Found it gave a segfault. IDUM was being set
incorrectly in ran1. Stopped that. Also made the subsequent setting
call -myid-1 to ensure that it's really negative.

With optimization turned off we are getting bigger differences in the 
first solve. And subsequently smaller and smaller differences. I'm not
sure whether that's significant. Still the difference early in the
first solution are not small, and in general the first iteration is
showing it. These are not rounding errors. They appear to be random
initialization differences.

Found that I was not initializing the u to zero (or anything). This
was apparently the cause of the differences. Fixing it got rid of the 
delta differences. But that has not fixed the restart. 

Tried checkuqcij on some different step numbers. Found that step 6 is fine
but that going into the sormpi on step 7 there are q differences (very slight).
How can that be? 

After long struggles with passing errors and segfaults, got testing of
uqcijpsumvolumes going. It shows that after a restart the psum is different,
pretty much everywhere by a moderate amount. The qs are different too, but
not on the boundary (where presumably they are not set). Again this is the
_second_ step after the restart. In other words the first step has caused
the psums to start to differ. They did not differ immediately after
the restart. This check is going into (right before the sormpi call).
So we can't tell if this difference is coming from sormpi. So put the call
right _after_. The answer is that the first uqcij checks fine. Thus the
first sormpi call is working correctly. Consequently it must be the first
particle move that is giving errors. However, putting after padvnc does
not show differences till the second test. Putting after the chargetomesh
shows differences on the second cycle. This is consistent with the particle
move giving differences that are transferred to psum at the first subsequent
chargetomesh. 

We rely on the partlocate in chargetomesh to keep current the mesh position.
Although we save it and restore it, it is the updated by chargetomesh. 

Printed out a sample of the x_part values immediately after the (first)
advance. There's no difference. That's a puzzle:

 Calling sorrelaxgen   0.000000       0.000000       1.072827    
 Return from sorrelaxgen           2 -9.6360259E-03   2.613914       1.072827    
 q    sample   1.000000     u    sample   0.000000    
0006  15 iterations. x_part sample   2.396352     -0.6855422      -3.690475      -1.057408      -1.120257     -0.6776624       4.751034       3.213245       1.688664     -0.4781322       2.480651       2.000758     -0.8974879      0.2682267      0.2117976       3.305810       4.726902       4.489779    
 Total flux number   42.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51802  52359  99.973   0.100
 ***** psum difference           2           1           1  7.6883718E-02  7.7840298E-02
 ***** psum difference           3           1           1   1.262272       1.280330    
 ***** psum difference           4           1           1   1.013222       1.011209    
 *

 Calling sorrelaxgen   0.000000       0.000000       1.072827    
 Return from sorrelaxgen           2 -9.6360259E-03   2.613914       1.072827    
 q    sample   1.000000     u    sample   0.000000    
0006  15 iterations. x_part sample   2.396352     -0.6855422      -3.690475      -1.057408      -1.120257     -0.6776624       4.751034       3.213245       1.688664     -0.4781322       2.480651       2.000758     -0.8974879      0.2682267      0.2117976       3.305810       4.726902       4.489779    
 Total flux number   42.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51802  52359  99.973   0.100
 ***** psum difference           2           1           1  7.7840298E-02  7.6883718E-02
 ***** psum difference           3           1           1   1.280330       1.262272    

They start to deviate very slightly the next step. But in reality if the 
positions of the particles are the cause of the problem, I don't see how they
can be exactly correct after the first push. 

Implement xpartcheck. I find that there are differences in x_part during
the second iteration, before chargetomesh. That is, I confirm differences
after the first particle push, for particle 29 47 54. The velocity differences
are large. Order unity. The position differences are small. Seems to imply
that the acceleration is where there's an error. There's nothing obviously
systematic about the particles with differences. 

Did check with uqcij right before padvnc and xpart right after. The xpart
diffs are present, not uqcij. Thus we've proved there are advancing differences
when there are no uqcij differences. Since no if_part differences appear,
that's clean. The 6-9 x_part values, which are the mesh fractions, are not
in error the first time. But by the third time, they are. Thus, we are 
not calling partlocate. These are not newly injected particles.

This is tough. Construct a diagnostic version of padvnc. It reports the 
value of i and field for the first 100 particles. I see that the particles
with differences are reported twice. That is they are going through
the routine a second time. This is because inewregion is being returned
as 3.

ccpic -s8
          28 -1.6265622E-03 -9.0324413E-03 -3.8206729E-03           2           2
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2
          30 -8.1677195E-03 -2.4931235E-03 -1.4990305E-04           2           2

ccpic --restart
          28 -1.6265622E-03 -9.0324413E-03 -3.8206729E-03           2           2
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2
          30 -8.1677195E-03 -2.4931235E-03 -1.4990305E-04           2           2

It appears that the field is actually being evaluated the same both times.

-s8
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
 Reinjected          29  0.5868658      -4.671297       1.682282      0.2139225       1.030760      -1.000355       3.793430       1.164375       4.341133      2.3144661E-02   0.000000             110           3           1           4  0.7934299      0.1643748      0.3411326               2
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2

--restart
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
 Reinjected          29  0.5868658      -4.671297       1.682282      -1.054011       1.133722     -0.2721342       3.793430       1.164375       4.341133      2.3144661E-02   0.000000             110           3           1           4  0.7934299      0.1643748      0.3411326               2
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2

The differences are in the 4-6: velocities reinject is giving the particle
different velocities, although the same positions. The velocities are got 
from gasdev.

GASDEV in randf.f was calling ran0 not ran1. Thus it was giving different
velocities. This problem arose because I did a grep on ran0 not on RAN0.
Argh! So I lower-cased the routine names there to prevent future problems.

Fixing that, I seem to get the right answer. No I am getting x_part 
differences for particle 2 (only). 

restart:
0006  15 iterations. ====== Finished uqcijckeck
           1 -1.2301615E-02 -9.4538052E-03  3.2518448E-03           2           2
           2  1.0301614E-02 -2.0927475E-03 -2.7766897E-04           2           0
 Reinjected           2  -1.689946      0.6363239      -4.662046     -0.2221324      -2.588727      0.2661515       2.655035       3.818159       1.169000    
           2 -2.2906796E-03 -2.6510621E-03  2.6147573E-03           2           2


s8
0006  15 iterations. ====== Finished uqcijckeck
           1 -1.2301615E-02 -9.4538052E-03  3.2518448E-03           2           2
           2  1.0301614E-02 -2.0927475E-03 -2.7766897E-04           2           0
 Reinjected           2  -1.689946      0.6363239      -4.662046      -1.117069      -2.251752      0.6365508       2.655035       3.818159       1.169000    
           2 -2.2906796E-03 -2.6510621E-03  2.6147573E-03           2           2

There's another problem. gasdev has internal state information so the first
calls to it are broken. Make the internal state of gasdev available in 
ran1com and include it in the save and restore. Yes. Now we are getting
identical results.

Summary of Restart Facility and Experience.  4 Jul 09
------------------------------------------- 

Code now restarts and gives exact numerical agreement with runs that
simply had more steps in the first place. The key problems were
associated with random number generators. 

I found that dden was not being initialized properly in
sormpi. Correcting that also reduced the number of iterations. [I
think that Leonardo noticed a problem that might be this.] Another
problem found was that u was not being initialized to zero.

A day-long search tracked down the fact that (1) gasdev was calling
the old ran0, not ran1, and also (2) needed its internal state saved.
With that fixed we have exact agreement.


Had a problem with the cvs commit. Wireless broke in the middle. Hope this
is going to work. But it does not seem to do so. 


7 Jul 09 Restart is still not right. 

We get Getfield no good vertices errors. This is because all fractions
are zero. We ought not to be using these slots somehow.  I think the
reason is that pinit initializes all slots up to 52359 but when run
for long enough, ioc_part drops below this. (That did not happen for
my short test ./check) Consequently when we read back, we don't write
over the slots that have been reinitialized, and so the code thinks it
is ok to use them but it's not.  The fractions are not set in
pinit. Perhaps they should be. But perhaps this is fortuitous because
it has shown me an error that might not otherwise have been obvious.

In partread zeroing the flag of slots higher than ioc_part stops errors.

8 Jul 09

Multiprocess calls don't work. On TP400.
There, I've installed the latest MPICH2 from source. Things then compile.
I've fiddled inside bbdy but that's not the cause (I'm pretty sure).
Yes verified that by getting back the cvs mpibbdy and doing a 2 process
run, which crashes.

         call MPI_BARRIER(MPI_COMM_WORLD,ierrmpi)
         stop
helps to localize where the crash occurs. If it happens before this,
then you don't get much output. If not, then you get all output up
to this. 

The crash appears to occur on the second call to sormpi. This is the first
time that communication is occuring. I've heard that mpd does not work
when the hosts resolve to 127.0.0.1, which is the case for me now.
Tried some other mpd.hosts lists, but they give "failed to handshake".
Can't even get the tp41 working. 

Turns out the result of $hostname must be the same, on either machine.
It does not crash on tp41 with -n 2 and blks=2. So we are looking for 
something tp400 dependent.
Checked out the cvs to tp41. It runs with two processes. Yep it's tp400.

Got the alltoallw.f test program. Compiled. It seems to work. Does not
crash. Yes, it makes sense (after some puzzling). Therefore we seem
not to have an inherent communication problem. It's really a problem
inside sormpi. And in fact the mpd stuff all works with the hostname
set to tp400-64.

SCP the new mpibbdy.f to tp41 make. It runs two processes correctly,
with no relevant code differences to here. It is the tp400. 

The code mpibbdytest.f is also broken. This is probably easier to
debug. I can see that communications are correct till the gather. 
Then crash.

9 Jul 09
Figured out the problem finally by doing some tests on Loki.

The problem lies in the routine MPI_TYPE_CREATE_HVECTOR or the older version
MPI_TYPE_HVECTOR. In many man and web pages MPI_TYPE_HVECTOR is said to be
obsolete and to be replaced with MPI_TYPE_CREATE_HVECTOR. See for example
http://linux.die.net/man/3/mpi_type_hvector and
http://linux.die.net/man/3/mpi_type_create_hvector

However, these calls are not identical. In particular the argument STRIDE is
simply INTEGER in MPI_TYPE_HVECTOR but in MPI_TYPE_CREATE_HVECTOR it is:

INTEGER(KIND=MPI_ADDRESS_KIND) STRIDE

I was not declaring STRIDE explicitly (I called it iSTRIDE). Therefore it was an
INTEGER. On a 32 bit machine, INTEGER is 4 bytes long, the same as
MPI_ADDRESS_KIND. On a 64 bit machine (in linux) an INTEGER is still 4 bytes
long, but MPI_ADDRESS_KIND is 8 bytes, and so CREATE code breaks.

I discovered this by accident because on Loki there is no
MPI_TYPE_CREATE_HVECTOR, for reasons I still don't understand. So we replaced
the call with MPI_TYPE_HVECTOR there. When I did the same on my 64 bit laptop,
my code worked, after which the explanation above became clear. Especially
helpful was the page

http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=/com.ibm.
cluster.pe510.mpisub.doc/am107_itchvec.html

It turns out that gfortran recognizes the above KIND construct (not F77). So I
can obey the advice to use MPI_TYPE_CREATE_HVECTOR. Alternatively I can NOT
define iSTRIDE and use MPI_TYPE_HVECTOR. Either works with the correct
declaration.

Tried replacing the sceptic mpibbdy.f with the new fixed one. It gives
identical results. But perhaps I am not using the right switches to 
exercise it.

Probable next steps. Communication of the particle data through MPI.
Most of what else I had previously planned is done.

12 July

Starting on particle gathering. We need a block structure for exchanging
all the data for the whole used array, for example for psum or q.

It makes sense to use the bbdyblockcreate routines to do this. 

First change the iside(imds,2) to iside(2,imds) so that we don't have
to have imds everywhere, rather we can pass ndims, and use that for the
second (but not first) dimension. iside occurs only in mpibbdy.f, as
does bbdyblock. So it appears that's the only file we need to fix.
Now imds is used only in the top level bbdy routine. Not in the block
creation routines.

These and other complications arise because the existing code has
built in two types of block length, bulk and top. Also tuned up the
resetidim code to choose better match to prescribed nprocesses.

Packaged the construction steps for the alltoallw types into routine:
bbdygatherparam. This helps to contain the issues. 
It seems to show that we really only need a call to bbdyblockcreate.
Then the bulk handle for dimension n is at ktype(2**n).
If we put iside(2,id)=0, then we will get a negative count, because
the pass does not include the boundaries, even though the length appears to.
Thus one should presumably pass q(2,2,2) but with iside equal to iuds.
This is confirmed in that iside is calculated from the difference
in origins but with 2 added. 

There's another issue to do with chargetomesh. It puts the charge onto
adjacent nodes without consideration of boundaries. That's not really right.
Checking inside volint, I currently set volumes taking account of the 
mesh boundaries. Vol is set to a large number outside region which will
divide the density down to zero. Vol is set to a corrected number for
a node with intersections. Seems as if this ought to work. So perhaps theres
no problem. If so then we just need the reduce with the correct structure.
There might be a problem with gradient at the boundary. This would mean
the BC is effectively not second order accurate. 

13 July 09

I discover that I have misunderstood the REDUCE capabilities. In order
to operate on a customized MPI datatype, one has to write a customized
operator. So even though I can create a datatype that refers to the 
block iuds(ndims). I can't reduce it unless I have the operator. 
Operator has to be of the form
	 user_function(invec,inoutvec,len,type)

In my case I'd be passing only one dataobject. So I'd have to know
internal to the function, the structure of the calculation required,
e.g. add up the active elements.

Alternatively, if I keep it simple and reduce the whole arrays, regardless
of how much is active, there are inefficiencies. E.g. if only half is used
in each dimension, the inefficiency is 2**3 = 8. How does this compare with
the other communications? Well the boundary exchanges take place more 
often: iterations x more. But they are smaller, because they are only 
faces, not volumes. The number of iterations is probably at max a few
times the linear dimension. So the total communications for a solve
is a few times a total volume. If that's right, then we are comparing 
few x volume with (e.g.) 8 x volume. The reduce is going to be more important
than the total solve communications. It is worth trying to minimize it.
Or else just never using subarrays would make life WAY easier.

The user_function has to know by common: ndims, iLs and iuds (or ium2)
It might do an mditerarg, in which case ifull is needed rather than iLs.

Wrote a user_function using mditerarg. It seems to work. There's an 
error using MPI_IN_PLACE with MPI_REDUCE, but not with MPI_ALLREDUCE.
I suspect that this can only be used by the root (for some stupid reason).
It does not do anything for the non-root in a REDUCE call, but IMHO ought,
nevertheless to work, for consistency. It doesn't.

Packaged the function all up into a decently easy call and moved it
into mpibbdy.f. mpibbdytest shows it working.  Commit.

Put ALLREDUCE of nrein and phirein into rhoinfcalc. This should be 
sufficient to satisfy the partcom reduce requirements. We need to
figure out the normalization and defaults for ninjcomp/nrein etc.

Found that I had to do the allreduce on psum so that psumtoq set
the external phi correctly to compensate for the electron density
which it otherwise did not for multiple processes.

Got phiexamine routine going to examine the potential after the run. 

We need a potential interpolation routine that works at the boundary.
We seem only to have a field interpolation routine at the moment.
Potential ought to be easier, but I don't know how it should work off hand.

If I did not set there to be a discontuity in phi at the edge, it might be
easier to do the interpolation sensibly. It is certainly possible to 
prescribe sphere type 1 instead of 257, which then gives a profile that
could be more easily extrapolated.

Deep thought. This is not a trivial issue. 

18 Jul 09

Built a getpotential function that does multilinear interpolation if all
the nodes are present, else falls back to another approach. At present
just uses the closest valid point. Probably not quite as good as using
a continuous phi, but should work without. Need a way to test this. 
Intend to use stencil extrapolation as the better solution, which will
also need testing.

Had an idea that I could calculate the cut volumes a lot quicker by
doing a geometric integration from one vertex (or actually anywhere in
the cell) along lines. Finding the nearest place where these move out
of the cell would be reasonably easy (bisection). This would work
correctly for cell shape that was convex. Concavity might mess us
up. For that the monte-carlo approach might be more convincing. But I
doubt if concave surfaces are usual. And we probably could avoid the
issues by specifying a particular direction for lines.

We currently use 10000 MC points which requires ndims*10000 random
numbers and 10000 insidealls. I could imagine getting decent results
from a 10x10 grid of lines each of which has to have its ends found to
(say) 1% accuracy. 1% accuracy by bisection requires about 8
insidealls (times two ends). But probably one could avoid a lot of
those calls by starting the end search at the boundary wall, which
will usually be correct. I'd guess, then that each line integration 
will cost on average more like 4 isidealls. In which case our costs are
going to be in the vicinity of n1 x n2 x 4 insidealls. Thus even if the 
insidealls are the main cost and rand is not (which is optimistic), there's
the opportunity for perhaps a factor of 10 win by going to n1=n2=15 which
ought to be fully satisfactory. In the very long run, an improvement of this
order is required if one is to contemplate a moving boundary that needs
its volume recalculated at each step. Such a situation might also call
for parallelization. 

20 Jul 09

Implemented getpotential multilinear interpolation
with two types of filling in of missing points:
1. Just set equal to the nearest point value.
2. Calculate gradients locally from nearby present points. Then fill in
by extrapolating from the value at the centroid. 
Added a phi plot to -gt

Type 2 is sometimes a lot better than fill type 1. However, it is basically
the same when we are on an axis, because then the type of absent points
is such that one face is absent in total. The gradient in that direction
can't be calculated and is taken by default to be zero. Thus cases with 
one whole face missing are almost as bad as the simple nearest point fill.
In a case like that, we must use more information.

I realize that I could use getfield (or some form of it) to obtain the 
gradient, rather than using the box vertices. This might be a more reliable
way to do it. But I'd still need an average or some other value to 
extrapolate from.

21 Jul 09

Very nearly got something working with a fallback to get the field at
the point if we fail to find the gradient from the existing box points.
However, there seems to be a bug. I think it is that it is possible to
set a box vertex that is actually on the other side of an interface 
as being present in the region, if it does not have a pointer set 
(i.e. it is not itself adjacent to the boundary). That gives a big error
when it happens, at the edge and -at2.1. It is certainly possible for
opposite vertices, even in 2-D, to suffer this problem. Consequently,
my implicit assumption that a vertex with pointer not set is in the right
region, is wrong.

This seems puzzling because cijroutine seems to be written such as to
fill in all vertex pointers that have crossings within any of their
boxes. However, the text3 plot shows that that is not happening.
Actually there's some code in cijroutine that is said to drastically
reduce the number of boxes counted. Perhaps that's ultimately the
cause.  For the standard mesh, it reduces the number of pointers from
14k to 11k. So the storage reduction is definitely not worth it.
Anyway removing that test and using the increased pointer count
fixes the bug, making my assumption correct. A 60x60x60 grid with
about 200k point uses about 50k pointers. Which by the way makes cij
objects almost as important as cij itself.

As anticipated, fillin gets a field value on average less than once
per call. So the cost of extra extrapolation is pretty acceptable. 
There are still some peculiarities at the mesh edge in the outside
region. We are getting the wrong field value. It ought to be zero
but it is not. Probably there's an issue with the cij values for
this type of BC. Actually, I don't see how I can correctly do the
extrapolation from the mesh edge. Because I think the point beyond
will be used. That's an issue. Maybe that's ok because of the cij facts.

Print out the maximum phi and grad phi errors for different grids.

grid  	  at?     solution	   phi      gradphi
16x16x10   .9	  3.62e-2	   -.330    0.376
16x16x10   --	  ''		   .385e-2  0.724
16x16x10   .5     ''               .411     0.272
16x16x16   --	  4.0e-2	   .199	    0.35
	   .5	  		   .223	    0.156
	   .9			   .256	    0.100
32x32x32   --     6.2e-3	   .27e-2   0.163
           .9	  		   9.7e-2   0.453
	   .5			   .105	    0.347
64x64x64   --     2.00e-3	   6.6e-3   1.69e-2
	   .9	  		   2.2e-2   8.8e-3
	   .5			   3.1e-2   3.0e-2
128^3	   --     1.61e-3          4.8e-3   2.68e-2
	   .5	  		   2.1e-3   3.5e-2
	   .9			   5.9e-3   4.6e-3

There are sormpi convergence issues with this the 128^3. Fiddled but not
great. 

It's not terribly obvious from these maximum errors what the convergence
is. The gradphi seems horrendously noisy. Phi does not seem quite so bad.
The domain is 10 in diameter, so the ratio of sphere radius to cell size
is 10/L. To have field errors that are of order a percent when the ratio
of characteristic size to cell is ~10 is consistent with second order
convergence, although things don't blow up as badly as you might expect.
32 is pretty bad. 

23 Jul 09

Convergence is pretty slow for the 128^3 case. Looked into the SOR.
Remember that the k_sor count is half-steps (red/black). So it is not
as bad as it looks. The biggest problem is that the omega gets too big
in the initial Chebychev acceleration calculation. It ought to be a
bit lower but I have not changed it as of now. The convergence can be
tweaked a bit but the omega choice is pretty good. I think the only
realistic way to improve it would be to do dynamic adjustment based on
actual response.  That would need more programming than I care to
devote at present.  When lambda is less than about 1, there is a
dramatic reduction in number of iterations. When lambda is greater
than about 50, there is also a dramatic reducion after the first solve.

Next we need to get the reinjection to a point where we can really compare
some cases with sceptic. At present we just have Gaussian reinjection.
We worked on getpotential in order to be able to calculate the averein
potential. Checked sceptic2 paper to see how good we need to be. 
Looks as if there's going to be an issue at intermediate lambda unless
we adopt the complicated reinjection schemes of sceptic.

The sceptic schemes are contained in

REINJECT=fvinject.o orbitinject.o extint.o maxreinject.o ogeninject.o

Of which the maxreinject has no drift. fvinject is for collisional
distribution functions, ogeninject is for gyrotropic distributions.
orbitinject is the standard form. If we hack the orbitinject file
to remove the fv, max, ogen references, then with
REINJECT=orbitinject.o extint.o
it compiles and runs. Therefore, it appears I could use this.
extint contains the netlib exponential integral function(s).
oreinject(i,dt) is the call to reinject. The main issue is the use
of piccom.f which has a load of stuff in it that I don't want.
I need to cut down those needs and rationalize the parameter passing.
The only thing that needs the reinject.f code is the test routine
fvinjecttest, and it does not actually call it, it just needs it for
advancing.f, I think. 

orbitint program tests the stuff, but it seems also too dependent on
piccom.f and sceptic calls to be useful. 
Made a version of orbitinject with implicit none. Seems to work in
orbitint. This makes clear all the locally defined variables.
Anything else is a reference to the common variables.
Then commenting out the include piccom.f should reveal those.
Ok did that an put in dummy declarations.

oreinject
c We put here the declarations that are needed to fix excluding piccom.
      real xp(6,1)
      real averein,debyelen,fluxrein,pi,spotrein,Ti,vd
      real Vcom
      logical lcic,localinj
      integer nthsize
      parameter (nthsize=2)
      integer nr,nrein,nrfull,nrused,nth,nthused
      integer nrsize
      parameter (nrsize=2)
      integer nvel,ntrapre
      parameter (nvel=2)
      real r(nrsize),th(nthsize),phi(nrsize,nthsize)

      r( is used only for determining the wall radius.
      nr is ditto.
      th( is used only for inverting to find th coord for phihere.
      phi( is used only for phihere.
      nrfull ditto. nrused ditto.
      nrsize is unused.
      nthsize is used only in the common below.

We also find that the following common is dependent on piccom
but only occurs in oreinject routine. 
c Testing
      real vdist(nvel)
      real tdist(nthsize)
      real crdist(nthsize),cidist(nthsize)
      common/rtest/crdist,cidist,tdist,vdist

oinjinit
c To fix exclusion we need
      real Vcom(1),pu1(1),pu2(1)
      real Ti,vd,pi
      integer myid,nvel

pu
c needed from common:
      real averein,Ti,pu1(1),pu2(1)

alphaint
c needed from common:
      real adeficit,averein,debyelen,Ti
      integer nr,ninner,diagrho(1),r(1),rcc(1),diagphi(1)

  diagrho( is used only in
           if(diagrho(1).eq.999.)then

  rcc( and diagphi( and ninner are used only in
            if(ninner.gt.iqsteps)stop 'Alphaint Too many r-cells read.'
            do i=1,ninner
               qp(i+1)=1./rcc(ninner+1-i)
               phibye(i+1)=diagphi(ninner+1-i)
            enddo
So none of the above are important if we are not signaling a special
case using diagrho(1) in a kludgy way. Actually it is a signal that
we read a file in in orbitint. Completely unneeded for most use. 

  r( is used only in 
            call initext(iqs,qp,phibye,phiei,r(nr),xlambda)

Now the plan is: construct a different common declaration that together
with existing ccpic declarations will satisfy the needs and allow
testoij to run properly. plascom.f has 
	common/plascom/debyelen,Ti,vd,rs,phip

averein and adeficit don't exist.

Start reincom.f with the random interpolate data from piccom.
      nvel, nQth
      common /rancom/Gcom,Vcom,Qcom,pu1,pu2,Pc,infdbl,bcphi,bcr

Add averein, adeficit, fluxrein, spotrein, myid, ntrapre

Comment out the 999 region. Replace r(nr) with rs. That satisfies
alphaint, pu, oinjinit.

In oreinject
Comment out rs declaration, and setting of it. It's already set.
Remove r( using rs. Remove nr.

Convert phihere section to a function. Get rid of th, phi, nrfull, nrused.
Define nthsize=201 in reincom.f for the commons same as piccom.f. 

We still need nth, nthused which appear in 
      if(LCIC)then
         icr=(1.+crt)*0.5*(NTHUSED-1) + 1.5
      else
         icr=(1.+crt)*0.5*(nth-1) + 1
      endif
In LCIC cases in orbitint, 
   	      nth=nthsize-1 nthused=nth nthfull=nth+1
In non.LCIC   nth=nthsize, nthused=nth-1 nthfull=nth
In sceptic, nth is set by command line. So how do we fix this?

Made two more parameters depended upon nthsize that fix this to the 
LCIC case.

To get a test going with the testoij code we need to deal with
averein, adeficit, fluxrein, spotrein ...

fluxrein and spotrein are set by orbitinject, but never used by orbitint.
So nothing needs doing.

averein is set by orbitint and not by orbitinject, except in the code that
has been commented out in testoij. So we just need to set it differently.

adeficit is default set to zero by orbitint and only set by commented out
code in orbitinject. So we need to zero it differently. 

ntrapre is not used.

nrein is used by orbitint after orbitinject is called, for normalization.
it is incremented by ilaunch each time, which might be bigger than one.
We need code to get it back.

Summary we need: avereinset(+avdeficit) and nreinget in orbitint.f.

Found rp=rs error in orbitinject.

Got orbitinttest going to the extent of completing although not 
correctly.

With further corrections, got orbitinttest working the same as orbitint.
Got rid of lcic, nthused, nth. Retain nthsize=nQth+1 for size of arrays.

Imported a version to ccpic. Got it to link by adding relevant stuff.
Segfault (as expected). There's a fair amount yet to fix in the initing
and in xp passing averein calculation, etc.

24 July 09


Found some bugs in padvnc.f. Was not correctly handling if_part on
reinjection, since it was not being set in reinject. This probably
accounts for prior particle decay. But in any case it changes
the results.
Rationalized reinject call not to pass particle number.
Corrected the rs handling to keep it equal to the sphere radius,
even though the mesh is a tad larger.

Now seems to run in a similar way with new and old reinject.
But I need a portable way to diagnose the reinjections. If it is built
into padvnc, then it will apply to every injection scheme. Probably 
that's best. 

Implemented some reinjection diagnostics and plot that are independent
of injection scheme and implemented by a simple call in padvnc. Seems
to give similar results from my two schemes: shifted maxwellian and
orbitinject. They ought to be the same here because averein is being
taken as zero at present. 

Printed out the velocity distributions because a simple gaussian comparison
is not correct. With counts peaking at 2k per box, the distributions
are indistinguishable. Conclusion: the orbitinject is working. Commit.

TIME TESTS using sceptic:

Single process 
time ./sceptic -s200 -ni900000 -nr20 -nt20 -g -f
tp400:
real	0m24.868s
unity:  
real	1m21.860s
Thats a pretty big difference. tp400 is only using one core.
It is dominated by particle pushing. 
-ni100000: unity gives 0m20.754s. and tp400: 0m6.555s.

Two identical processes doing this on tp400: 0m7.002s. unity: 0m36.481s
I.e. very little overhead on tp400 maybe more on unity.
But that's not real multiprocessing we need scepticmpi.

scepticmpi -ni100000
unity 0m41.520s, tp400 0m7.199s. Not much difference on tp400. 

sceptimpi -ni300000
unity 1m51.364s  tp400 0m20.609s

four processes -ni300000 tp400 2m10.836s !!! Wow, that's a big hit, nearly
a factor of 10! Both processors showed 50% utilization for each sceptic.
There must have been a monstrous hit from swapping. 

Quick summary:

Single processor speed difference is about 3.3 times faster cf unity head node.
Per cpu speed is about 5.0 times faster on tp400 than unity running mpi.
It is useless to run more than 2 processes on tp400.

Total cpu capacity on unity = 36x1 on tp400 = 2x5. 36 vs 10.
tp400 is about 1/4 of the total cpu capacity of unity in multiprocessing.

[But probably there are some big breakpoints.]

The Loki tests of Feb 2008 showed Loki processors to be 3-4 times faster
than unity head node. It therefore seems that tp400 is getting about the
same per-cpu speed as the Feb2008 Loki cluster. cmodws45 got about 2x slower
in 2008. 

25 July 09

Need for averein to be set.
In sceptic averein is set from diagphi, which is part of the diagnostics
and is averaged over nstepsave. So averien is time-smoothed. We would have
to save it for restart purposes. It does not make sense to average every
injection. We ought to add injection potentials to a counter and then
divide by the number added together when we complete the advancing step.
The addition should be done in diagrein. The computation might be done
at the end of padvnc. Let's use spotrein for the sum of potentials at
rein and fluxrein as the sum of reinjections. That's what sceptic uses
them for. They need to be zeroed at the start of each padvnc.

There's currently a conflict between reincom and partcom, both of which
contain nrein. I'm not sure that nrein is really needed in partcom, although
it is written and read back, and needs to be MPI reduced. As presumably
may the other accumulators. There's also a variable phirein, which is not 
currently used except in rhoinfcalc.

27 July 09

First make reincom just the extra stuff and put explicit inclusions of the
plascom and rancom headers, where needed.

Now need to rationalize averein, phirein, nrein, ninjcomp. There's no
phirein in sceptic. 

For restart purposes, anything that influences the orbits or field must
be saved and restores. nrein is set to zero at the start of padvnc. It
does not need to be saved and restored, I think. However ave/phi-rein
if averaged from step to step, will need to be saved/restored. It is more
of a field quantity than a particle quantity, in a sense.

adeficit is used in orbintinj, and in sceptic set in advancing as part
of fcalc_lambda:
c Boundary slope factor calculations:
      do j=1,NTHUSED
c Current fractional ion deficit due to collection.
c Coefficient of 1/r^2 in modified shielding equation is
c a = deficitj * r_edge^2 / \lambda_De^2
         deficitj=1-phi(NRUSED,j)/Ti -rho(NRUSED,j)
c         write(*,*)rho(NRUSED,j),phi(NRUSED,j),deficitj
         blfac1=(deficitj/debyelen**2) * redge
         adeficit=adeficit+blfac1
c BC modification is (a/r_edge)[exp(EL*r) E_1(El*r)] given by approx.
         blfac=blfac1*expE1
         blfac=alpha*blfac
         phislopeconst(j)=blfac*redge*delredge/
     $        (redge+delredge*rindex*0.5)
         phislopefac(j)=(redge-delredge*rindex*0.5)/
     $        (redge+delredge*rindex*0.5)
      enddo
c Actual a factor averaged over angles:
      adeficit=adeficit*redge/NTHUSED
      if(adeficit.lt.0.)then
c         write(*,*)'Negative adeficit',adeficit,' set to zero'
         adeficit=0.
      endif

This gets used in the boundary condition of the sor solver. I don't think
it makes sense for CCPIC to think that we are going to be changing the 
BC every step (maybe not at all). 

29 Jul 09

Implemented OML adeficit setting (only once in the stencils).
This overrides the boundary condition in the input geometry file,
but seems to give very sensible results.

Implemented infrastructure for getting averein (in reincom) and
phirein (in partcom) from spotrein, which uses getphihere. So far
getphihere gives zero and no changes of result are found. 

There's a problem with the fact that getpotential needs lots of stuff
like u, cij, etc. So it can't readily be called from reinject.
It could be called from padvnc, but padvnc does not know the ilaunch
values etc. If I add ilaunch to the reincom, then its value along with 
nrein, spotrein and fluxrein are available to reincom. Then if we
return to padvnc before doing any advancing; getpotential; and pass it
to a routine to advance; that would work. Clumsy though. 
We could take the nrein passing out of reinject if we did this, because
it would be updated at the same time as averein etc.b

This needs rationalization. 
1. We don't need to update adeficit.
2. We update the potential used by reinject once per cycle.
3. We need to average the phihere to supply this value.
4. nrein and potl needs to be incremented by an amount that depends on the 
   number of launches.

I get rid of the localinj section. Then I don't need phihere inside the 
oreinject. Get rid of the accumulation inside of oreinj. 
Pass ilaunch to reinject. So the accumulation can all be done in padvnc.
We no longer pass nrein, because we do the accumulation in padvnc.

Pass potential and ilaunch to diaginj, and therein increment spotrein.

Remove nrein and ntrapre from reincom.f. Now not needed.
Remove fluxrein and spotrein from reincom.f. Now not needed really. 
The only things that really have to be in reinextra are averein, adefict.
These are to be set (only) by avereinset and adeficitset.

Revert diaginj to not passing or calculating with phi or ilaunch.
Update phirein in padvnc. Call avereinset and adeficitset appropriately.

Seems to be working. phirein seems very stable for l>1 and very small 
for l<1.

At present I am using nrein, phirein in rhoinfcalc for calculating
rhoinfinity. It seems to be correct, but not thoroughly checked.

30 July 09

Implemented saving of rhoinf and dt per step which then allows one to 
get the flux density normalized by rhoinf 

We have a result. The OML value flux density is 1/\sqrt{2\pi}
\sqrt(T_i/T_e) (1+\chi) which for \phi=-2 is 3/\sqrt{2\pi}=1.19. We
get this value within less than 1% with large debyelength (40x40x20)
at T_i=T_e.

At lower -l flux drops, which qualitatively it should. There is some
systematic flux enhancement at the poles. The potential and density
get very noisy, and without multiple processors, it is tough to do
much about that. We also need to look at more-negative potentials. But
first off we have a decent result that verifies the orbit solver,
though not really the potential solver with finite rho. Also verify
that acceleration is working.

31 jul 09

Unfortunately this is not working. At higher probe potential only
moderate enhancement of the flux occurs. Not nearly enough for
OML. E.g. at probe potential of -10. Also there's a major difference
between running 200k particles on one processor and 2x100k on two
processors. Therefore there's a bug in MPI somewhere. The total flux
count from both processors is about 420, and it's about twice that
reported by each. Rhoinf is about 420 (too). With one processor, there
seem to be significantly more flux count, about 460. It's unfortunate that
we need to go to a fairly high step count >100 otherwise things are not
converged. 

2x100k Total flux 440, 1.65, rhoinf=430, nrein=2077
200k   Total flux 440, 2.08, rhoinf=340, nrein=3166

tells the difference. The nrein and rhoinf are printed after padvnc and
fluxreduce but before rhoinfcalc. So I think they refer to just one 
processor, since nrein and phirein are reduced in rhoinfcalc. 

I just realized another issue, although I think not one that is relevant
here, that is that if the MPI_COMM_WORLD had different processes from the
CART_WORLD, then I might end up with doing particle calculations on a
node that does not participate in the information about potential. So it
might be completely bogus. 

I have to decide what to focus on. I'll start with single processor issues.
The flux is way too low. Why? Idea: replace getfield with a coulomb force.

Do this easily in padvnc. Then get

./ccpic -ni100000 -s200 -dt0.05 -da5 -l1000
0200   2 iterations. Phirein=   -1.983818 Total flux   219.00000   2.0631711    
 Total flux   219.00000       2.0631711    
nrein,n_part,ioc_part,rhoinf,dt= 1488 100000 100000   168.939     0.050

 Average flux over steps         100         200  All Positions:   4657.2266    
 rhoinf   164.82874       Average particles collected per step:
  4.9406  4.5545  4.9208  4.8515  4.8020  4.6139  4.9604  5.2970  4.2475  4.5842
  4.2673  4.4356  4.7624  4.6139  4.4752  3.9901  4.4653  4.8119  4.7723  4.4158
  4.9802  4.9109  4.9505  4.5644  4.4851  5.0000  4.4752  4.0990  4.6832  5.0396
  4.7723  4.4653  4.1782  4.7228  4.6436  4.8911  4.9901  4.6931  4.5941  4.0792
  4.7030  4.8020  4.7624  4.7624  5.0099  4.3861  4.8119  4.5743  4.5545  4.4950
 Flux density, normalized to rhoinf   2.2484589    

For comparison, still with the coulomb force, but with -l1. instead of -l1000.

0200  36 iterations. Phirein=   -0.072735 Total flux   339.00000   3.4161470    
 Total flux   339.00000       3.4161470    
nrein,n_part,ioc_part,rhoinf,dt= 1110 100000 100000   157.937     0.050

 Average flux over steps         100         200  All Positions:   7102.1768    
 rhoinf   165.44156       Average particles collected per step:
  6.7327  7.4752  6.8020  6.9703  7.2475  7.1584  7.2475  6.6040  7.3069  7.0297
  7.0495  6.6733  6.9901  7.1881  7.5644  8.0297  7.1980  7.6832  6.6733  7.1188
  7.3663  7.1188  6.4950  7.4257  7.0099  6.4653  7.2574  7.2673  6.3366  7.0891
  6.5743  7.0891  8.0495  7.2079  7.0297  7.0495  7.1089  7.3564  6.9406  7.1584
  7.1287  6.9307  7.1287  7.1683  6.7624  6.9307  6.9109  7.4356  7.9109  6.6634
 Flux density, normalized to rhoinf   3.4161532    

That's pretty strange, since the force is always coulomb. 


Try setting phirein=phip/5 immediately before padvnc. Makes the the flux
value low although I don't understand all the output:

0200  41 iterations. Phirein=   -0.297464 Total flux   335.00000     1.766086
 Total flux   335.00000       1.7660868    
nrein,n_part,ioc_part,rhoinf,dt= 2439 100000 100000   301.893     0.050

 Average flux over steps         100         200  All Positions:   7347.1270    
 rhoinf   309.72659       Average particles collected per step:
  7.7921  7.5149  6.9703  7.0594  7.5743  7.6535  7.6634  7.3762  7.8812  7.1782
  7.4851  7.2574  7.4455  7.3960  7.0594  6.9802  7.5347  6.7030  7.4653  7.8119
  7.2871  7.3069  7.2871  7.0495  7.1881  7.2673  7.0693  7.5743  7.6832  7.4950
  7.5248  6.9406  6.9406  7.8416  7.4752  7.1683  7.5842  7.6634  7.6238  7.4455
  7.3663  7.2376  7.2079  7.3861  7.3069  7.3960  7.3267  7.1584  6.7624  6.9901
 Flux density, normalized to rhoinf   1.8876851    


sceptic run like this:
./sceptic -l1000. -nr50 -nt10 -g -ni1000000 -p-10.

gives with some extra diagnostics:

  Flux, Phiedge, Trap, ReinjFrac
  401:  2 nrein=        3047   averein=  -1.9901596      riest=   325.22070      rhoinf=   328.70486    
  402:  2 nrein=        3059   averein=  -1.9901588      riest=   326.50159      rhoinf=   328.61774    
  403:  2 nrein=        2995   averein=  -1.9901581      riest=   319.67062      rhoinf=   328.56485    
  404:  2 nrein=        3083   averein=  -1.9901574      riest=   329.06339      rhoinf=   328.34250    
  405:  2 nrein=        3083   averein=  -1.9901565      riest=   329.06348      rhoinf=   328.36053    
  4.197 -1.990  37401  0.008

This is with dt=.025, so the nrein is of order 6000 for a .05 step.
Plainly the major difference is in nrein. Actually in ccpic there are
essentially no repeat reinjections. nlost=nrein. So any changes that
are arising from edge potential must be through the velocity
distribution. Arrghh.

1 Aug 09
Found the cause of segfaults in sceptic. It was brcsq=0. Changed the 
c Reject a particle that will not reach boundary.
c Was brcsq.lt.0. which gave segfaults.
      if(brcsq.le.0.) then
         goto 1
      endif
This probably should be fixed in a couple of other places in the sceptic
code too.

Implemented mean speed diagnostic of reinjection. It does not look the 
same for sceptic and ccpic. In particular the speed of ccpic takes a step
at the value of phirein, which is wrong (should be sqrt(phirein)).
No. That was an error. I put -p10 instead of -p-10. With -p-10 we get
speed up to 2 in both. Perhaps there's another factor of 2 floating around.
To the naked eye the distributions look the same. If that is right, then
it says we are injecting correct velocities and positions.

5 Aug 09
Ran equivalent cases in sceptic and ccpic. For ni=200000, dt=.05, -p-10
-l1000
ccpic approximate numbers:
ninner=450. nrein=3100. phirein=-1.9839. rhoinf=330. 

sceptic: 
ninner=450. nrein=3080. phiedge=-1.99. rhoinf=164.

Thus the problem is with rhoinf in ccpic. It is a factor of 2 too high. 

In sceptic rhoinf comes from rhoinfcalc:
c smaxflux returns total flux in units of Ti (not 2Ti)
            riest=(nrein/dt) /
     $           (sqrt(Ti)*
     $           smaxflux(vd/sqrt(2.*Ti),(-averein/Ti))
     $           *r(NRFULL)**2 )

and in ccpic
c Calculate rhoinf from nrein if there are enough.
         chi=min(-phirein/Ti,0.5)
         riest=(nrein/dtin) /
     $        (sqrt(Ti)*
     $        smaxflux(vd/sqrt(2.*Ti),chi)
     $        *rs**2 )
         rhoinf=riest

But the expression for chi is wrong! It needs to be max(-phirein/Ti,-0.5).
We were constraining the effective phirein to be -.5, when it was really
-1.98.

With this fixed, we get flux 4.472 normalized to rhoinf. (Sceptic 4.3916).
11/sqrt(2\pi)=4.3884. So ccpic is a bit high. rhoinf=165.
This is with pure coulomb test potential for acceleration. Running with
-l100000, phirein=-1.9947. flux=4.4755. 

Full ccpic mesh potential. flux=4.4122. 
Using 400 steps instead:   flux=4.3756 (mesh). flux=4.4008 (coulomb).
Using dt=.025 4.4049 (mesh).

These numbers appear to be consistent with a flux count that is for dt=.05
400 per step times 200 steps =80000 total particles => 0.4% accuracy.


6 Aug 09
There's still a bug in the mpi code. For 2 processes we get
[tp400 CCPIC]$ mpiexec -n 2 ./ccpic -ni200000 -s400 -dt0.025 -da5 -l100000
Average flux over steps         200         400  All Positions:   17157.578    
 rhoinf   262.65146       Average particles collected per step:
  8.6119  8.1891  8.6269  8.4826  9.1791  8.4378  8.7761  8.1144  8.9701  8.6119
  8.7811  8.7164  8.0547  8.8060  8.0945  8.3433  8.5373  8.7363  8.3134  8.4179
  8.5970  8.9154  8.3234  8.6617  8.5423  8.7612  8.3831  8.6020  8.3781  8.7164
  8.4876  8.6866  8.6965  8.4925  8.4478  8.9005  8.6269  8.1791  8.6368  8.8259
  8.9900  8.7313  8.9751  8.4826  8.7811  8.4527  8.1841  8.8955  8.5174  8.2687
 Flux density, normalized to rhoinf   5.1983638    
 
 0400   2 iterations. Total flux   441.00000       5.3087544    

And the standard case:
[tp400 CCPIC]$ mpiexec -n 2 ./ccpic -ni200000 -s400 -dt0.05 -da5 -l100000
Phirein=   -1.994918
0400   2 iterations.  nlost=        4186  nrein=        4186  ninner=      419
 Total flux   861.00000       5.1920910    
 Wrote particle data to T1e0v000r05P10L1e5.000
 Wrote potential data to T1e0v000r05P10L1e5.phi
 Wrote flux data to T1e0v000r05P10L1e5.flx
 Average flux over steps         200         400  All Positions:   17160.164    
 rhoinf   264.44150       Average particles collected per step:
 17.3831 17.1393 17.1443 16.6119 18.4229 16.7662 17.6169 16.2786 17.9900 17.0448
 17.1294 17.8358 16.1493 17.7214 16.2338 16.9154 17.2786 17.3831 16.5970 16.9005
 17.1493 17.7811 16.5274 17.2687 16.7512 17.5075 16.6219 17.2488 16.6617 17.7264
 16.9900 17.2289 17.2139 16.9204 16.9154 17.6219 16.8607 16.5174 17.3333 17.7711
 17.6766 17.7413 17.7363 17.2338 17.6716 16.6219 16.5373 18.0348 16.9154 16.6816
 Flux density, normalized to rhoinf   5.1639533    

This appears to show that rhoinf is too low, because nrein is too low. 
With two processors, it ought to be about twice the one-processor value
which was 3100. Or it ought to be the same if I am only seeing one processor's
worth. Actually seems as if we are monitoring each processor separately
and then getting too great nrein?
The flux of 861 is roughly twice the ninner number of 419,
which is about right. 

One problem was that numprocs was not being set in common by the sormpi
initialization, because it is stored in partcom, which is not available.
For now, set it explicitly with an MPI call in ccpic. This seems to 
fix it, but the plotting at the end seems to break when specifying -l
switch to mpiexec.

 nrein,dtin,Ti,vd,phirein,chi,rs,numprocs=        6328  5.00000007E-02  1.00000000       0.0000000      -1.9947799       1.9947799       5.0000000               2
0200   2 iterations. Phirein=   -1.994886  nlost=        3106  nrein=        3106  ninner=         445
 Total flux   905.00000       4.2716675    
 Wrote particle data to T1e0v000r05P10L1e5.000
 Wrote potential data to T1e0v000r05P10L1e5.phi
 Wrote flux data to T1e0v000r05P10L1e5.flx
 Average flux over steps         100         200  All Positions:   18373.264    
 rhoinf   331.71246       Average particles collected per step:
 19.1683 17.9604 18.8515 18.6040 18.6733 18.7921 17.6931 18.7723 17.7525 18.5743
 17.8317 18.3960 18.5049 19.3267 17.5644 17.4158 17.9010 19.0990 18.4455 17.9406
 19.2376 18.4752 18.5941 18.0495 17.5644 18.6040 18.2475 17.8218 17.7624 18.8911
 18.2475 18.7030 18.2475 17.5644 18.2277 19.2079 18.7921 18.8515 17.9901 18.3366
 18.8317 18.9208 18.6733 18.2277 17.9802 18.1287 19.0990 17.9901 18.4752 17.6535
 Flux density, normalized to rhoinf   4.4077301    


08 Aug 09

Rationalized some of the MPI functions into psumreduce to get them out of
main and other functions. 

It appears that the first time that numprocs is required is in pinit. 
This is called before the first psumreduce. So it won't do to set numprocs
in psumreduce. To get the MPI call setting numprocs out of main
requires a plan for this. This and the finalize are the remaining calls.

Move the additional MPI routines into reduce.f. Can't cvs add because of
bad nameserver in Venice. Do it later. 

How to fix possible mismatch between cartesian communicator and particle
(MPI_world). We don't want every gather in solver to be to all the world. 
However, 

10 Aug 09
gprof shows that field interpolation dominates in particle-dominated
runs. The interp is for partlocate. Gradinterp is for getfield.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 22.96      3.44     3.44 148586924     0.00     0.00  gradlocalregion_
 15.75      5.80     2.36 37146731     0.00     0.00  getfield_
 13.82      7.87     2.07 36890568     0.00     0.00  interp_
 11.35      9.57     1.70       30     0.06     0.15  chargetomesh_
 10.21     11.10     1.53 38305999     0.00     0.00  gradinterp_
  5.64     11.95     0.85 50755040     0.00     0.00  inside_geom_
  3.14     12.42     0.47     1121     0.00     0.00  sorrelaxgen_

So the getfield is more than half of the cpu time:
-----------------------------------------------
                0.02    0.04  256163/37146731     fillinlin_ [25]
                2.34    5.34 36890568/37146731     padvnc_ [3]
[4]     51.6    2.36    5.38 37146731         getfield_ [4]
                3.44    1.53 148586924/148586924     gradlocalregion_ [5]
                0.08    0.33 37146731/37146731     boxinterp_ [14]
-----------------------------------------------
                3.44    1.53 148586924/148586924     getfield_ [4]
[5]     33.2    3.44    1.53 148586924         gradlocalregion_ [5]
                1.53    0.00 38305999/38305999     gradinterp_ [9]

By implementing a jump out of the loop in getfield when ii1=0, reduces
the time in that routine from 2.36 to 2.00, a 15% saving. 

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 23.44      3.41     3.41 148586924     0.00     0.00  gradlocalregion_
 15.22      5.63     2.22 36890568     0.00     0.00  interp_
 13.71      7.62     2.00 37146731     0.00     0.00  getfield_
 12.78      9.48     1.86       30     0.06     0.16  chargetomesh_
  8.49     10.72     1.24 38305999     0.00     0.00  gradinterp_

c Probably most of the time is spent in this calculation.
            uprime= ((2.*xm+1.) * (up-u0)
     $           +(1.-2.*xm) * (u0-um))/(dx0+dx1)

This is (2xm.up-2xm.u0 -2xm.u0+2xm.um + up-u0+u0-um)/(dx0+dx1)
= (2*xm*(up-2*u0+um) +up -um)/(dx0+dx1). This might be quicker.
But it isn't. 

Summarizing the computational costs for particle mover, they are
dominated by the getfield, gradlocal, and gradinterp region costs,
which is finding the field. It does not seem possible to make 
substantial savings in this area. 


12 Aug 09

Systematic comparison of sceptic and CCPIC with:
  dt    vd     Ti     steps. -c5  -ni200000 -nproc 2 -nr50 -nt10. -p-10. 
 0.0250 0.0000  1.000  500 
								ni700000
 		Flux:		-da3		-da5		60x60x60
debylen		Sceptic:	CCPIC
.1		1.6146		1.7244405			1.6327829
.2		2.2890		2.3339274			2.2894616
.3		2.7719		2.7993238			2.7707448
.5		3.3643		3.3790596 			3.3812742
.7		3.7056		3.7201011			3.7148750
1.		3.9970		4.0019903			3.9962780
2.		4.3465		4.2363796	4.2306852	4.2459707
5.		4.3954		4.3413186			4.3627992
10		4.3775		4.3506932	4.3558187	4.3643198
		
[Note theoretical OML: 11/sqrt(2\pi)=4.3884]

It appears there might be slight systematic discrepancies at the OML
end and at the small debyelen end. The debyelen is not being properly
resolved at -l.1 because of -nr50 and 40x40x20 resolutions. Therefore
discrepancies are to be expected. In the intermediate regime the
discrepancies are ~1% or less.  This seems very encouraging.

26 Aug 09

Fix the mismatch between numprocs and the cartesian geometry by using
the MPI_DIMS_CREATE function that fits the cartesian geometry to the 
exact number of processes available, in an optimal way. It is not called
if the number of processes is equal to the idims product already. So 
by setting idims and numprocs right, one can override the function choice.

Known weaknesses/needs
1. Possibility of undefined potential (/field) when in a concave region with
no adjacent points in the region.

30 Aug 09

Filled in the 60x60x60 column above. This shows much better agreement
with sceptic at low debyelen. Better than 0.5% for debyelen<=1.  But
also seems to indicate there's a problem at large debyelen. It's not
100% certain that the problem is CCPIC. It might be sceptic (also).
mpiexec -n 2 ./ccpic -dt.01 -da10 -s500 -ni700000 -l10
gives 4.3776546, which suggests that shorter timesteps increase the 
apparent flux. This might suggest a going-through-sphere process. 
Sceptic avoids that, but CCPIC does not. 
mpiexec -n 2 ./ccpic -dt.05 -da3 -s500 -ni700000 -l10
gives 4.3439922, which definitely confirms there's a timestep effect.

mpiexec -n 2 ./ccpic -dt.025 -da5 -s500 -ni700000 -l100
gives 4.4045405, which is perhaps surprising. 

Using -p-25 we get from
mpiexec -n 2 ../ccpic -dt.025 -da5 -s500 -ni700000 -l100
10.382284. Should be compared with 26/sqrt(2pi)=10.3725. Pretty good!

Conclusion

The symmetric results seem to indicate excellent agreement at 60^3 mesh
size with OML and with SCEPTIC, down to l=.1. 

Things to do next.

1. Add momentum and energy to the flux quantities monitored and displayed
by fluxdatatest.

2. Add ability to account for flux of momentum across a surface (presumably
rectangular) including the Maxwell stress.

3. These things might be incorporated into a new way of handling events
consisting of surface crossings: movement from one region to another. 
To do this we would have to keep account of what the region is that a
particle is in. Then we compare new with old before updating it. 
This seems to be already available since tallyexit will be called crossing
any boundary. We need to add the ability not to reinject if this is not
a solid boundary, and the ability to have variable oldregion (if we continue
to use insideall). 

4. Develop the ability to do more general object construction, e.g. sums
and differences of regions. This requires a way to mask insideall. Insideall
returns an integer whose bits are set for all the objects that the particle
is inside. If the object is the (inclusive) OR of two others, then the test
we want to apply to it is whether both bits are zero, in which case we are
outside. This would call for masking the others. Our current test is that
we want the particle region to be outside sphere 1 but inside 2. This works
without current problems because it is a single region.

Cases: I1 and I2 = ~(O1 or O2), O1 and O2 = ~(I1 or I2), 
       I1 and O2 = ~(O1 or I2), O1 and I2 = ~(I1 or O2).
This shows that every or can be rewritten as the negation of an and.

The values to define a region might be numerically this:
1: inside region, 0: outside region. [As currently with iregion].
A mask would say pay attention only to certain bits, not others. 
But it would have also to accommodate boolean combinations of regions.

Since we have the inside_geom function for individual objects, which is 
a faster call, probably we ought to rely on that. 

5. We need the ability to ignore the boundary of certain objects when
setting up cij. Those are objects that don't affect the poisson solution
but are there for other purposes. One way to do that would be that if
all of ABC for the object are zero, then no BC is set: so ignore in potlsect.
Did that. Seems to work correctly. 

6. Then there's an issue of volume setting. This ought to be affected only
by the particle region. That's actually true except for the tests for the
region to calculate and the volintegrate routine, which just use insideall
tests. Thus we basically need a more general insideall test that can handle
unions etc. Then it should work. 

At present compare the result of insideall with a number to tell the active
region. This is not general enough. Probably the simplest solution is to
have a logical function instead, that says: am I in this generalized region?
Then I just need a way to specify the generalized region. I don't think
it is worth the effort to devise a way to specify an arbitrary logical
operation. So instead, code possibly more than one directly and make the call
logical linregion(irg,ndims,x) where irg specifies the generalized region
numerically, but it doesn't generally do so just by its bits. For example
we could make negative irg point to a list of logical combinations, while
positive irg could be bits. 

A general way would be to have a 2-d array of numbers bool(ni,nj) such 
that the logical expression is   Prod_1^nj Sigma_1^ni inside(bool(ni,nj))c
where inside is true if we are inside object bool(ni,nj) (and negative values
of bool means true if outside). To simplify passing, one can make this
a self describing array: n1, n1*values, n2, n2*values, ... , 0

Implemented the self-describing boolean array ibool_part and gave it
a standard default. Eliminated insideall from 
pinit.f
volint.f
test in padvnc

Unfortunately there are rather a lot of places where iregion is used 
in the code. It's not so obvious how to disentangle the presumption that
iregion carries the region information. 


1 Sep 09
Installed ftnchek and made make target ftnchek. 
Started to get rid of warnings. Got most of them gone. 

2 Sep 09
Realize there are multiple types of generalizations of iregion that we need.
1. Particle region, which also defines the charge-density volumes.
2. Potential and field, solution and interpolation.
3. Additional surfaces for force and other calculations.

The particle region is defined by ibool_part. We have purged the use of 
insideall from the volume integration and particle initialization.

We need to purge iregion_part, because that usage no longer makes sense.
At least we need to get it out of being used. Currently it is _set_ in
ccpic and fieldtest, and _used_ only in padvnc. The checkcode tests its
saving, but that's not important.

In padvnc it is used to set iregion, which is passed to getfield, and 
the difference between inewregion and iregion is passed to tallyexit.
In the long run, I want to (possibly) tally all surface crossings. So
the tallyexit must be changed to use the difference between the two regions
of the start and end of step. 
We need to correct the iregion passed to getfield. It's not clear we should
pass anything. Probably getfield ought to decide its interpolation 
separately. 

Ok iregion_part is now unused.

The gradlocalregion code uses
         if(idob_sor(iregion_sor,icp0).ne.iregion)then
for testing the possibility of region crossings. Therefore, provided the
idob_sor region values are correctly adjusted to use the potential region,
it ought to work. There's no reason why getfield needs to be told the 
region. It is told the position. It can figure out the region for itself
from the position. Actually it's not so straightforward, because getfield
is given the mesh position, not the physical coordinates, and what's more
it can be given internally-based values. So really, getfield _can't_ 
currently get the region for itself. However, it could be passed the value
of insidemask and then compare it with idob_sor, which should have been
initialized with insidemask as well. Implemented insidemask and made
the changes in 3dobjects. Seems to work the same. The mask is currently
all 1s.

padvnc needs insideall for the logging information and insidemask for
the getfield information. These might be different. It's inefficient
to call both. Maybe I need a imaskregion function. Yes do it using 
the F95 intrinsic. Works. Now need to set the masking from the readgeom.
Implemented mask setting using F95 intrinsic IBCLR.

Found there's an issue with parallelpiped objects. abc are put at
the end in standard objects, which is at 8=1+1+3*nd. The objects are
described by 2*nd= center, 3 radii/lengths. But a parallelpiped needs
more lengths: an origin and 3 vectors= nd*(nd+1). If abc are at the end
then they would be in a different position. These are 3dobjects of course,
so the total is 1+12+3=16. One approach would be to require the parallelepiped
to put its data in a funny order: type origin vector1 abc vector2 vector3.
That's not totally stupid. Do it for now.

4 Sep 09

Got the code going with masking in input to getfield and getpotential. 
It works provided first two objects define the probe and the boundary, 
which is assumed in ccpic. You can then add on other objects as desired.

The sensible thing is for fluxdatainit and tallyexit to be able to cope
with other types of objects and crossings. They currently do the required
tests to see if the object is mapped or not. However, the interface left
open is the grid on the object which we are asking flux etc to be mapped
into. Currently this is programmed. 

Perhaps what is most needed is to specify what is programming API and what
is input configurable. And to separate definitively the specific aspects
from the other code.

Got xoopic (and got it going after fiddling). It is the same code base as
techx oopic, but uses the xgrafix graphics library, which restricts it to
unix-like machines. Oopic uses Qt and runs on mac and windows. It implements
most things in input files, although I think extra diagnostics can be
added on through code modifications. Its boundaries appear to be made
of individual line segments, each of which can do accumulation. There's
a load of code for parsing input and presumably setup. Boundary ends are
said to be moved to the nearest grid point. I don't know how the interpolation
is done if the lines are diagonal. 

I think the xoopic approaches are not all applicable to 3D. It's just
too cumbersome to have generalized facets and use them to construct
all objects. That was the conclusion I came to early on. I think it was right.
But I still haven't really defined the API.

7 Sep 09.

We need a way to specify the flux collection for an object in the input
file. This specification should be along the lines of the code in fluxdata.
For a sphere, we might wish to collect in angles cos\theta and \psi, with
a variable number of angles for each. In general, then we need to specify
a type of gridding and the number of bins in each of up to ndims-1 dimensions.
This would still be true if we specified the assignment in some kind of 
fourier space, such as spherical harmonics. Therefore, we ought to write
a general flux initialization code and objsect code that can handle this.
What we currently have is only rudimentary. 

The additional information we therefore need for each object is the 
flux collection type, and two integers. 

We currenly have a bit of a problem with       real obj_geom(odata,ngeomobjmax)
because the odata are ordered such that the abc is at 8, which is not enough
data prior to it for the geometric information. It might be better to 
reorder it so that this is more flexible. To do that, I need to make sure
there are no direct references to explicit values of the first index.
Ok purged all direct references to position in obj_geom. Using only
relative to the defined parameters such as otype,ocenter,oradius,oabc, ...

Permuted the order of oabc and other things. Then found a bug. It is not
in what I've just done, but it is that when an extra sphere of radius .5
is added to the objects, a change in one of the flux counts occurs.
The addition of the sphere causes one less particle count for one step
and one position. But making the radius 0.8 instead of 0.5 makes the 
differences much greater. And making it .1 gives no differences. Obviously
the counting is not working properly. This appears to be because tallyexit
chooses to count the crossing as being for the last object crossed.
This is presumably a mistake. One ought to count it for all objects crossed.
Changed tallyexit accordingly. That fixes the difference.

Implement ofluxtype ofn1 ofn2 as the object flux collection parameters.
The problem with my input file is that deciding things by ordering is
very problematic when the number of parameters on a line gets large.
Especially when one wishes to set values above others and the number
of initial values is variable. Perhaps therefore we need to rethink
readgeom routine and input format. This would be a big issue. I don't 
want to do that now, and if it needs to be done, I might write a
code to prepare the input file.

Gradually getting this done. Increase nf_posdim to 2 from 1. Fix the order
as 0 -> 1, -1 -> 1 for the position data. Then fluxdatatest still works.
(But is not yet general.) Save the dimensional structures of the
different quantities (two ints each) as well as the totals. Then we can
in principle use different grids for each quantity although at present
we can't set them to be different through the input file.

Ok. We now seem to have two dimensional accumulation grid setting through
the input file for spheres. But not yet the actual accumulation.
Rewrote the objsect code for spheres to do accumulation. Seems correct.

Try to test with different velocities. Sort of works apparently.
 time ./ccpic -s100 -v1. -ni100000  real    0m15.954s
 time ./ccpic -s100 -v1. -ni200000  real    0m30.662s
 time ./ccpic -s100 -v1. -ni400000  real    0m59.150s
Time scaling is linear in this number of particle range. (Small grid).

Got accumulation of multiple objects working and corrected the mapping
back and forth from flux objects to geom objects. Also got fluxdatatest
to work with multiple object accumulation. 

Conclusion for today
---------------------

We have 2-D accumulation of flux working. We ought to add momentum and
energy. We might want to sign the accumulation, that is, subtract or 
add according to the direction of surface crossing.

8 Sep 09

Made mf_quant equal to the fluxtype. Sensible choices might then be
1 Flux only. 4 Flux plus momentum. 5 Flux, momentum, energy. 

Found a slight problem. mf_quant needs to be set for each object, and
so needs to be an array. At present it is only a scalar. Looks doable,
but needs follow through.

10 Sep 09

This done and the flux reduction also extended to cover all quantities.
At present assuming that the grid is the same for all quantities. 
MPI seems to be working. Commit.

Edit fluxave so that it can be told a particular quantity to average
and plot. Include reporting of average flux in ccpic for objects that
accumulate it.

14 Sep 09

Implementation of maxwell stress calculations.

Having thought about the best way to implement, my idea is that calculation
is best described for a general object by a set of surface elements. 
We regard the object's surface as being approximated as a set of facets.
Each 3-D facet has
     A center position (3 reals) P.
     A normal direction and area (3 reals). A vector in the normal direction 
       with magnitude equal to the surface area: A. 

For general number of dimensions, these just become ndims+ndims.

Then the total force on the body due to Maxwell stress M is 
     Sum_{surfaces i} M_i.S_i,
where M_i is the stress at position P_i.

The nature of the approximation is left open by this datastructure, and
can be chosen appropriately. Obviously, though, the approach will work
best if the areas are approximately equal. 

For a sphere the division into cos\theta and \psi equal spacing will 
give equal areas, but it costs no extra to use the general representation.

If there are n_c cos\theta elements, the end positions are
   c_i=1-(2/N)i, (i=0,N) and the areas to be applied to element d\psi are

A_zi/d\psi = -[c_i^2-c_{i-1}^2]/2
A_yi/d\psi = (1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]
and the optimal position to evaluate the stress for linear dependence on theta
is
	\theta = (1/2)[arccos(c_i)+arccos(c_{i+1})]
	x=r cos\theta, y=r sin\theta
[See notes]

These are referred to a plane of constant \psi, in which lie both A and x.
If there are M psi-positions, then \psi_j= \pi*(-M-1+2j)/M  , j=1,M. 
However, the area to be attributed to large \psi-angles is not given
correctly by r d\psi, because of curvature. Using a grid with cell ends at
	  \psi_j=\pi*(-M+2j)/M
we find that for A_xij and A_yij, the axes perpendicular to the polar axis,
and with \psi measured from the x-axis, the values to be used for d\psi are
cos\psi_j-cos\psi_{j-1} and sin\psi_j-sin\psi_{j-1} respectively. So

A_xij = (cos\psi_j-cos\psi_{j-1})(1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]

A_yij = (sin\psi_j-sin\psi_{j-1})(1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]

A_zij = -(1/2)[c_i^2-c_{i-1}^2] .d\psi

Also, the psi_cj value at which to evaluate the stress is \pi(-M-1+2j)/M.

15 Sep 09

In testing the simple field evaluator needed for the maxwell stress 
calculation, I find that there's an error in padvnc, in that the fractional
position was not always being advanced, which meant that the getfield
call did not always give the correct answer for the current position. 
Put an explicit evaluation of the fractional position into the move loop.
I don't really understand what was happening with the field evaluation
and why the errors seemed only to be in the high positions.

I think this is because partlocate is called in chargetomesh. But I see
now why there's a problem. The iteration of chargetomesh it carried out
only up to n_part. That's an error if there are empty slots. One needs
to go to higher slot numbers. That definitely needs to be fixed.
Change the iteration to up to ioc_part. That fixes it; and the update
to fraction is not needed in padvnc.

This has changed the flux. Therefore this error is significant in prior
code tests. They'll have to be redone. But anyway we now have the 
fieldatpoint code working and checked. Actually this error is not significant
in the scans I did because I used fixed particle number which does not
call this bug up.

17 Sep 09

Created stress.f that includes routines to handle the surface facet 
arrays. Code to create the arrays for a sphere has been implemented and
debugged. Tests include integrating the arrays over the surfaces to obtain
the surface area, and calculating the force on a charge/dipole configuration
in an external field. The convergence appears to be quadratic, as it should
be. However, moving the charge close to, and then outside of the sphere
leads to significant errors in the total force when the charge is close to
(within .1 of) the sphere. I guess that it is becoming the resolution issue.

18 Sept 09

Implement potentialatpoint. Tested in padvnc section. Tests in ccpicplot
are confusing because there the potential is measured outside the region.

Now we need a data structure for storing for each time step:
    maxwellforce(3)
    pressureforce(3)
and maybe other field combinations such as
    charge (integral of normal field over surface).
For each object that we are tracking.

This bears some similarities to the ff_data structure (e.g. maxsteps, obj). 
However, that data structure is rather opaque. It does not seem very 
helpful to load it up even more. Rather, it would seem sensible to 
define some linear arrays of length maxsteps (or small e.g. ndims transverse
dimension arrays).

Currently we write the whole potential phi out at the end of run. To
write it out every step would be rather crazy in terms of size. I can
imagine that we might write out some sample of phi. The maxwellforce and
pressure are more manageable. 

It seems reasonable to assume that we are going to want the field quantities
only for objects for which we are doing flux tracking. That would argue
for putting the field quantities into the ff_data or at least use the
nf_map information. Perhaps that's the compromise: use the nf_map info,
but create different data structure. This might then be
    stressdata(nsdatas,nf_obj,nf_maxsteps)
since nf_obj=5, and ndatas is in the ball park of 10, this is
an array of size 50,000 for maxsteps=1000. This is negligible. 
In the end probably easier to have separate names fieldforce, pressforce
etc in 3dcom.f.

Having made that decision it is natural to put the output into the flux
file. At present, we are just writing the whole fieldforce, pressforce,
and charge traces, regardless of used length. 

In addition we need the surfobj structure for each object being tracked.

Implemented and got to run sphere treatment only. Not yet debugged to
my satisfaction. We need some tests. 

I'm a bit confused about the flux tally type/number. Check reading codes.

19 Sep 09

The first byte of the 3dobject input (otype) is the object type 1: sphere.
Second byte is currently being used for special boundary conditions.
1:256 is used for setting phi to zero outside instead of continuity. 

The input ofluxtype is used to indicate the number of flux quantities
to be tracked, currently <=5. I don't know if this is the best use of it.

Although it is a bit inconsistent to track stresses if we aren't tracking
momentum fluxes (tally 2-4) probably it is reasonable to say that we 
track stresses if obj_geom(ofluxdata,i).ne.0. 

Got worried about the analytic testing on the 0th step. This gives
a big error unless lambda debye is very large. Yet it is a call with laddu
equal to zero. So the solution ought really to be a good vacuum solution,
since charge is zero. Checked out with older version of sorrelaxgen.f.
Same result.

I figured out that the change is in the boundary condition at the
outer sphere. Orbitinjnew.f sets it different from the natural vacuum
condition in geominit(myid). So this is actually a vacuum solution,
but with effectively a potential offset that allows the outer BC to be
satisfied.  The analytic comparison does not account for this
potential offset at infinity. To make a proper analytic comparison, I
would have to pass also the value of phi_infinity. The boundary
condition is a phi + b phi' + c =0. The values of abc are available in
obj_geom(oabc,2). A vacuum solution phi= p0/r + pi satisfies this BC if
a (p0/r+pi) + b (-p0/r^2) + c =0. Which means that the solution for pi 
must be 
     pi = -p0/r - (c-b.p0/r^2)/a 
but then p0=(phip-pi)*rc. So really
    a ((phip-pi)*rc/r +pi) - b(phip-pi)*rc/r^2 + c =0.
So 
   pi( -a rc/r + a + b rc/r^2) = -(a phi*rc/r - b phip*rc/r^2 +c)
It works.

Found that fillinlin works slightly better if we always use the field
at point to determine the gradient of the fit, rather than doing a
full gradient fit to adjacent points. Sort of makes sense.


25 Sep 09

Implemented calling of tallyexit for all boundary crossings. Only those
that are being tracked actually do anything. But now we can put a surface
in the particle region and track the momentum flux across it. Therefore
we ought to be able to compare the force components for different objects.

7 Oct 09

Getting back to this. I had updated fluxdatatest to examine the force 
integrals. It shows that for l=1, the total force is very similar at 
r=2, 4, but not the same at r=1. So there's a discrepancy not yet sorted
out. 

Run case with -l10. in which the dominant force is particles. We do
not appear to have momentum conservation. The different radii give
very different force totals. Hmm. Also, there are some Tallyexit no
intersection errors. The force at r=4 is almost twice that at r=1.  We
ought to have a delicate balancing problem with field force and
particle force. This is not working properly.

Case with (nearly) zero potential on sphere and -l200. Gives a small force
that is equal for all radii, as expected. About 3.68 for partforce. So 
momentum accounting is working when there are zero fields. 

At present, partforce is in units that are normalized momentum per sec
per rhoinf. Fieldforce is in units that are grad(\phi)^2 times
area. pressforce is in units of nT times area (I think).

Multiplying fieldforce by debyelen**2 there is a balance. But it seems
to be closer to 0.5 times the fieldforce. I need to check the coefficients. 
There is also some 9% discrepancy at the inner boundary, which needs to be
investigated. It sure looks as if 0.5 is required.

8 Oct 09

Partforce was not being reduced. Therefore for two processes, the partforce
was half of what it should have been. I think this probably explains this
factor of two discrepancy. But we still need to sort out the comparison
with sceptic.

The particle force for ccpic is  90.14934, 119.96814, 175.66409, 195.43149
for radii, 1., 2, 4, 4.9.
The particle force for sceptic is 87.89902, 196.57240 for radii, 1., 5. 
Thus, we basically have agreement in the particle forces.

The field forces are quite different for the two codes. However, this is
probably because of the different phi boundary conditions at the outer
boundary. For large debyelength like this the cutoff is outside the boundary
so the code is not really determining the force, it is being imposed by 
the BC. The fact that the totals for ccpic 224.91051, 208.83559, 206.43303
197.83185, more or less agree shows that we are pretty much conserving
momentum in the code region. But these are very different values from the
ones being given by sceptic: 540.51074, 545.98761. 

It hardly seems worth the effort of trying to sort out what the
differences are. More useful might be to impose an outer boundary
condition that corresponds to a known field (e.g. uniform) for which
the EM force is therefore known. And verify that we get the right 
field force.

To do this through the BC would require conditions whose normal component
varies over the surface. Might be tricky. Perhaps we could simply add
a uniform electric field to the field calculation. It will still satisfy
the same poisson equation. It won't then have the total potential constant
on any surface (including the inner). Probably does not matter. 

Implemented this external field. The inner surface error remains. 
The error on the inner surface with our current mesh (32^3) is about
halved at a position r=1.05, and does seem to make a finite transition
to the conserved region.

Probe potential 10. extfield.01
Field,part,press forces: -2937.02954     4.97746     0.00000 -2932.05200
================== End of Object 2 ->  3
Field,part,press forces: -2996.83130     3.71953     0.00000 -2993.11182
================== End of Object 3 ->  4
Field,part,press forces: -3087.99609     1.97120     0.00001 -3086.02490
================== End of Object 4 ->  5
Field,part,press forces: -3081.75708    -3.63166    -0.01428 -3085.40283

Probe potential 1. 
================== End of Object 1 ->  1
Field,part,press forces:  -294.28650     0.67058     0.00337  -293.61252
================== End of Object 2 ->  3
Field,part,press forces:  -300.21112     0.44164     0.00290  -299.76657
================== End of Object 3 ->  4
Field,part,press forces:  -308.82449     0.03669     0.00001  -308.78781
================== End of Object 4 ->  5
Field,part,press forces:  -307.54568     0.12403    -0.07416  -307.49582

I find that the charge is unequal in exact proportion to the fieldforce.
In other words, the main force error is arising from the integral of the
normal field over the surface, which is not giving exactly the right answer.
This is understandable, given the approximations that are made close to 
the sphere.

Thus I think we have the force calculations working, but they are showing
that positions very close to the sphere suffer from the approximations that
are involved in the field extrapolation. The charge on the inner sphere
is systematically underestimated by the extrapolation. 

Since field is accurate only to a linear approximation, and we are
using a mesh that has 16 across a radius of 5, we have a mesh spacing
of about 0.31.  The inner sphere has radius 1, so the spacing is about
1/3 of the sphere radius. It is perhaps not surprising that there are
substantial symmetric errors in the field that affect the force.
I'm not sure there's really anything one can do about it.

Fiddling with fluxdata to see what I should do about tallyexit errors.
They seem to be rounding. The tests I have don't make much sense.
Changed to a more sensible test and crossing direction calculation.


15 Oct 09

Playing with comparing ccpic with sceptic, I realize that I still
haven't really got good solutions for storing and analysing the ccpic results.
Sceptic postproc has contour plots of n, \phi. Line-outs, flux density plots
etc. ccpic has phiexamine which can do plots of \phi, but n is not well served.
Also, fluxdatatest has some plots but as yet no output useful for comparing
with other data. 

Do we need density to be output separately from the particle data? We can
certainly calculate density (instantaneously) by reading in all the particle
files. But it might be pretty cumbersome doing the accumulation. Actually
chargetomesh seems just to accumulate, not to initialize. Therefore we
just need to call it for as many particle files as exist. However, the particle
data files do not contain the mesh structure.

Easier route just reproduce the phiexamine structure as
denexamine. Including writing out the density. This works. But there's
a serious problem in that the noise level of the density is very high,
at least on 32x32x32 with default particles. The noise level is of
order 1. This is, of course, no different from what one might observe
with graphics during the run.  The files for density (and phi) are
about 15 times smaller than one file of particle data (for 52k
initialized particles). So it might be worth considering outputing multiple
steps. One might also do time averaging. Implemented averaging and output
the decaying average with 100steps default. Even so with the standard 
settings the density plot is pretty noisy, and with 2 processors, added
steps and 200k particles there's still a considerable noise level.

16 Oct 09

I've plotted the density as a function of r for individual mesh
points. It would reduce the scatter if the 2-D case were gathered together
into bins that correspond to theta angles.

20 Oct 09
Completed additions to phiexamine, denexamine, which bin things together
and then print them out. 

Created a general purpose plotting routine that can read the print out from
ccpic tools and from the sceptic tools. Compared the -p-10, -l1. cases.
Excellent agreement with potential and density right out to the edge.
See in src/plotting/.

Conclusion.
__________

I declare success with the comparisons between ccpic and sceptic in
moderate debyelength cases. They have excellent agreement on flux,
potential, and density for v=0 cases. Forces for flowing cases need
more study because of the finite-domain effects. Also for -l.1 there
is a significant discrepancy at the boundary. The potential is too
small in ccpic and the density is rapidly trending up towards 1. It
looks as if there's a problem in the boundary condition. However, it
is hard to be sure because the discrepancies in average values are
several times smaller than the instantaneous fluctuation levels.


Next Steps   30 Oct 09
---------- 

I think we need to move away from the spherical region and use cartesian
region out to the boundary. We need to work on such facilities as periodic
boundary conditions, or at any rate rectangular BCs at the computational edge.


-----
Did some housekeeping to clean some testing code out to the subdirectory.
Also rename fluxdatatest to fluxexamine. 


11 Nov 09

Renamed bbdyroutine.f to bdyroutine.f. Cleaned it up so that it is easier
to understand the boundary setting routine. 

'Free' rectangular boundary conditions. Might consist of putting the
logarithmic derivative equal to 1 or 2 or something. But on a
rectangle the logarithmic derivative is not so obvious. If r.grad(u)/u
is the radial logarithmic derivative D, then the component in a
particular cartesian direction x is grad(u).x = D (r.x) u/r^2, so
presumably we should set grad(u).x accordingly. Notice the 1/r^2
dependence involves the orthogonal coordinate values. Consequently, one
needs ways within the bdy routine of knowing what the mesh positions are,
or else some other more direct way of passing the setting information. 

Implemented that in fieldtest and bdyroutine bdysetfree. But needs a
different geomtest file: geomtest2.

Also geomtest3 has two separate spheres.

24 Nov

We need now to deal with the outer boundary for particles, when there
is no explicit bounding object. Particles contribute their weight to
the second row/column until they reach the exact edge node
position. Consequently, we ought to consider the particle region to
extend all the way to the edge nodes, if we wish to get the second row
density correct. The contributions to the edge nodes is half what it
ought to be because there are no particles outside. However, the edge
node density is never used, and indeed the volumes of the edge nodes
are not currently set. There will never be an overflow of index for
charge assignment beyond the edge, provided that we never go past the
edge nodes.

In summary: the default particle boundary is the edge nodes. 

The routine insideall is used by padvnc. This approach needs to be 
extended to include the default outer boundary. Currently the mesh is
set using xmstart/end, but there are no switches to change those. They are
+-5. Also rs is in the common, and used for a few things. We need to change
to using the real mesh size, which is set by meshconstruct. 

Relevant routines.  

	 readgeom reads in objects

	 geominit sets rc from first, rs and rsmesh from the second object
	 assumed to be a sphere, and overrides some BC, from orbitinjnew.f.
	 This will be changed when we are doing away with outer sphere. 

	 meshconstruct accepts xmstart(3) xmend(3). 

Implemented command line switches to change xmstart/end. A problem emerges
that stored3geom is not changed, even though the mesh has changed. Because
of that ccpic thinks the volumes are what they used to be, and does not
recalculate. I think we need to store the mesh data in the stored file. 
Implemented storing and checking of ixnp. Fixed.

Crunch of geominit prevented by softening the oabcset error. 
Then pinit hangs. So we can fix this. We need to fix linregion and
then fill up between mesh limits. Actually I don't think there's any
need to edit linregion. I find that the issue is that rs is apparently 
zero. Anyway, dispense with using it by making pinit fill the whole mesh
region with particles in so far as linregion is true.

Now we get as far as stepping and find Relaunch NaNs. 
Found that problem due to rs being reset to zero, and fixed.

It seems as if the best approach to keeping particles within the mesh is to
use the partlocate call. This is tricky because at present partlocate
is called by chargetomesh. But this separation just makes the code harder
to understand. The best thing is to get all the partlocates into padvnc
and ensure after every move, we do the partlocate so the position is in
fact known. To remove partlocate from chargetomesh, we perhaps need
to call it for all the particles when they are initialized. We do, but
somehow this is still not working.

30 Nov 09
Was not setting the pointer iinc properly. Fixed, but now the calculation
of the pointer to position (iu) in partlocate appears to be redundant.
It is not used in padvnc, only in chargetomesh, where it had to be 
done separately. So purge out of partlocate the calculation of the 
pointer. Simplifies and reduces argument count.

Now we need to implement some convenient way to detect when partlocate
overflows the interp. Perhaps a specific large iregion. No. Add a logical
argument. Then test this. Implemented.

1 Dec 09

Putting Mesh information into the geometry file.

It becomes clear that we ought to put the mesh information together with
the object geometry information. Presumably in the same input file.
A general way to specify the mesh information would be to provide for 
each dimension two flexible length arrays:
imeshstep: 1, ms2, ms3, ..., msN
xmeshpos:  xm1,xm2,xm3, ..., xmN
which determine the mesh number corresponding to mesh position.
Thus imeshstep=1,32; xmeshpos=-5.,5. corresponds to a uniform 31-step 
mesh 1-32 over domain -5. to 5.
imeshstep=1,12,20,32; xmeshpos=-5.,-1.,1.,5. is mesh with 3 domains 
1-12 covers -5. to -1.; 12-20 covers -1. to 1.; 20-32 covers 1. to 5.
An imeshstep value of 0 indicates no more array. 

To convert this to the meshcom ixnp and xn values requires that the 
dimensions be known in the correct order (x,y,z,...) so one has to set 
them all (perhaps to defaults) and then do the conversion. 

Implemented structure and default initialization for imeshstep and xmeshpos.
Implemented new meshcontruct to use that default. Gives same answer.

Format in the geometry file will be for dimensions 1,2,3:

91/2/3, ms1,ms2,...,msN,0,xm1,...,xmN,0

Seems to work.

Now trying to rationalize things, we find a logical problem. 
iuds is now set by meshconstruct. That has to be done after we 
have determined myid. But the initialization of sormpi discovers myid,
and it needs to be passed the value of iuds. 

I think this requires me to get rid of the early initialization, which 
was anyway very artificial. Instead I can use a routine getmyid to
get it. This does not seem to need any changes to sormpi. Yes it works
nicely and easily. This is a much better approach. Also got rid of
xir the point assumed to be in the region. Now just use linregion.

Don't seem to have gotten to the bottom of the difference with or
without stored geom. There is a call to ran1 that ought to reset it.
Thus it's not clear that the difference arises from the random
numbers.  However there did seem to be a difference in pinit: 100776
when set, vs ntries= 100414 when read back. So if the random number
generator is not this issue what is? 

Ahh! It WAS the random number generator, because pinit was erroneously
using the old rand instead of ran1. I use rand() _only_ in volintegrate
because of the big cost there of number generation. So now fixed. 
We get the same answer whether or not the stored3 read is successful.

Today
_____
We implemented mesh specification in the geometry file.
Rationalized some of ccpic, notably the mpi initialization. 
Fixed a bug in the pinit call and stored geometry differences. 

Discovered a bug in slicegweb to do with non-equal side domains. Actually
it's a feature. Hidweb is set not to rescale until done explicitly.

2 Dec 09

Now we need sensible reinjection for the whole rectangular domain.
Issues include
1. How to account for any non-zero potential at the domain edge?
2. Are there many interesting situations where periodic reinjection
   would be useful?
3. How to deal with situations where some part of the boundary is outside
   the particle region. 

One thought about 1. is that one might use reinjection at infinity followed
by integration in an approximated spherically symmetric potential. The same
approach as in orbitinject. One would have to use an impact parameter range
that extends to the boundary of the cuboid region. This is substantially 
larger than the largest sphere that fits within the region. Radius is larger
by sqrt(3), so impact parameter area is larger by 3. Consequently there
would be substantial (factor 3) inefficiency in the process. It might also
be tricky to determine whether the orbit intersects the sphere. 

Probably the use of the Green's function solution for this purpose is
overkill. Certainly it will not arise naturally for the actual boundary
conditions of a cube. Other issues arise if the domain is a very elongated
cuboid. Then the spherical symmetry is a bad approximation (perhaps). But 
in any case the injection inefficiency will rapidly increase. 

Overall, it seems unlikely that an orbitinjection approximation will have
sufficient accuracy to warrant the trouble. A straight approximation of
cartesian reinjection of the infinite-distance distribution function ought
to be pretty easy, but will not account for external acceleration. 

Options for ad hoc adjustment of the injection to correct for external
acceleration include 
1. Adding normal energy. (Enhanced normal inward velocity).
2. Adding total energy. (Keeping the velocity direction fixed.) 
3. Some other directional assumption.

Should one change the distribution across the boundary? If one were
in a quasineutral regime, then perhaps the density ought to be weighted
by the Boltzmann factor. One way to do that would be to roll the dice
and reject the particle if above exp(\phi), where phi is the local
reinjection potential. This would appropriately contour the weight. 
However, it would not be correct for a long-Debye-length situation. 

 Periodic Reinjection. Even if periodic reinjection were used for
particles escaping the domain, one would still have to deal with particles
lost to the objects, and their reinjection. Therefore periodic reinjection
does not actually solve the problems of reinjection. It could be an
option for some (or all) of the external boundary surfaces. It could be used
whether or not the potential boundary conditions were periodic. 

The first thing to do is to implement a shifted maxwellian calculation.
If 
   f(v) = C exp[-m( v - v_d)^2/2T]
then the total number with positive v-direction is
   N_+ = C \int_0^\infty exp[-m( v - v_d)^2/2T] dv
       = C \int_{-v_d/\sqrt{2T/m}}^\infty exp[-t^2] dt \sqrt{2T/m} 
       = C \sqrt{\pi}/2 \sqrt{2T/m} erfc(-v_d/\sqrt{2T/m})

Since erfc(0)=1, erfc(-\in+fty)=2, the normalization is such that \int f dv
= 1 if C \sqrt(\pi}/2 \sqrt{2T/m} = 1/2.

The flux density above velocity v_0 is 
    F(v_0)  =  C \int_v_0^\infty exp[-m( v - v_d)^2/2T] v dv
       =  C \int_{t_0-t_d}^\infty exp[-t^2] (t+t_d) dt {2T/m} 
(putting\ t=(v-v_d)/sqrt{2T/m})
       =  C2T/m [-exp(-t^2)/2 - t_d \sqrt{\pi}/2 erfc(-t) ]_{t_0-t_d}^\infty
       =  C T/m [exp(-(t_0-t_d)^2) + t_d \sqrt{\pi} erfc(t_0-t_d)]
       = \sqrt{T/2m} [exp(-(t_0-t_d^2))/\sqrt{\pi}  +  t_d erfc(t_0-t_d)] 

Approach: 

Decide the face to be injected at, by random choice weighted by F x Area. 
Decide the position on face by random 2D rectangular.
Decide the tangential velocities using gasdev (drawing from maxwellian).
Decide the normal velocity by drawing from the above distribution:
       how do we do that distribution draw?

We need to be able to invert F(t_0). 
That's not obviously feasible analytically.
Seems as if we need to form a numerical table of F(t_0) and interpolate
the inverse lookups. This would require six tables: one for each surface.
But at least they only have to be one-dimensional. 

7 Dec 09

Created integrator code. Programmed a fairly smart integrator
subroutine cumprob. 

8 Dec 09

Programmed reinjection code.
Programmed testing/creintest testing code.
I find that the histogram reinjection distribution shows up on the 
testing distributions. Thoughts about fixing that include the idea
of some higher-order interpolation. However that is not straightforward
at the edges, where it most matters, because the inverse cummulative
probability curve has infinite slope there, so how does one fit that? 
It's not clear that a parabolic interpolation will work. If I add a 
further node outside the real range, which has a value much (by some
chosen factor) beyond the bottom. This might work. On the other hand it
probably won't be faster than simply using a finer mesh for the inverse
cumulative curve. Using 1000 arrays seems to give a pretty good result.

9 Dec 09

Constructed (finally) a four point cubic interpolation routine. Checked
numerically. However, when used in creinject, it makes things worse. There
are spikes near the last but one point. Don't know why. Perhaps it means
that the interpolation is non-monotonic (or nearly so). This is not 
completely crazy, since a parabolic interpolation of a sharp corner from
nearly flat to nearly vertical will produce overshoot. This may be
the reason. At any rate it appears to confirm that it is very tricky to
improve the inverse-cumulative approach by higher order interpolation. 
I think it comes down to needing either to use a decently high number
of nodes, or not using the inverse cumulative approach. The old finvtfunc
approach was not bad. Settled for 1000 array for now. Linear interpolation.

10 Dec 09

Finished creintest.f to include testing of position. Seems correct.
Now we need the cartesian versions/equivalent of nreincalc, geominit,
rhoinfcalc.

Implemented nreincalc and rhoinfcalc using 
flux = ninfty sqrt(2Ti/pi)[A_1+A_2
       	      +A_3{exp(-td**2)+sqrt(pi)/2 t_d[erfc(-t_d)-erfc(t_d)]}]
and including a phirein correction factor of
         cfactor=smaxflux(vd/sqrt(2.*Ti),chi)/smaxflux(vd/sqrt(2.*Ti),0.)

(Not yet particle energy correction.)

geominit is specific boundary setting for this geometry. Null for now.
It compiles.

It would be nice to have geometry files ccpicgeom.dat that were automatically
invoked for the different geometry assumptions. Might be possible to 
replace ccpicgeom within ccpic with a value that depends on the REINJECT
case under consideration. Better to build a link into the make structure
and change it according to whatever reinject we are using. Did that and
cvs added geomsphere.dat and geomcubic.dat. Cartesian reinjection is
actually not yet working. We aren't reinjecting anything. 

Made some progress on getting things to run. But not to the bottom of it
yet. denexamine and phiexamine don't run quite right.

11 Dec 09

Ok the main problem seems to be that the sign of the normal velocity
is wrong and so particles leave immediately. Fixing this, there is still
a small problem with sign of particles because the inverse cumulative
probability extends slightly across zero, presumably because of the
integration and interpolation. Consequently there are a few wrong signs
still. I suppose we could fix this artificially. Did that.

Now things run with sensible particle numbers but there are some very 
strange peaks in the density that make no sense. And seem to be in straight
diagonal lines for some number of cells. This was because I used idum as
the argument for ran1. It needs to be >=0 for standard call. And idum
was uninitialized.

Finally got some runs that look plausible, although currently with
potential set to zero at the boundary (the current default). Actually
no. The potential is set to zero on two dimensions and something else
in the third.

Cartesian boundary flux tests. 

32^2 grid ./ccpic -ni200000 -dt.025 -da5. -s500 we get rhoinf 212.67125
Flux density*r^2, normalized to rhoinf   3.9655340. This is to be compared
with 3.997 with sceptic or with 60^2 grid. Broadly we are getting the 
right answer. Close to OML (4.388 for -p-10).

With -l.3 we get: 2.8120570 c.f. 2.77 with sceptic etc. All pretty promising.
I see that the printout from the run is different from what fluxexamine
prints out 2.8006816, but close. Not quite sure why. 

With -l5. I don't expect a good result, because of the BCs. Get 4.795. 4.777.
C.f. 4.395 sceptic. This is 10% in error. Not a great result. 


Summary
-------
Code seems to be working with outer injection boundary at the mesh edge.
Gives promising agreement with prior runs at -p-10, even with relatively
small (32^2) mesh (and inappropriate boundary conditions). 



14 Dec 09

Implement boundary setting of slope to -(1.+r/\lambda_s) with lambda_s
currently set as the linearized value. This works reasonably well to
maintain the phi contours circular at intermediate values of debyelen,
and presumably is close to the right answer for large debyelen, although
arguably it ought to be closer to -2, not -1 in the far distance. 
At small debyelen, the value of the slope doesn't much matter.

Then ./ccpic -ni200000 -dt.025 -da5. -s500 gives
3.9668663. Essentially no change. -l5: 5.2031398 yikes! Too large by
big factor (rhoinf=135.45, 8818). Try logarithmic slope of -1.: 5.3926058
(rhoinf=122.93, 8330) . Also very large. Repeat old bdy run: 4.7955675 (this
forces phi to nearly zero at the boundary rhoinf=161.79, 9714).

This is rather worrying. It looks as if one gets a very bad result if
the boundary potential is non-zero. I think this is because I don't
yet correct the energy of the reinjected particle by phirein (or some
other local potential correction). Scale velocity by
(v^2-2.averein)/v^2. Initially I used phirein and got problems with
overrunning the diagv. This is because phirein is not equal to the
averein of the prior step in the middle of the particle advance.

Now with -l5 get 3.6797631, rhoinf 149, 6897. Too small by a comparable
amount. -l1 3.8620880 214.66699 10418.302
-l.3 2.8107293 219.31303 7746.276

What I think this experience shows is that the cartesian geometry is
not very good for treating an isolated spherical object. Not because
of the cell shape but because there are no convenient approximations
for the boundary condition that allow one to take credit for the 
isolation of the object, and not have to go as far out with the domain. 

If the domain were part of an array, so that periodic boundary conditions
apply, then this weakness is turned into a strength. Probably periodic
conditions ought to be investigated. 

To get into a physics problem soon, we could address the wake issue. This
would also lead to exercising different meshes and perhaps even differently
spaced meshes.

For now, commit. Getting back to sceptic machine we don't get the correct
answer. This proves not to be a function of the cvs version because getting
it to ihhutch gives ok result. I notice that although there's a gfortran
on sceptic.psfc mpif77 appears to be using g77 instead. This might
conceivably be the cause of the major problem. It is that face of the
grid is being significantly depleted of density. Perhaps there's a problem
with reinjection, e.g. that no reinjection is occurring there. In any
case it is probably worth pursuing the problem on sceptic.psfc to figure
out the portability of the program. 

Prove that it really is the mpif77 version by using the -f77=gfortran 
switch. That gives correct answers on sceptic.psfc, although not _exactly_
the same as on ihhutch. At least the major error has gone away. The version
of gfortran is not the same. It seems there are very small differences. 

It is by the way noticeable that the g77 version runs very substantially
faster on sceptic.psfc. Maybe as much as a factor of 2! That's pretty 
significant. 

The depletion of the g77 version is at the -5 end of dimension 3. It appears
gradually with steps, which suggests that perhaps it is associated with 
reinjection; but it could be other things. Since we don't have obvious 
code errors, it is not completely obvious how to proceed. There is no
evidence that creintest gives any difference. 

A force shows up in the faulty version. And eventually a significant
flux asymmetry. There's definitely a particle flux towards negative z,
which is consistent with a reinjection deficit there.

The faulty version shows substantial n_part degradation after 100 steps:
0096  53 3.758| 0097  54 3.228| 0098  53 3.565| 0099  53 3.767| 0100  54 4.023| 
nrein,n_part,ioc_part,rhoinf,dt= 2394  76849  99993    94.749     0.100
compared with
0096  53 3.882| 0097  53 4.035| 0098  53 3.875| 0099  53 4.144| 0100  53 3.888| 
nrein,n_part,ioc_part,rhoinf,dt= 2394  91379  99984    97.415     0.100

Thus there really does seem to be a loss of particles. Got g77 installed
on tp400 and then I can reproduce the g77 problems there (and its speed!)

16 Dec 09

The reinjection diagnostics do not show any asymmetry problem in cos\theta.
Nor are any warnings given about reinjections outside of region. Therefore
there's a bit of a puzzle as to where to look for the error. 

Perhaps we need a way to examine the particle data and plot
distribution functions etc. This would involve reading back the
partdata and then binning it by velocity, for some selection of
cells. One ought to allow the possibility of reading more than one
file worth of data. But it is not straightforward to provide
sufficient storage for the actual data of many particle files. Each is
potentially quite a lot of megabytes. Also the file reading apparatus
might break unless we shuffle stuff. It might be better to reread the
files if a new selection were required. That would be reasonably quick
if the files were in disk cache memory. So the process would be
1. Read a file .00N 2. Bin its particle data. 3. Repeat till all done.
This could be done on a selective basis (perhaps). 

Created partexamine code to do this. It shows that in the depleted region
at the bottom z end, the distribution function is (approximately) 
one-sided. Also, density goes to zero at the edge, not half, because
the particle bins can be chosen finer than the charge mesh. And starts to
drop at about -4 (which is about 3 cells from boundary) after 5 steps.
This effect can be seen propagating inward as one increases the number of steps.
If we set the number of steps to zero, there is no depletion. 
Setting dt to a small number we can see more clearly the depletion region.
It is the bottom _half_ of the bottom cell. 

Putting a test hack padvnc call outside the main loop and using -s0,
I find that it is definitely padvnc that is causing the depletion.
It seems that x_part(3) is corrupted on entry to partlocate.
Also xpart(6) is corrupted and seems to be the source of the corruption.
Yes this is coming from the getfield, and is corruption of the value.

Mesh Domain Boundary Pointers.

I think the problem is in field interpolation at the boundary. We are
not (it seems) setting the boundary pointers in such a way as to
prevent trying to get potential from past the end of the mesh for
interpolation.  Actually I can now see the problem from within -gt
tests.  I think this problem would be fixed if the boundary points
(which all have pointers to data) all returned a region that was
unique. getfield would then never allow itself to use those boundary
points as the centers of extrapolation. As a result, it won't grab
data beyond the boundary.

Implemented this different boundary value as -1. Also fixed the
text3graphs to cope. Then found a logic bug in the getfield code that
did not treat the whole thing appropriately. Found a bug in the
solu3plot routine that overran the mesh domain at certain angles and
hence gave nans returned for the field. Have not made getfield
completely bullet proof against incorrect fractions in call, because I
was nervous about slowing it down. But now things ought to work.
Ok. No obvious evidence now that we are broken.

There's an inconsistency in rhoinf, n_part, and nrein. With multiple
processors, we initialize 100000/numprocs particles by default. Which
is probably a bad choice, but then also we seem to be injecting more
particles. The defaults are as follows:

17 Dec 09

c Fixed number of particles rather than fixed injections.
      ninjcomp=0
      n_part=0
c Default to constant rhoinf not n_part.
      rhoinf=100.
Then in initialization we do
c      write(*,*)'Doing nreincalc',n_part,rhoinf,dt
      if(n_part.ne.0)rhoinf=0.
c Set ninjcomp if we are using rhoinf
c This does not work until after we've set mesh in cartesian.
      if(rhoinf.ne.0)call nreincalc(dt)
Nreincalc does:
c Correct approximately for edge potential depression (OML).
      chi=min(-phirein/Ti,0.5)
      cfactor=smaxflux(vd/sqrt(2.*Ti),chi)/smaxflux(vd/sqrt(2.*Ti),0.)
      ninjcomp=nint(rhoinf*dtin*cfactor*flux)
      nrein=ninjcomp
      if(ninjcomp.le.0)ninjcomp=1
      n_part=rhoinf*volume/numprocs
where flux is normalized flux-density times area. 
Then we call pinit which does:
      n_part initializations (per proc) and then
c Initialize rhoinf:
      if(rhoinf.eq.0.)rhoinf=numprocs*n_part/(4.*pi*rs**3/3.)
which should be changed, for cartesian volume, incidentally. 
When nrein is reduced, it is **summed**. 
rhoinfcalc is called each particle advance cycle. nreincalc is not.
rhoinfcalc does:
      if(nrein.ne.0)then
c Calculate rhoinf from nrein if there are enough.
c Correct approximately for edge potential depression (OML).
         chi=min(-phirein/Ti,0.5)
         cfactor=smaxflux(vd/sqrt(2.*Ti),chi)
     $        /smaxflux(vd/sqrt(2.*Ti),0.)
         rhoinf=(nrein/(dtin*cfactor*flux))
      else
         if(rhoinf.lt.1.e-4)then
c Approximate initialization
            rhoinf=numprocs*n_part/(volume)
            write(*,*)'Rhoinf in rhoinfcalc approximated as',rhoinf
     $           ,numprocs,n_part
         endif

Thus nrein during iteration refers to the total reinjections, and
rhoinf to the total density including all processors, but ninjcomp
refers to one processor's injections, as (of course) does n_part.
I want the default injections to be rhoinf=100*numprocs really. 
But in any case, the initializations should be such that things are
consistent. ninjcomp is only ever set in nreincalc, i.e. only ever
during initialization.

Need to make a decision about how defaults are command lines work with
different processor numbers. I want adding processors to add particles,
which means that rhoinf goes up. Thus if I specify rhoinf on command line
or default the actual rhoinf is not this value it is this value times
numprocs. This is correctly handled within rhoinf at present if we 
change the meaning of rhoinf from being per processor to total when
we call nreincalc. Hence we change it to 
      ninjcomp=nint(rhoinf*dtin*cfactor*flux)  ; per processor
      nrein=ninjcomp*numprocs		       ; total
      if(ninjcomp.le.0)ninjcomp=1
      n_part=rhoinf*volume		       ; per processor
      rhoinf=rhoinf*numprocs		       ; total
This appears to work consistently. But ought also to be changed
in orbitinject. Did that.


Discovered a problem with testing/fieldtest. Points on mesh edge are
having their phi values changed, apparently to match the derivative
across the object mesh boundary. This presumably is not really needed.
Actually this happens also in the orbitinj version of ccpic. Although
you need to look harder to see it. Basically this is a problem with 
the bdyset routine, which ought not to do gradient setting in these
situations. 

Timing. 

G77       ./ccpic -s200 real    0m31.220s
Gfortran  ./ccpic -s200 real    1m10.779s

This is a really quite amazing discrepancy. Compare with 

./ccpic.gfortran -s50 -ni500 (negligible number of particles) 0m16.886s
 time ./ccpic.g77 -s50 -ni500  0m5.003s

 time ./ccpic.gfortran -s50 -ni500000 0m50.750s
 time ./ccpic.g77 -s50 -ni500000  0m45.196s
Hardly any discrepancy here.

Conclusion: Gfortran is taking 3x longer for the poisson solve, whereas
the particle mover is pretty similar for both compilers. (These are all
on concentric spheres.) The number of iterations is pretty similar.

Profiling gfortran shows time is dominantly in sorelaxgen mditerate,
bdyslopescreen, r2indi. There are 20 Million calls to bdyslopescreen.
Seems probably too much. For g77 bdyslopescreen and r2indi are also
the major players (though not quite so dominant. 19M calls. 

Is this reasonable? bdyset is called for each sor iteration (I think),
average 60. For each step 50. For all boundary points 32x32x6faces.
Total roughly: 18M. Yes this is a reasonable number. Seems as if this
boundary setting is not negligible, as I thought it would be. 
For 32^3 mesh the number of boundary points is only roughly 1/5th of total
number of points.

First thing. I don't think we should be calling the bdyset routine 
every step, only every _other_ step for the red-black routine. 
Second, one might argue that for the PIC the boundary routine ought
to be called only each particle step. 
Third, this is really scary because bdyset does not get shared out
between all the processes. So parallelism won't help. Scaling will
be terrible. 

I notice that the total number of seconds isn't adding to the elapsed
time. And actually the amount of time (about 2 seconds) isn't much
different. Maybe I'm not really profiling where the real time is being
spent. If that's so, then I'm probably not really getting to the bottom
of what is taking the time. I think I ought to be getting library routines
that allow profiling but perhaps I'm not.

On sceptic results are similar except the seconds count is 7 not 2.
Still the calls are more or less in the same place. Same with g77 and
with gfortran. It might be nice if one could compile without MPI, 
which might be some of the problem. 

Installed valgrind. The picture it draws is rather similar, but it detects
that there's a significant amount of time spent in the exp function.

There's no question that g77 is way faster -- up to a factor of 3 --
than gfortran, no matter what optimizations I use. Both are 64
bit. They use the same libraries except for libgfortran vs libg2c.
A real puzzle.

Profiling does not track libc libm without the profiling libs.
Installed them libc_p etc. But then can't get a program that does
not crash. This is a known bug:
https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/193487

In general the flags required are  (-g) -pg -static-libgcc -lc_p
Installed gcc-4.1 which is said to work with these. But that does
not seem to propagate to gfortran. So I haven't really made progress.

Found a small time saving in sorrelaxgen where the parity change is done.
Saves about 10% of time in this routine, but not much overall. 

21 Dec 09

Made a unique version of accis and added to the repository as a subdirectory.

Found that even g77 version runs slower on cmodws48 etc. 
These machines have 
cache size      : 2048 KB
whereas both sceptic and ihhutch have
cache size      : 6144 KB
This is probably the reason. Cache misses may be the secret to understanding
the performance.

Tried reducing Li in ccpic. No significant difference. Reduce partcom
n_partmax to 100000: no significant difference. Well I haven't proved
that it is cache misses.

Ran some cases to see if I can get wake structures similar to those
in sceptic. With +-10 mesh extent, and -l3, -t.1, -v1., p-2, a trailing peak
can be clearly seen (but trough seems past the end. 
Generally the results seem qualitatively consistent with what I saw
with sceptic. 

Ran some rectangle cases. Also don't show negative dip in wake.
Ran an asymmetric case -5,+15. It shows a tiny dip in the wake. 
All this is consistent with sceptic. Now in 3-d.

23 Dec 09

Found problem with cumprob. There are cases when the integral is zero.
We need ways to cope with those, and also with cases where the integral
is too small to be significant, but still appears non-zero. Implemented.

30 Dec 09

Ran a number of cases designed for wake studies on sceptic.psfc.
These show quite oscillatory wakes (unlike the sceptic case). Using 
-l10 and -l20 one gets quite similar scaling with domain size up to 
400 long (50 wide). However, it is very clear that the size of the first
potential peak never gets much larger than .15, no matter how strong the
perturbation (charge \propto \phi x r scaled to lamda_debye). It seems 
as if I can actually reproduce the oscillatory wakes of the linearized
cases, but only with correct amplitude if that amplitude is less than
0.1(Te). It would be nice to do some really quantitative comparisons. 

1 Jan 2010

Investigating partial edge error with zero-derivative boundary condition.
I find this emerges only with multiple processes, but then with standard
cubic boundary conditions. Must be a bug in the communications logic. 
I believe that the edges are not communicated on principle. Why is that
a problem? 

The error seems to be arising in the final boundary setting after the
allgather. It appears to go away if the final boundary setting is
done twice. I think the logic of this is that, after the allgather,
boundary conditions are not set, so boundaries don't satisfy the condition.
Then calling bdyset implements the setting. However, at edges/corners, the
setting might be from an incorrect boundary value along a face to the edge.
In that case the edge value is incorrect after one setting. If a second
setting is done, then the extrapolation is from a value that is correct.
So the whole is then done properly. In general, I think this shows that
when a corner is set prior to the face value that it is extrapolated from,
it is incorrect. I believe this incorrect setting probably does not affect
things in normal operation, because the edge values ought not to be used.
But I'm not sure about that. In any case, we ought to fix the error. 
I've assumed that we can set an edge value from either of its faces.
That's not so. In fact sometimes we can't correctly set it from either,
because they have not themselves yet been set. This problem would not
exist if we could start from the middle of faces instead of the edges.
Maybe, but maybe not, I think we'd need to start at position 2s. 
Do the non-edge faces first, then do the edge faces. It doesn't look 
easy. 

Actually it is not very difficult to implement a more correct
algorithm, based on extrapolating from the nearest body (not face)
point. (Still iterating in the natural order).  It appears to be just
as fast and direct although recoding was necessary.

The run of the v10 cases was done with -t.01. I confused this with timestep
and did comparable runs with -t.1. These are much higher temperatures.
They'll be interesting. The old runs with -dt.1 are fine as far as
timestep is concerned. We've now got two temperatures.

2 Jan 2010

Working on padvnc to incorporate collisions in a compact way.
First clean out the testing code. 
Now reorganize the logic to make it more readable and comprehensible.
Localize the reinjection in one place (which requires logic adjustment).
This is all working. 

Now there appears to be a bug in that dtaccel=0.5(dt+dtprec). This is not
correct. It ought to be dtaccel=0.5(dtpos+dtprec). On reinjection, dtpos
is put to a random fraction of dtpos; and dtprec is put to zero. This means
that we are treating the prior step as of zero length, so that the position
and velocity are simultaneous. For the next step, we therefore want to
advance the velocity by half of the timestep (with nothing for the prior
timestep). But dtpos is the timestep, not dt. Correct this small error.

Then we can use dtpos as the remaining timestep in a subcycling associated
with collisions. 

Implemented both collisions and subcycling.

24 May 10
Got a problem with NANs in case with very large number of particles.
The Field corruption test was triggered. 
This turned out to be caused by initialization to a position exactly
on the mesh boundary, where the field was obtained incorrectly. 
Fixed partlocate to declare NAN position to be out of mesh.
Fixed pinit to ensure exactly on boundary is rejected.
Also fixed interp to reject NANs. 

28 May 10

Thinking about names. One idea was Cartesian Orthogonal Particles and
Thermals in Cell: COPTIC. It would be better if the O could refer to
the object boundary representation. (Since sceptic has orthogonal
coordinates.) Ideas: Oblique (referring to the boundary). I can't
think of anything better

Cartesian[-cell] Oblique[-boundary] Particles and Thermals In Cell: COPTIC 

Changed the makefile and the main filename.
Added to the CVS modules file the alias coptic CCPIC.
Committed. Now one does a cvs checkout coptic and gets the file with
the program name changed to coptic. There are some residual files with
ccpic as part of their name. Don't change them because it is too much
trouble.

Implemented a make feature that saves in REINJECT.f a single fortran
line that sets a character variable equal to the REINJECT configuration.
Then added a feature into coptic that checks if the particle region 
boolean has any objects that it is _inside_. If so, this is a 'bounded'
region. (Even if there is an unbounded region outside as well as the
bounded region inside.) If the particle region is 'bounded' and the 
injection scheme starts 'cart' (cartreinject.o), then this is an incompatible
scheme, and the code stops with an error message on initialization. 
This will probably be tripped only if the objectfile is explicitly 
specified and incompatible.

30 May 2010
Implementing more satisfactory potential BC. We want the potential to
be constant along a direction d which is a vector radial in x,y and
has a z-length v_d times the radial length. Implement this as a
boundary condition that set the boundary value to satisfy this
condition. One is probably safest deriving the tangential derivatives
from the values on the active mesh, one in from the boundary. The
normal derivative from the boundary point and its neighbor in the
active mesh. Should the tangential derivatives be centered? If so one
requires to go forward and backward from the point in the tangential
direction. This will work ok except for the edges. (Faces are ok.) At
the edges, one could just use an uncentered difference, but it might
be better to extrapolate from the 1,1 point diagonally in from the
edge point. If one does that, then an uncentered difference might not
be required.

We first analyse the current slopeD routine:
ccSet the cumulative registers
      r2=0.
      fac=0.
      ipd=ipoint
      do n=1,ndims
c BC is du/dr=D u/r     in the form   (ub-u0)=  D*(ub+u0)*f/(1-f)
c where f = Sum_j[(xb_j+x0_j)dx_j]/(2*rm^2), dx=xb-x0
c Thus ub=u0(1-f-D.f)/(1-f+D.f)
c Here we are using radii from position (0,0,..)
         x=xn(ixnp(n)+1+indi(n))
         r2=r2+x*x
         if(indi(n).eq.0)then
c On lower boundary face
            dx=xn(ixnp(n)+1)-xn(ixnp(n)+2)
cc ixnp(n)+1 and ixnp(n)+2 are the point and its adjacent.
            ipd=ipd+iLs(n)
cc ipd is the index of the point adjacent in the n direction.
            fac=fac+x*dx
cc accumulate dx*x, the projection of the step along the r-direction.
            if(n.eq.1)then
c The exception in step. Do not change!
               inc=iused(1)-1
cc This will remain the inc only if we are not on a 2, or 3 boundary.
            else
               inc=1
            endif
         elseif(indi(n).eq.iused(n)-1)then
c On upper boundary face
            dx=xn(ixnp(n)+1+indi(n))-xn(ixnp(n)+indi(n))
            ipd=ipd-iLs(n)
cc On upper ipd is the point adjacent in the -(n) direction.
            fac=fac+x*dx
            inc=1
         endif
      enddo
cc When we reach here, we have ipd equal to an index to the point
cc adjacent in each of the dimensions, positive or negative according
cc as that direction was lower or upper face, zero if neither.
cc This is the point from which we are going to extrapolate. 
cc We have fac equal to x.dx where dx is the delta-x to position ipd.
cc And r2 is r^2. fac is r.dr 
      if(ipd.eq.ipoint)then
         write(*,*)'BDY function error; we should not be here'
         stop
      else
         fac=fac/(2.*r2)
cc scale fac by r^2
         u(ipoint+1)=u(ipd+1) *(1.-fac+D*fac)/(1.-fac-D*fac)
cc The boundary point is equal to the adjacent times 
cc (1-f+D*f)/(1-f-D*f). Multiplying this out gives:
cc U_p(1-f-Df) = U_d(1-f+Df). I.e. (U_p-U_d)(1-f)=(U_p+U_d)Df
cc The radius to the half-way point is rh2=(x-dx)^2 = r^2-2x.dx=r2(1-f)
cc So rh2*(U_p-U_d)/rh2.dr = D(U_p+U_d)/2 
      endif
c^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

So that more or less makes sense. I want to use the same sort of
algorithm if possible. I can use exactly this algorithm to determine
the point from which we extrapolate. Then when we find a non-zero dx
for a particular direction, if it is in the z-direction we need to 
use the M scaling. The criterion we need to satisfy is: 
  du/dr + M du/dz =0
So we can use the x,y difference to estimate the extrapolated value
as: u_p = u_d + (dx.r + dy.r)/r (du/dr) + dz (du/dz) 
with du/dr = -M du/dz. 
So u_p=u_d + (-M*(dx.x + dy.y)/r + dz).(du/dz)
If the du/dz is measured at u_d we can do it centered.
On the z-ends, dx=dy=0. This is then an identity and we can't do it
centered, but if we do it uncentered, it puts d^2u/dz^2 = 0. 
Why can't we do that for everything? Might be better. Sounds
unstable. 

We must do the substitution for du/dz at the z-ends instead.
   u_p=u_d + (-1/M)(du/dr)dz and du/dr estimated from mesh.
This looks as if it might get problematic at low M. 

Things don't look very implicit and I am not convinced that I've got
the right sort of approach. To do it right we need at least one
tangential and one normal step at each point. And the tangential step
must be in the z-direction unless we are on the z-face. If we are on
the z-face, we could use two tangential steps. A simpler fall-back
would be to make du/dz=0 on the z-faces.

Implemented that: mach on x,y-faces, zero-slope on z-faces.

It's pretty hard to test on the big domains.  Perhaps I ought to do
something on a smaller domain.  It is not obviously broken, but small
numbers of steps give a potential that is rather offset.
Anyway it is now easy to swap between the old and the new boundary 
conditions. 

Removed randc.o from the objects.

Ran for 650 steps after a restart. Still seemed to have a rather
negative average potential. 

Doing tests on a small 5x5 domain. The direction of contours is about
right. So it seems the boundary condition is doing its job. However,
there are cases where the potential is very depressed, so it seems 
as if that's an issue that needs to be resolved. Actually this effect
exists even with the old BCs. It gets really bad at 500 steps for
-v.5 and the small domain. This is not unique to the new version. The
problem's been there for a long time. Probably I need to fix it. 
I see that there's a big particle pump-out from a starting number of
90k down to about 30k. That's a symptom. How is rhoinfinity being
calculated? is the question. This is badly broken. Setting constant
number of particles does not fix it.

First it seems the sign of slpD was incorrect. But still that's not
the main problem. Seems to be worst at low temperature. Bad also with
zero velocity. Problem seems absent at -t1. This must be a bug with 
rhoinfinity calculation associated with temperature. At -t.1 -l1. it
asymptotes to -.13Te. At -t.01 to -1.75Te.

Seems as if the problem arises from phirein, which is exceeding its
capped value because of the low temperature. This corresponds to a 
value of chi equal to 0.5. But no, just setting chi to zero does not
fix it. 

I think there's a major problem with the way that rhoinf is being 
calculated from flux. If phirein is substantially bigger than Ti, 
which it is for pretty much all small domains when Ti is small, then
we are going to get very big swings in cfactor. This is going to be
unstable, probably. But the whole approach is highly dubious, for
a rectangular domain anyway. If the presheath drop to the edge of 
the boundary is significant, then we don't really have a decent
way of determining the rhoinf from the flux, because we don't have
a decent way (for a rectangular domain) of calculating how much flux
is going to enter. The cfactor is mostly just a kludge. If I turn off 
the chi/cfactor, then we just get the maxwellian flux calculated
across the boundary. But in reality there is a faster flux to the 
object than that. This means that at constant density there are many
more injections, which the rhoinf calculation takes as a sign that the
rhoinf is actually much higher than reality. But the electron density 
can't really be that high, because if it were, the whole box would be 
very negatively charged. So the potential everywhere in the box has 
a tendency to drop so as to reduce the electron density.

The flux at zero drift velocity is area x sqrt(2T_i/\pi). This seems
like a factor of 2 too much because it is for both faces.

The low potentials are mostly a sign that the rhoinf is being taken
too large. 

31 May 2010

Fundamentally, there has to be a way to allow the phirein to adjust
the rhoinf. The instability I have been observing is a problem but
just putting it against a rail is not the solution. 

Tried putting a relaxation parameter. It needs to be very small
to bring any degree of stability. Also, It seems that phirein
is quite often exactly zero. This is because phirein is capped in
padvnc never to be above 0. Also the energy of the injection is 
adjusted using averein, which is set to the prior value of phirein at
the start of a step. However, energy is never reduced, only enhanced.

Set the phirein cap to 2.Ti instead of zero. This works ok. It lowers
the edge potential a little, but it still tends on average to be
slightly above zero. It bounces around somewhat, which seems to be 
mostly statistics.

So what we have implemented is that the correction factor for edge
potential is relaxed by only by Ti/(1.+Ti). This is small at small
edge temperature and never goes above 1/2. These values appear to be
stable (experimentally). Also we have raised the phirein cap to 2Ti,
which helps prevent excessively positive edge potentials by allowing
the positive excursions to push down the chi value somewhat. 

We still find that the corners often rise above zero potential, but
this is probably caused by the incorrect boundary
conditions. Generally seems to work for a range of temperatures and
drift velocities.

Commit.

2 June

Had discovered that the wake has a problem in this new configuration.
There appear to be long wavelength waves along the box. This seems likely
to be caused by the release of the BC constraint. Since this case is 
poorly represented by the OML enhancement factor, one really ought not
to rely on that with such a big and asymmetric box. So make the 
leading face have zero potential. This ought to stabilize things.
This was a small modification to the bdymach routine. 

There's an issue about self force. I probably ought to make sure that
I avoid it. This is so only if the electric field interpolation and the 
charge assignment are done "consistently". It is not clear that I am
doing this. In principle either the charge or the field interpolation could
be adjusted to make things consistent. 

4 June Self Force.
Adjusting the charge assignment to zero the self force. 
This can be done. It is probably best done by assigning some of the charge
in a cell that is narrower than its neighbor to the adjacent cell further
away from the wider neighbour. (But not subtracting charge on the wide 
side). This can be considered to minimize the size of the charge. 

When the charge is at the half-step position, which is where the field
is defined, and can be considered the cell-boundary, the charge is
always shared equally between the two adjacent cells. When the charge
is exactly at the node position x_0, between nodes x_m and x_p, which
are distant from it by m, p, with m < p, the fractional charges are
q_p=0, q_0=0.5(1+m/p), q_m=0.5(1-m/p). Charge assignment varies
linearly between these values in the intermediate positions.

For uniform spacing this is exactly CIC. For non-uniform, it can be
considered to be an assignment by volume, of a rectangular charge
shape whose width changes in an area-conserving manner. When it is in
cell 0, its width is equal to the larger of m or p. And the area which
extends beyond the distances x_m/2 and x_p/2 (i.e. past this cell
boundary) are allocated to the adjacent nodes (not to non-adjacent nodes
even if the width extends far enough). 

Another way to think of it is that the particle weight at the nodes,
considered on a uniform mesh-fraction grid, when at a node that has
different step sizes on either side, takes an extended form. The
form morphs linearly in step-fraction from that form to the form of the
adjacent node, when the particle is at an intermediate point. The form
is S(x) m: 0.5(1-m/p), 0: 0.5(1+m/p), p: 0 (and linearly in between those). 
The linear interpolation distance is half a cell step (I think). 
Haven't got a good handle on this description. 

17 June 10
Found that the oscillations that I got stuck with were caused by using
fixed particle number -ni, instead of fixed injection rate -ri. 

30 June 10

Also found that even using fixed injection rate at lower M=0.5 there
are oscillations. The plan is therefore to implement a switch that
totally simplifies the BCs so that reinjection is at a specified rate
calculated as the rate corresponding to distant fluxes regardless of
potential, and the potential boundary condition is simply on the
gradients, possibly using the diagonal expression. This ought to be
sufficient to give a stable scheme. It might lead to some slight edge
screening effects, but that ought not to be a problem. 

Try to sort out the reinjection number logic and the way they are
controlled for the following parameters:

rhoinf the density at infinity summed over processors.
calculated in rhoinfcalc each iteration
if nrein!=0, rhoinf=nrein/(dtin*cfactor*flux), else guess from n_part.
cfactor is the adjustment for non-zero edge potential. 

nrein
the actual number of reinjections in the step summed over all processors.

ninjcomp refers during iterations to one processor's complement.
At initialization, if rhoinf !=0, set in nreincalc.

n_part one processor's number of particles. Set by -ninnn switch. 
If n_part!=0 during initialization, then set rhoinf=0, so nreincalc is not 
called. 

-ri sets value of rhoinf __per node__, which then is converted into ninjcomp
by nreincalc, at which time, rhoinf is multiplied by nproc to make it total.

-ni sets n_part.

To reduce confusion, rename nreincalc to ninjcalc. ninjcomp is the most 
important thing set by ninjcalc. Also introduce new initial variable
ripernode which is what rhoinf was prior to being multiplied by 
numprocs. In other words we no longer overload rhoinf during the initalization.

Introduce new switch -rx determining the relaxation rate of chi, the
edge potential correction (and of averein). Default 1 immediate.
Set to zero there is no edge potential correction of the flux or of the
injection energy.

Corrected dtprec to store for each particle, as per sceptic. This is
the only way to get subcycling etc to work properly. Also set it to
zero in the pinit, do not set it to dt at the start of padvnc. This
latter change makes a big difference to the value after 5 steps. Amounts
to supposing that the particle load occurs with perfect reinjection 
form of distribution function. 
Need then to set dtprec(i)=dtpos at the standard end of each cycle.
That restores much of the difference in flux. 

Working on subcycling. If we calculate the square of the field, f2. 
then perhaps we can determine automatically whether the step is 
small enough. A criterion might be field*dt (= delta v) > some typical
velocity. The parameter subcycle could perhaps be used to set the
criterion, e.g. sqrt(f2)*dt > subcycle (normalized). Another way to
say the same thing is that dt should be shortened until f.dt < subcycle.
An integer subcycle criterion would then be
   dtc = dt/max(1, int(f*dt/subcycle))

This seems to work. Arrange to print out the number of particles subcycled,
if non-zero. 

13 July 2010

Tried out v.5 -rx0. case on 8-processor machine. Its oscillations are
substantially reduced c.f. the previous problems, but not gone away.

Found a bug in fluxdata reading and writing. This is because we need
the address of the next (unused) ff_data slot, to determine how much
ff_data to read/write. I had hacked it to omit the last. Now I need to
make it right. Trouble is with old data flux data files. Do I want to
be able to read them? I think it's not worth it. Assume for now that I don't. 
But put comments in about how to recover the old data files.

15 July 2010

The -rx0 without holding the leading edge potential fixed has rather low
edge oscillation problems and seems to run reasonably convincingly. 

However even at -p0.2 we have not apparently reached a linear regime.

Set going the -v0.8 cases.

Some code clean-up, especially of the main program. 

-------------------------------------------------------------------

31 July 2010

In view of the discrepancies with Lampe's plot of the wake, I am
considering a way to represent a point charge as a pseudo-object of
zero extent. It could be added to the object file as a new type.

There seem to be two ways to implement this. 

1. Simply put extra point charge(s) of specified magnitude greater
than unity into the region at arbitrary position in the grid, and
place the charge onto the grid via the standard chargetomesh process.
This is essentially trivial, but it suffers from accuracy challenges
close to the charge. In effect, accurate field representation in the
vicinity of the charge will be limited to the size of the mesh there. 

2. Put into the field a coulomb representation of the point charge(s).
This has the benefit of NOT requiring fine mesh to represent the point
charge influence. However, we need to solve the rest of the shielding
problem properly. It becomes a sort of PPPM approach. If we add to the
field the force of the coulomb field, and solve the Poisson equation
for the residual potential (with the coulomb potential removed), then
we will have a good solution provided we resolve the shielding. This
will permit us to do without a fine scale near the charge, and resolve
only the shielding (Debye) scale length. This would be a serious benefit.
(There will be some display issues in that we will want to add back the
coulomb parts of the potential for display purposes. Perhaps we can do
it right at the end so the output files are at least consistent.) 

Since we solve the electron shielded Poisson problem, 

      \nabla^2 \phi + \phi/\lambda^2 = \rho  ,

we need to account for the coulomb field in that solution. We will be
tracking only   u  =  \phi - \phi_c  where \phi_c is the coulomb part
of the potential. Consequently the correct equation for u is

      \nabla^2 u + u/\lambda^2 = \rho + \phi_c/\lambda^2  ,

Thus we simply need to add \phi_c/\lambda^2 to the charge. This could
perhaps be done in psumtoq.

It might therefore be convenient to store \phi_c for use both in obtaining
q and in plotting the total \phi = u + \phi_c. It is fixed (but the charge
could perhaps be permitted to change in time). In any case it ought to be
in a separate common block that contains the information about the additional
charges. (There might be a cache inefficiency in it being far from u).

There will then be an issue with boundary conditions on u. The
boundary values of \phi_c will have to be subtracted from the BCs for
\phi.  This looks like the tricky part. For example bdymach would have
to have access to \phi_c. Since it will have the same structure as u,
it ought not to be too problematic to access its elements appropriately.
Looking through bdyroutine.f for u(, it does not look too difficult
to change those all to be u+\phi_c. In the long run it might be a bit
cumbersome to do this, but probably not more confusing than the logic
of those iteration routines anyway. 

The problem with the above is that it is only true for the linearized
approximation to the Boltzmann factor. Really we have to solve

      \lambda^2 \nabla^2 \phi = n - \exp{\phi}  ,

and the non-linearity of the additional operator prevents us exactly
translating from u to phi by the addition of constant phi_c into the
equation.  It has to be in the exponent. If we recalculate the
contribution of phi_c nonlinearly at each step, then we will get an
accurate result. But the contribution is then changing. One probably
does not want to update phi_c every sor iteration step, which might be
expensive, but if one wants a nonlinear solution, that update is needed. 
 
Part of the awkwardness arises from the fact that faddu is written so
that it just takes the exponential of its argument. Because the index
of the u array is not passed, it is not possible for the function to
correct for a phi_c offset. It seems that it ought to be unproblematic
to have additional arguments for faddu such as the array index. Faddu
is called only in sorrelaxgen. One could add the index as the last
argument without breaking the current call (I think). Yes. Did that in
the call without changing the function. It still works. (Might be a
slight time cost).

Thus we would have to change faddu to compensate for phi_c, instead
of doing something to psumtoq. That in itself may be more compact and
manageable. 

To summarize: we could implement PPPM equivalent by introducing point
charge(s), calculating their potential on the grid phi_c, changing
faddu to use u+phi_c, changing the BC so that it applies to u+phi_c,
and solving for u alone, which is the shielding potential with the
phi_c subtracted off. As far as the potential calculation is concerned,
only bdyroutine.f (which contains faddu) would have to be reprogrammed.
(Now sorrelaxgen has been editted to add the index). Force also has
to add the analytic (point-charge) force. This sounds as if it is fairly
low cost.

--------------------------------------------------------------------
Found a problem with restarting. Ugh. With the check script we see
x_part differences, but not uqcij. Moved the checkx before the padvnc
and it goes away. So padvnc is the problem. This was the case when
I last worked on restarting. However, this time, EVERY particle is 
wrong after one padvnc step, and so is the particle count.

It dawns on me why this is. I have to save and restore the dtprec for
each particle, otherwise it is not initialized properly for padvnc.
Yes that is it. Fixed.

Removed the necessity for extra storage to be supplied in the main
by limiting the check code to its own maximum size. This makes it
completely self-contained. And we can use only modest size of storage
so that the impact is negligible.
--------------------------------------------------------------------

The size of the phi_c array must be such as to accommodate the mesh,
presumably the same as u, etc. Those dimensions are currently set in
the main program (and u etc are not in common), so that's a bit of a
problem.

Start by eliminating Li from coptic. Use na_ijk.
coptic still works. There's a problem with phiexamine. It assumes
(defines) its arrays to have equal length in each dimension.
When I change that in coptic, the phiexamine is not correct.
It's not completely obvious why. array3read just reads the used 
dimensions and appears to store accordingly. It defines the size
of its full dimensions by its input parameter (so they must be 
passed correctly). Needs investigation.

1 Aug 2010

Can't make the phiexamine misbehave this morning! Maybe it was just
too late last night. No, it is flaky. the geombigcube.dat file caused
problems. This is going to be hard. It appears to be the second entry
ifull(2) that causes the problem. No sometimes the third does it.

After considerable thrashing, I still can't get to the bottom of it.

Try switching from g77 to gfortran. The link fails in the same way 
that the memory overrun of sceptic caused. Perhaps there's some
problem I don't understand here. phiexamine will not link either.
This was caused by the n_partmax being set to 40M (not 4M) which 
overflows the 2G virtual (though not resident).

With this problem fixed, g77 compiles do not seem to give the phiexamine
problem. Also the gfortan succeeds in compiling. 

CONCLUSION: the phiexamine/coptic misbehaviour was triggered by erroneously
specifying a 40M partmax that caused the (virtual) size of both coptic and
phiexamine to exceed machine memory. In g77 this gave the strange errors
that I could not diagnose. In gfortran it cause it to fail to compile.
Both of these behaviours could be considered to be inadequacies in the 
compiler. But anyway...

Today was an excursion that mostly just discovered a typing error in 
n_partmax. But I now have dimension setting in the main program more
rational. It is done crucially with the following statement:
      parameter (na_m=100,na_i=40,na_j=40,na_k=100)

I could presumably separate out this declaration into a file that was
called by appropriate programs to define the array dimensions, in particular
phi_c could be sized by it.

Observe that phiexamine occupies more virtual memory than coptic itself.
This appears to be because of just dumping all the objects into it. 
It is probably better to create a library and link against that. 
So change the makefile to do that for both coptic and other mains.
This does not drop the virtual size of phiexamine well below that of coptic,
so it does not have the desired effect. It is not clear why phiexamine
is so big. It makes not difference whether the lib is simply put on the 
command line or search using -L. -lcoptic.

3 Aug 2010

Rationalized array size declarations into mini-file griddecl.f
Checked out phiexamine, denexamine, partexamine to bring them into
conformity with the new declarations and to get rid of warnings.

Implemented additional force, with the magnitude corresponding to a
coulomb potential at the specified radius represented by the
2nd extra data argument. Of course, the actual extra potential at that
radius is zero because of the charge shaping/cutoff. This does not 
perturb the plotted potential very much. But we have not yet implemented
the effect of faddu.

4 Aug 2010

Changed the order of arguments in mditerate to be equivalent to that
of mditerarg. This is to reduce potential confusion.

Implemented setadfield to insert the uc and rhoc. It also plots slices
if -gs is set, so we can examine the results. 

When we write out the data to disk, we either need to add on the ptch
potential, or we need to write it out. Implemented the addition to both
phi and pha.

Implemented faddu modifications.
Ought now to be working.

Try some cases with nothing but a point charge. Found a few problems
with fluxwriting. Rationalized that. Found that there was a problem that
arose from accidentally using the point-charge object to define the
particle region. That does not work, because volint breaks when it can't
rely on intersections being set at the boundary. They are not set with
an object that is masked out by having all its abc equal to zero. Such
an object can't be part of the structure that defines the particle region.
Implement a trap to prevent that happening.

Now we seem to have cases with nothing but a point charge working.
The u potential (initially) seems to take the peak value approximately
equal to the charge value specified (phi at r_o). I don't know if that
makes sense. It drops a bit after some evolution and ion shielding.

However, the simple test develops an overall instability in the average
of potential! Annoying but perhaps not totally unexpected, since we
now have no potential set anywhere. It's basically an instability
in the rhoinf. It is suppressed by using -rx.5, or -rx0, or even -rx.8.
Good!

5 Aug 2010

Exercising some benchmarks with pure point charge. 
With large debyelen there is a problem if the bdymach condition is used.
There is simply a big offset in potential. That's bad. 
Instead one must switch back to bdyslpd and set the radial derivative
to -1 (logarithmic). Then convergence is very rapid to a potential that
agrees almost perfectly with Coulomb. (After some mods to phiexamine to
make the plots sensible). This shows that the potential solver and 
compensation is working correctly. At large lambda, it is dominated by
the additional charge.

Testing orbits. Used  ./coptic -s100 -go1 -l100. -ri1. 
but did not get a closed orbit. Strong precession. That's not right
it ought to be at least elliptical or even circular if the energy 
is correct.

Switching back to sphere potential, we get a circular orbit, potential
-2 at r=1. 

Worked on this problem and found and fixed some bugs in the getadfield
calculation. Now we get a good circular orbit and a decent elliptical one
affected by step size. The step size issues are puzzling. Using quite big
steps for the circle does not produce large cumulative errors, but turning
on subcycling produces substantial errors. Subcycling had seemed to help
with the ellipse. Subcycling is not working as expected. The effect of
subcycling gets worse and worse, as we force more steps with a lower limit.

Logic of padvnc needs reexamining. We have the following timestep parameters

dt
dtprec(i)
dtremain
dtc
dtaccel

At top of each particle cycle, dtpos is set to dt and dtremain to 0.
100 continue
Fields are calculated at the current position.
Subcycle evaluates dtc based on the field, and if it is .lt. dtpos,
then it calls for a substep, setting
       dtremain=dtpos+dtremain-dtc              dtpos=dtc
Seems to be the same for collisions.
Then            dtaccel=0.5*(dtpos+dtprec(i))
Here we have a bit of a puzzle at startup as to what ought to be taken
for dtprec(i). If we took a previous step, dtprec ought to be the duration
of that previous step. 
Then acceleration by dtaccel is done
Then moving by dtpos is done.
Then set     dtprec(i)=dtpos. 
And          if(dtremain.gt.0.)then   
	     dtpos=dtremain  dtremain=0. goto 100 
In other words we try to take a step equal to the remaining time in
this step if it is positive.
Then we save the orbits, and move to next particle. 


The problem is nothing to do with point-charges. Exists with finite
objects. We appear to be seeing a sling-shot effect as the particle
approaches the perigee. Too much acceleration when close, too little
far away. The problem is not due to changes in dtc. Setting dtc to
a constant gives the same problem.

Found a bug on the very first getfield call. The xp fractions are not
set correctly, so the field is incorrect. This is for the special particle.
Fixed that bug by moving the special particle to pinit.

Still haven't spotted the problem, which still seems to be present when
dtc is set at a constant value equal to half the dt. The main thing is
that energy conservation is broken badly. But I don't know why.

FOUND IT. The effect is self force or some other related force
error. I was running tests with -l100. Increasing that to -l10000
essentially kills the effect. I had only one particle total. Therefore
it could not be interacting with another particle. It must be
interacting with itself. It is certainly the case that the field was
coming back subtly different, and that seemed to be the cause of the
orbit difference. By contrast, with -l10000 the differences in the
field are greatly reduced. It is extremely interesting that the 
case that is not subcycled appears to have a self-force such that it
keeps it in orbit, while the subcycled case seems to be forced off
the orbit; in particular, its energy is reduced. (I'm using subcycle
directly in the pinit special orbit.)

With -l lowered to say 20, we can see the potential perturbation (for the
first steps when the particle is on the midplane). But obviously it is
there even when -l is larger.

Conclusion: the difference between subcycling and not subcycling is
that in subcycling the potential is not updated between
(sub)steps. The result is that the particle moves in a static
self-force well, whereas without subcycling, it always experiences a
self-potential that is centered on the current particle position. 

Presumably the particle is self-repelling, so this effect ought to 
accelerate the particle in the direction of velocity. Since it is always
a bit ahead of its influence. That does not seem to have been what
happens to the particle in well, but perhaps that's some interaction 
between the well and self-force. Presumably similar spatially localized
effects exist in unequal spacing meshes. But they are not likely to be
progressive the way the subcycling force is.

Specifying one ion but finite lambda effectively assigns a very great
charge to the ion. Remember \lambda_D = sqrt{\epsilon_o T_e/n_e e^2}
rhoinf is approximately the inverse of the box size. So e^2/epsilon_0 
= 1/(lambda^2 n_e) ~ (2r_s)^3/lambda^2.N_i .

6 Aug 2010

One can confirm this is the problem by running different numbers of
particles. setting -ni16 rather than -ni1 drops the orbit error a lot.
The self force ought to be inversely proportional to -ni. But actually
it looks as if rhoinf is actually behaving proportional to (-ni)^2 as
we go from -ni1 to -ni2. This seems to be because the nrein is becoming
non-zero. When nrein is zero, rhoinf is not changed. Therefore, if any
particle escapes, setting rhoinf from the nrein, it remains set to this
larger value thereafter. This is obviously a bias. rhoinf is used in
chargetomesh.f to set rho from psum. That's how it determines the effective
size of the ion charge. Arguably, if nrein is a small number it is
unreliable to use it, and probably it ought to be ignored. Use nrein=10
as a tolerable minimum, to avoid this bias at low -ni. 

Reducing the step size (keeping subcycle half of it) does reduce the
error, seems roughly proportional. Error increases slightly decreasing
subcycle, but quickly saturates. In this case the step size is <~ mesh
size. Which means we are inside the charge cloud, and roughly the
self-force will rise linearly with (subcycle) step size. This
expectation seems consistent with observations. If it is bigger than
the cloud size, it falls inversely with step size. So taking steps
about the size of the mesh is the worst self-force regime. 

7 Aug 10

Implementing automatic boundary conditions. Plan is to use bdyslopeDH
if debyelen is >~ size of mesh, and bdymach if it is << size of mesh.
In order to do this, we need to know the size of mesh. plascom.f has
rs which is the radius of the domain for circular domains but at present
is not set properly otherwise. 

Put into meshconstruct() code that sets rs to half the length of the 
maximum side-length. Move geominit to a position after meshconstruct.
That way if a special circular case is called, rs will be reset in it.

Implemented in bdysetfree a choice between a logarithmic slope if
debyelen.gt.0.19rs, or mach bdy condition if debyelen smaller. 
Also changed crelax default to be Ti/(1+Ti). This stabilizes the 
bdyslopeDH case but probably one still needs to set -rx0 for mach
cases.

Fiddled with phiexamine to improve radial comparisons and to plot
yukawa potential. 

Improve -gs switch to allow setting ipstep. 

Start to try on big domains. Something is badly wrong with the
potential. Get back onto t400 the wakedomain.dat so we can test
locally. Shrink to wakehalf.dat so it's more managable. Seems as if
the point charge is appearing to be in the wrong place for these runs.
Might be better to get an even smaller x25 case. Let's do that.
Ok now we have wake25.dat which runs at a decent pace. It seems clear
that the point charge is in the wrong place. Or some part of it is.

Actually the plots of uc and rhoc show the charge in the right place.
But the potential solutions have features that don't belong.

The strange features appear only to be present with multiple cpu
mpiexec runs. Seems unaffected by changing griddecl na_i, na_j etc.

Is still present with the bdyroutine hacked to use bdyslope rather than
bdymach. It's different, but still pretty much bad. Can increase -l and
still see it very clearly. Probably it must be in faddu. This seems confirmed
by making the faddu test .true. so we just use an unmodified faddu.

uci seems to plot correctly from main coptic. so does rhoci.

Tired. This problem is caused by faddu, but it is hard to see anything
wrong with faddu. 

8 Aug 10

After sleeping on this, it dawns on me that the pointer being passed
is relative to the origin of the particular block, rather than the
whole array. This must be the problem. It cannot be avoided by locating
the changes in psumtoq, because faddu needs to have an effective argument
that includes the uc (even if rhoc were in q already). So probably my
plan to use faddu was reasonable, but ran into this unforseen problem 
that sorrelaxgen is passed an array whose origin is already at myorig,
and is not at present aware of where in the total array it is.

It looks as if the natural way to fix this is to pass myorig to sorrelaxgen
so it knows where it is in the total array. 

Seems to work.

9 Aug 10

Having run some big cases on sceptic machine overnight, I find the
agreement with the small-object equivalent runs to be excellent. This
is using the same mesh as the object runs. Therefore, start cases with
uniform mesh and point-charge. Basically I can already conclude that
the wavelength discrepancy with lampe is NOT caused by the finite
charge size.  Notice, though, that the discrepancy in wavelength is
worse at T_i=.01, while the damping discrepancy is better (smaller)
there. 

Made significant upgrades to partexamine to be able to choose xlimits
and vlimits, and to specify a particular partfile by giving a 3-character
extension. One can see a slow-down of the z-velocity at the top of the
first potential hill, which is in the right ball-park to be explained
by simply the energy reduction there. Evidently this is a non-linear
effect. However, it acts in the wrong direction to be an explanation of
the observed longer wavelength. 

17 Aug 10

Planning for diagnostics etc. 
We need information output about particle parameters. 
Thought about ways of representing the distribution function in a 
compact enough way to document its variation with position. Hard.

Instead implement standard moment diagnostics: velocity, temperatures.
May as well do it on the potential mesh (otherwise confusion!)
Process is relatively simple. At the time we form the psum, form also
sums for v(3) and v^2(3) moments. We require 6 more quantities on the
mesh. This is not a big fraction of memory increase, since we already have
cij, u, uave, q, qave etc. It is probably best to regard the moments as
separate quantities on the grid, so we can slice plot them if desired.
Also has the merit of giving the same structure as (e.g.) psum, for the 
purposes of mpireduction. For ease of passing, we probably ought to make
the diags an array whose trailing dimension enumerates the diagnostics.
c Diagnostics
      integer ndiagmax
      parameter (ndiagmax=6)
      real diags(na_i,na_j,na_k,ndiagmax)

For speed during particle pushing, we just want to assign the moment the
same way we assign the charge in chargetomesh. It is not necessary in 
principle to mpigather the moment information until something is required
to be done with it. And we should not. In the case of psum, it is zeroed
at each step prior to chargetomesh. However, there we probably should not
zero the moment diagnostics if we are averaging them. Instead they should
just be accumulated. If we are doing trailing averages, we might need to
multiply by (nave-1) before tomesh, then divide by nave after. But if we
did a pure box-car average, it would be unnecessary to do that. In any
case those decisions can be made separate from the chargetomesh process.

Have to generalize array3write/read to allow an extra dimension. Did that
and implemented detection of old version written files through the first
two characters of charout. Fixed phiexamine, philineread, denexamine.

Implemented writing in coptic. I wonder if we ought to write out the
box addition number of steps? (array3write does not).

18 Aug 10

Continue working on diagnostics. I think I want to use the whole of the 
diag iuds arrays. Psum does not. It does not use the boundaries. And it
does not reduce the boundaries. Therefore I need a reduction code that
reduces the whole block including boundaries. 

Start by adding the used dimensions to mditcom.f. Also it was mistakenly
omitted from the headers list. Add it. Add code to set iasuds in 
mpisubopcreate.

Found a prior bug in mpisubopcreate that the initialization call was
occuring every time, because lfirst was not set to .false. Fixing does
not seem to change anything.

Seem to have it working for diags reduction. One can use denexamine or
phiexamine on one diagnostic (which is the equivalent of psum). It
appears to have face values about half of the core values, and edge
values about one quarter. That is what is expected.

It is not obvious that I want to do any additional processing within
coptic main program. 

Time costs (for particle-dominated cases):

time ./coptic -s10 -a5 -ni1000000
real    0m12.361s

time ./coptic -m1 -s10 -a5 -ni1000000
real    0m12.485s

time ./coptic -m4 -s10 -a5 -ni1000000
real    0m14.347s

time ./coptic -m7 -s10 -a5 -ni1000000
real    0m16.837s

Looks as if there's a non-trivial extra cost for a substantial number
of diagnostics. 34% extra for 7. Not too bad.

Test of solver at low step size shows that using steps of ~ -dt0.00005
or less reduces the sor iterations to 2. But that if we increase it to
maybe -dt0.0001 we get more typically 11 iterations. (These are
half-iterations actually). -dt0.0005 gives about 25. -dt0.001 31.
-dt0.005 43. -dt0.05 60. Basically I think this shows the sor solver
works sensibly to converge to its tolerance of eps_sor=1.e-5. 
It presumably also means that if the particle numbers are large
enough that rho fluctuations are small, then the sor solver will be
fast. (These are 32^3 meshes).

Implement minimalistic diagexamine that reads all the arrays and 
displays them. 

21 Aug 2010
Fixed some bugs in the restart code that put time step data into the
wrong array elements.

25 Aug 2010

Remove the obsolete ran0 function from randf.f.

Discovered that randc.o is needed on loki. But earlier this year I 
removed it from the objects. volint still uses it. I don't understand
how it is still working! The answer seems to be that gfortran and g77
have an intrinsic random generator. The float version is called rand()
and gives numbers in the 0-1 range, which is correct. This does not
seem to exist for pathscale compiler, which is why randc is necessary.
It probably does not hurt to list randc in the objects. With gfortran,
etc, it will not be used. The intrinsic fortran call will be used 
instead. Add back.

I find that the Loki problem seems to be with MPI_IN_PLACE. 
Some searching on the net shows that this is an MPI-2 spec only.
Not in MPI-1. That presumably explains the problem. 
I find that sceptic has a piece of code that removes the IN_PLACE
saying that it does not work on Loki. The usage in sormpi is easily
fixed, but the usage in reduce.f is less easy to fix.

By the way, mpibbdytest is the easiest way to get to the bottom of these
communications issues.

Found that it is possible to compile a coptic with MPICH2 on gigabit
on loki. But the flush has to be removed from the main program because
it causes segfaults. Then in works, but one needs to run multiple processes
under:
/opt/mvapich2.gbe/path/bin/mpiexec


26 Aug 2010
Fixed some phiexamine things.
Implemented -w<iwstep> switch to determine how often one saves the code
state to disk.

29 Aug 2010
Fixed some things in the makefile. It is now possible to use override
option settings in the make command. Also make vecx will use the nonglx
driver and remove the glu libs.

4 Sep 10
Established that short domains z11 have indistinguishable results with
-dt0.05 -s3000 and -dt0.1 -s1500. So we can save on computation, and be
confident there is not a timestep convergence issue.

Had a problem with cumprob on loki because tiny seemed to be a bit too
small and we experienced interp equal points. Lowered to 1e-14 from 1e-15.
Probably ought to do some more checking with this. It arises with a big
shift of the distribution function when calculating reinjection for the
downstream face (I presume). 

There are various problems on loki with communications etc. It appears
to take several minutes for the particle files to be written to disk
at the end of a run. Also there are unterminated runs and mpds that
hang around and screw things up.

11 Sep 10
Succeeded in runs on the cmodws cluster but only by removing the 
-machinefile argument. 

partexamine modified to save the distibution information, since it takes
so long to acquire. For 4M particles, it takes several seconds to read
the data per file. So a multiprocessor postprocessing takes a very long time.

In point of fact, postprocessing all the particles from a particular
run takes roughly as long as advancing them. This means that it is not
totally crazy if one requires the average over many steps, to think that
one should run the multiprocessing code over again to acquire the averaged
distribution function as one goes. This has the advantage of doing the 
computing over many nodes. The hard part is deciding the domains in which
to do the accumulation. (And specifying that). 

Right now I have accumulation in ndiag=200 mdims=3 x 4 quantities: fv,
px, diagv, diagx. 200x3x4 = 2400 points in cartdiag for partexamine
accumulation. But actually the diagv and diagx could reasonably be the
same everywhere. And arguably all I need is f(v). So that's 3
quantities.  For these quantities, it would not be excessive to
accumulate into an array equal in size to the cell array. Then at each
step one would be able to communicate it without excessive cost.
Alternatively, if the communication is only at the end, we would not
need to worry about it, and one could consider an array of size equal
to the number of particles or bigger. (E.g. 4M) In fact it does not
make sense to accumulate into an array bigger than the number of
particles for one processor, because there would be less than one
particle per cell. Of course if we are averaging over time and
processors, that might give a decent count even with close to 1 per
cell. Consequently, the spatial array size we could consider
collecting distributions over would be no larger than 4M/200 =
20k. Notice that this not quite the size of the spatial array 32x32x96
~ 100k. But if we have only fv to accumulate, then the extra factor of
10 is compensated by each particle having 9 reals. We could use a
resolution in 3-D equal to the cell-mesh; but probably reduce the
storage to an unimportant cost at about twice the cell-size (factor of
8=2^3).

Put it another way. We use typically 50 times as many particles as cells.
Each particle costs 9 reals. We can therefore afford fv 450 reals per cell
and only double the storage cost. 

Notice that we need a computationally efficient way to assign the
particle weight or else we'll die from the extra particle calculation
cost. For example, suppose we had 450 moments, each of which had to be
calculated when a particle was accumulated. This would undoubtedly far
outweigh the moving cost for that particle. We already know that
forming the sums for the first two velocity moments is a significant
expense (+35%).

Alternatively, if the process is simply assigning to a cell in 3
dimensions, this requires roughly 3 multiplications, which is going to
be cheap.

What's more if we use the cell mesh at the spatial basis, we already
have the fractional mesh position, so that is free. On the other hand
we might well do CIC-style assignment, in which case we have to assign
6 adjacent fractions for each charge and velocity. That means we can't
use integers for accumulation. 

Adding an extra array ndistmax,na_i,na_j,na_k, with ndistmax=300,
(50,50,100) takes the virtual memory from 485M to 772MB. Thus if 
we can keep the distribution to 3x100 we'll be ok.

Then presumably we'll have to write this array: 300MB. If it were 
reasonably sparse, we might be able to save a lot by not writing 
out those velocities that were never present. This could perhaps
be done by keeping track of a total count for each velocity (which
is then the total over the whole domain which might be useful in its
own right). Any zero of that is not an fv required to be written. 
However, we would have to consider the order of indices for optimal
access. The gain from this complication might be a compression of a
factor of 3 or 4 in typical cases. But it would probably be better 
to keep from being sparse. 

To ensure that we have a well chosen set of velocities (assuming this
is possible for the entire domain) we could use analysis of the actual
particles after a tolerably small number of steps. Choose velocity
ranges that cover the substantive range(s) before we start
accumulating. We probably would want to be able to choose a range that
covered all but a tiny number of particles (not all but zero particles).
That would be a bit more elaborate than just finding the min and max
of velocity components.

Specification (q, nq, nmia, rmi, rma) for array q find the range rmi, rma
such that there are nmia values greater than rma and less than rmi.
Actually we can't pass a 1-d array so it's got to be more complicated.
Also we need to do in 3-d. Have to retain the nmia smallest and largest
values (or pointers to them). 

Slot new values into the sort, in order, with overflow out of the end.
Initial values ought to be chosen to be overflowed out. 
Wrote code to do that. Took longer than planned. Seems to work when
used in partexamine.

13 Sep 10

Idea: why use equal spaced array of bins? That's inefficient. If one
had a bin array with (roughly) equal numbers of particles in each,
that's efficient.  One could probably do the same sort of adaptive
approach. Use the first step or file to decide on the bins. Thereafter
accumulate in the fixed bins. This needs a good algorithm to decide on
the bin placement. That's basically integrating the distribution
function to get the cumulative dist function. Then using equal spacing
on it (or something like that). How does one get the cumulative dist
function or its approximate equivalent efficiently? One way would be
to do the accumulation on a uniform grid with a large number of bins
and then add it up.  One also then needs an indirection step so that
one can assign particles to non-uniform bins by a single
division. This process would in any case require the larger _uniform_
bin array and its mapping to the non-uniform.  The mapping would be
universal, so no big storage is required. We also need ways of dealing
with non-uniform binning. 

Implemented in partexamine. I find that using a non-linear curve to
choose the non-uniform bins enables one to get a tolerable
representation with only 16 bins. 32 is really quite good.  Using this
technique, which will port readily to coptic itself, one does not need
more than 100 times the cell mesh, and possibly 50 times would be
enough. Then accumulation on the entire mesh will require only about
25M reals: less than the particle storage (11 reals times 4M). This
mesh based accumulation is highly appropriate for multiprocessing and
averaging over a decent number of steps, because then the numbers per
cell can become substantial. 

2 Oct 10
Reorganization of analysis routines into subdirectory analysis.

We need a more general potential variation on specified objects.  One
idea for such an object might be the general cuboid (actually general
parallelopiped) which is defined by three vectors vj from an origin
corner. The corners are then defined by the triple n=(n1,n2,n3) where nj
is either 0 or 1, and the position is \sum nj.vj. If we consider potential
to be given at (0,0,0) and that \delta\phi is equal to \sum nj,dj, with
dj being the potential difference at the end of vector vj, then we will
have a consistent and linear potential progression across the object.

One could do the same for a tetrahedron but that is not as likely to be
useful.

4 Oct 10 

About to try to implement some of this. But first want to understand why
with orbitinject etc we are getting Getfield No good vertices errors.
Find that we get a few such errors with the test field case. We used not 
to do so. 

The change seems to have been in the transition to revision 1.19. Where
the edge region was set to -1 to prevent reaching beyond the arrays. 
Now, when a sphere cuts between the penultimate node and the edge, the 
region outside it is 0, but the edge nodes are region -1, so for outside
points there are no good vertices. This is actually correct in one sense. 
When there is no object boundary nearby, this won't cause any problems. 
But when there is, we get these nogood regions.

Actually the error in the test routine only shows up when I shrink the
sphere radius to 4.9999. Otherwise the only drawn points are inside the
object. Basically this is working correctly.

With particles, however, we are getting nogoods. These are all reported
with fracs 1.0 or 32.0. Region 0. We ought not to have particles there.
Guessing this is a reinjection problem.

Seems to have something to do with the makefile not compiling the 
reinjection routine. But now that I've cleared it I can't reproduce
the error.

Ahh. I think I see the problem. ar -rs is inserting with replacement
into the libcoptic.a archive (library). When switching from one
reinject to another, the old reinject is left in the library. Then
when linking is done, the linker finds the old reinject code before
the new code, and it is using the wrong reinjection code. If the
makefile is changed we ought to delete the old libcoptic.a for
sure. But in any case this explains why running make clean fixed
things because it deleted libcoptic.

For now just always delete libcoptic before inserting into it.

Now things are all working correctly.

5 Oct 2010

Improving field interpolation near boundaries.  Implemented a wider
search along the direction of field for valid points.  This helps a
lot to improve the interpolation in cases where the point would
otherwise be omitted. Especially when there is only one point of four
(of a 2-D box) in the correct region. However, when the two missing
points along the field direction are opposite a leg with two points in
the region, then the extrapolation is not so good. A discontinuity
occurs between the extrapolated value and that for a case where the
upper gradient cannot be extrapolated from anywhere and so it is
omitted.  The latter (omission) can give a _more_ accurate result than
a long extrapolation. Maybe we should have a ramped weighting of the
certainty of this field value, rather than a sudden omission. How to
implement is a bit tricky. Perhaps the flags become weights?

6 Oct 2010

Fixed the above problem by introducing weights rather than flags for
missing nodes. Instead of simply dropping a missing node, we try to
interpolate it from further away. When we do this, its accuracy is
substantially reduced compared with the accuracy of legs that are all
in the region. So the weight of a leg that is extrapolated is reduced
and the box interpolation is modified (actually simplified) to account
for a continuous weight rather than a discontinuous flag indicating
absence. 

For lattice legs that have both points out of region, call this
"overextrapolation", the weight is reduced linearly from 1 to zero as
we move from the outermost point to the innermost along the
extrapolation direction. This allows the adjacent point (if any) in
the region to control the value towards the inner end. This does not
much affect squares with three points outside. But it helps very much
for a row with four nodes outside adjacent to a row entirely
inside. In such a case box overextrapolation from either side is
possible for the two outer boxes of the row. They are linearly
unweighted, reaching zero at the boundary of the middle box. for which
the nodes are entirely missing. As a result, discontinuities that
otherwise occur in the extrapolated values are avoided. All this is coded
in. 

However, there remains a big error in Ez when in the z-constant plane, at
oblique angles. It is associated with boxes that have a single missing
node. It abruptly reverses as z varies across the midplane. This must be
arising because of unidirectional extrapolation whose direction reverses
as we go from one half of the box to the other. What we need for this is
to have weighted bi-directional extrapolation to ensure continuity when
it is possible to overextrapolate from either direction. 

There appear to be other problems that are tickled by moving the mesh
around, and are real bugs. We need better diagnostics that really know
where the sphere center is and do proper analytic calculations.

7 Oct 10 Coded in proper analytic calculation. Looks sensible.  Now
starting to figure out the major bug(s), using shifted mesh.  

Implementing some internal writing diagnostics, we find that the
easiest way to test this is by using a specific test on the value of u
and the gradient to flag an error. Then we find that the bug consists
of a single point with offset 33363 whose cij pointer is 3276 which
has both boa and coa set to zero, but fraction equal to
0.602049947. This is incorrect for a point inside the fixed potential
sphere. But why is it wrong? Perhaps I need a sanity check on the entire
cij set and its pointers during initialization. It could hardly be general.
It is specific to this particular problem, since boa=coa=o amounts to setting
potential to zero. 

8 Oct 10
Found the source of this bug in boxedge:
                     f1=1./(sign(max(abs(fn(i)),tiny),fn(i)))
c Warn if any strange crossings found. Removed not necessary.
                     if(f1.lt.1. .and. f1.ge.0)then 

This warning and removal of the strange crossings was commented out.
The results was that strange crossings were being allowed in the
boxedge calculation whose purpose is to set some fractions determined
by planes of intersections with nearby lattice legs. That information
is basically unused in the present incarnation, since I found that it
was infeasible to embed the object geometry into the cij information,
and I currently don't.  Therefore this complex area of code ought
probably to be cleaned up and the complexity removed. For now make
sure the strange crossings are in fact removed. This was the real
bug. Now there remains the poor interpolation problems. Actually this
does not seem to be the only bug affecting the flat potential
region. There remain some strange results but not as large in
error. They are associated with uprime=-1.850757 and many (maybe all)
of them with icp0=3276 (as was the case with the old bug). So perhaps
I have not really fixed this bug.

Using the stick plot and wiremesh to diagnose this more fully. The reason
we are getting the Warning of strange crossing is not that the strange
crossing is wrong. It is that there really is a crossing there, but for
some unknown reason it is not being found or documented in the original
pass. Thus the bug is not in the warning code but earlier. USE the stick
plot (with outer sticks removed by removing outer object).

Finally tracked this to an error in the algorithm of spheresect. PHEW!

So now we really seem to have the bugs fixed. And we can move on to the
problem of whether the interpolation can be improved. Right now with the
spherical test case (shifted) we are getting max tangential field errors
in the ball-park of 1, and lower radial errors.

12 Oct 10
Work on encapsulating the potential solution and the pic code separately.
Created sortest which does not depend upon particles at all. It is in
the makefile but still depends on a lot of routines. 

Also get all the MPI routines into just mpibbdy.f and reduce.f
No direct reference to anything mpi is outside those routines.
A non-mpi version would need to have a certain number of dummy routines
for the abstracted mpicalls. But would not need mpi. 

Got a version of sortest going without mpi, using nonmpibbdy.

Thus I've accomplished a couple of things. (1) Separated out the potential
solver. (2) In the separated potential solver, separated out the mpi into
a single file, and developed a dummy file. This is a demonstration of the 
effective encapsulation of the mpi within the solver. 

Because I can't access cvs at the moment, I note that phisoluplot.f replaces
ccpicplot.f ; we add sortest.f nonmpibbdy.f. 

How to cement the encapsulation? 

13 Oct 10
Start to get the include files sorted out of the sortest objects.
Make faddu a separate file. Make a different bdyroutine bdysetsol.f.
Get rid of svdsol.f by putting it into cijroutine which is the only
place it is used.
Get orbit3plot out of phisoluplot and make its own file.
Clean out the includes of plascom and partcom from the solobjects.
Then the only includes are meshcom, objcom, 3dcom, bbdydecl, accis/world3.h
It's pretty much impossible to purge out the accis/world3.h reference from
slicesect. 

14 Oct 10 Thinking more about improving field extrapolation. If just
one point is absent from region, it would in some cases be better to
extrapolate based on both directions. Then when we reach the point
itself, we would balance out errors, rather than them making an
instantaneous transition from one to the other direction of
extrapolation. There's a puzzle about a situation where the object
simply is only one point thick. In that case one probably should NOT
extrapolate from the other side. In fact the field might be totally
discontinuous from one side to the other. Therefore any improvement
obtained by smoothing is inappropriate except when the region itself
continues from one side of the point to the other. I.e. when there's
an unintercepted leg to the region somewhere from the point.

The switch in this case is done within gradlocalregion. So there's a bit
of a puzzle with how to set weights if one wants to. But in any case it
is not really obvious that there is much gain to be had in further 
refinement of the extrapolation. 
 
Ran a series of cases with different resolution. 

N	16	32		64		128
Er	1.71	0.80		.167		.081
Et	1.86	1.74		.337		.100
Pm	1.01	.296		.056		.016
Ey	1.90	0.82		.342		.076
SDEy	0.33	0.11		.030		.008

The Ey error really needs to be explored over the whole volume. When it is,
the maximum converges rather more slowly. 128 max is about 0.25. But probably
64 is also larger. It might be that the maximum error is only linear. But the
SD is quadratic. 

15 Oct 10

Implemented the integration/search over 3-D in ereplot to find the worst
error and plot at that level. 

Implemented over-extrapolation from both directions being counted when
possible. That occurs when there are just two points outside the region.
This may be less dangerous than the case where there is just one. But maybe
not. Still this is easy to reverse if necessary. It produces roughly a
20% reduction in the maximum error, so arguably not a big win.

Values with this new implementation and searching over the volume:

N   	  16  	  24     32       48       64 	       128
Ey	  1.67	  1.57   0.91     0.68     0.50	       0.223
SDEy	  0.201	  0.17   0.089    0.060    0.022       0.007
Nodal	  0.200	  0.04   0.031    0.014    0.008       0.002

Plainly the maximum error is (only) linear with mesh size. SD is faster
than linear, but not obviously quadratic (till convergence care is taken). 

This is perhaps disappointing, but I think explicable on the basis of
the fact that there are places where tangential-field is evaluated using
no derivative information because of omitting points on the other side
of the boundary. 

If the boundary has continuous potential, it might be a better approximation
_not_ to omit the points, but always to be sure that we never take a 
derivative based upon potential values on either side of the interface.
This would help only for tangential derivative. It would make things
worse for normal derivative. 

Another maybe better view might be that when multilinear interpolation
fails because a point is outside the region, instead of omitting it,
we should extrapolate from the other side. This ought to have the
merit of quadratic accuracy, rather than the linear accuracy of the
above scheme.  The danger is that it might over-reach into other
regions. But that's probably unlikely. I think that the omission of ML
points leads to linear convergence only if two (or three) points (in
3D) are missing, so that no information on a gradient is obtained.

There are issues to do with whether one reaches too far and thus out of the
domain. Perhaps it is not a problem. 

19 Oct 10
Plan for validation. Get a UROP student who will then run both coptic and
sceptic for equivalent cases. 

27 Oct 10
Getting back to the code development. Particle distribution diagnostics.

Note that we track the fractional mesh position of each particle. This
makes it trivial to assign the particle to a bin that is based upon
the (potential) mesh. Consequently, if we generalize the partexamine
data structure to add ndims extra suffixes to it, we will easily be
able to assign. In fact we can pass the x_part, and that's what we
should do.  Addressing might be a bit tricky in general
dimensions. There's another issue that although the total number of
assignment memory slots is less than the particle data (11 reals times
4M) when the mesh is small, it may not be if the mesh is big. I really
need a transparent way to either include storage for the distribution
on the mesh or not.  Or a way to reduce the storage demand by
concatenating cells if desired. This might be done by defining a
concatenation number which is the number of cells (in each direction)
that are counted as one for the purposes of fv-storage. If this is 2,
then there are half as many in each direction. Which cuts the storage
down to 1/8. For bigger numbers there is trickiness in accounting for
unequal numbers concatenated. We could choose to prevent that in the
storage assignment, and perhaps in the actually array assignment at
run time. In fact, though, it may not matter if the concatenation is
unequal, provided we account for it in normalizations etc. But the
idea is that if this number is equal to the whole mesh array, this
reduces the accumulation to the whole domain.

5 Nov 10
Implemented a spatially resolved accumulation in partexamine. The spatial
boxes are based on the x-mesh full structure divided by isubdiv. 
A problem arises that partexamine does not know the mesh spacing structure
because it is not stored in the particle file. Similarly it is hard to
make sense of the numbers in cell because their volume is different, 
and particle file does not know the volume. These would not be so
problematic for coptic itself, since it would know that information. 

Mostly the reason for these issues is the use of the fractional mesh 
information in x_part for position registration. If one had a totally
different, perhaps uniform, mesh for the distribution, then this problem
would not arise. So one should probably reconsider the idea of using 
the x_part fraction. Once the spatial boxes are combined, it is far from
obvious that it is a good idea to base them on the phi-mesh. 

Suppose instead that we use the xlimits structure to define the total
spatial domain, and simply divide it into equal sized boxes. It will be
quite straightforward to assign position to bin, and I imagine not very
much computational cost. 

Changed to this approach. It is much easier to understand and implement.
It works sensibly in partexamine. 

15 Nov 10

Moved most of the serious work of particle accumulation to a new file
coptic/partaccum.f and moved fvgriddecl.f to coptic/. Then recompiled.
This prepares us for incorporating some of the accumulation into
coptic itself. For that purpose we need to consider the parallel 
operation. 

Recall the overall approach: we run the total accumulation first.  It
puts every particle that is within the xlimits into a uniformly spaced
set of velocity bins. In so doing, it produces xnewlimit.  Then
bincalc groups those velocity bins by pointing each one to a specific
sub-bin. With those pointers determined, it is then cheap to calculate
the velocity sub-bin via the uniform bins and the pointer array.  The
routine fvxinit initializes. Then the subsequent call(s) to subaccum
accumulates particles to the spatially resolved sub-bins. Of these
processes, the bin and newlimits etc need to be universal, otherwise
there is no way to reduce the data back consistently if the bins are
different between processors. xnewlimits could presumably be reduced
as maxs and mins. But bincalc needs to be done centrally, after
reduction of the original accumulation. Alternatively, the whole of
the first global step could be done only on the particles of the
master node. In that case no reduction would be required, only a
scatter of the resulting pointer and xnewlimit information. If the bincalc
were to be based on a very localized xlimit, then there might be a risk that
the noise level would be excessive if we use only one node's particles. 

Each process can do (sub)accumulation at each timestep. Then presumably 
a reduce to the master node is required, and the data to be written to
disk. Presumably the sub-bins would then be reinitialized and the process
repeated. If this gather-write is not done too often then it ought not to have
a significant time cost. 

Probably the way to proceed is to package the code in partexamine without the
plotting (routinely). 

17 Nov 10

Having packaged the update and plotting of the subaccumulation into
partdistup and pltsubdist, incorporated them into coptic, and got working
with the appropriate default values. 

The extra cost if both partaccum and vaccum are done each step is
about 10% of the particle cost. Nearly negligible (but not quite).

Now we need to incorporate into the control structures. We have two
step lengths -a which is the iavesteps the averaging length and
diagnostic writing step length and -w which writes out to the (same)
final files. One presumably would use the diagnostic step length,
although at present that is examined only if ndiag is non zero. We
could invert that priority. Ok did the inversion. Now we need a control
for whether we are doing the particle accumulation. And to incorporate 
data write-out. 

Incorporated the switch -pd and display and writing.
Allowed arrow keys to change the cell position in different dimensions.

Basically this is now working in serial but needs mpi reduces for 
parallel. This also needs to worry about the initialization using the
full information.

19 Nov 10

Combined fvgriddecl into ptaccom.f.

Seem to have got the parallel version going with some reduces.

-----------------------------------------------------

What to do next:  
     General objects with potential variation. See Oct 2.
     Magnetic Field.
     Collisions.
     Benchmarking with sceptic (student).
     Self forrce tests(?)
     Space and moons.

22 Nov 10

Discovered that there's a problem with the written out data from the particle
diagnostics. It seems to arise because I've incorrectly assumed that the
velocity bins for the subaccum can be recalculated from the other data. 

Found a bug giving Field corruptions. It appears to arise at the outer
boundary of the cuboidal region. It comes from one of the new getfield
calls that is invoked when there are missing interpolation
points. (Which is presumably why it is a new bug.) Tracing it through
various diagnostic writes is tricky because of the dereferenced origin
and the multidimensional aspects. But my best guess is that this is 
arising because of the way that the extra getfields reach beyond the
immediate vicinty (at least along the gradient direction). 

23 Nov 10

Thus my theory of the problem is that when adjacent to (e.g.) boundary
of 1 (x) when getting gradient component 3 (z), the box interpolation
fails to find acceptable gradient on the outermost-x (boundary) and
starts to look further along it. In doing so, if it is near/at the
z-boundary too, i.e. at the edge, then it may reach outside the iused
limit in the z-direction (gradient direction). It will try to check
the cij region there, but that won't have been set, so it is not
obvious that it will say, don't use this. Also, it is possible that
this might reach beyond the ifull limits, in which case in will
violate the bounds of this array as a whole. 

Although I can't quite verify this theory, I can see that I am
accessing a negative array position for the first corrupt localregion
detected.  Perhaps I should simply ban the calling of getfield from a
general origin and do checks about exceeding bounds. That might
eventually be a handicap if the domain was divided for particles. But
that's only a remote possibility.  In fact, checking the rest of the
code, the only calls to getfield that currently exist pass the bottom
of the u-array. There are no local origin calls. Therefore extended
the tests in the extra getlocalcalls to prevent over- (or
under-)running the array bounds. This prevents the corruption
errors. So it presumably proves that my diagnosis was correct.

There are still some box recut errors. They probably ought to be 
understood. They go away with very small adjustment of the mesh. 
So they seem to be a mesh clash. It doesn't seem very rewarding to 
go chasing after it at the moment. Just tell to adjust the grid.

There's still a bug in pex reading and writing with the fv data.

26 Nov 10 

Working on additional diagnostic plots, especially wrt velocity.
Cleaned up some of phiexamine.  Found that there are some spurious big
velocities in the diagnostic files saved from recent runs for
wakes. These apparent bugs need to be understood if we are going to be
able to use the diagnostics for velocity plotting. It seems likely
they arise (at the edge of low-density regions) because of divide by
(near) zero. Yes this is the problem, because one can look at the 
unnormalized values and they are fine. So we increase the minimum
value required for the division in normalization to 5. rather than 0.
But there's another calibration problem, which is that the value of the
velocity is too large: 40 instead of 5. This must surely be something
to do with the number of processors (8 for these runs). Yes there is
a bug in diagreduce that means the first diag (density) was not 
reduced. That explains the 8 times larger velocities. Fixed.

27 Nov 10

Implemented arrow overplot on phiexamine radial plot. It does not 
show too much on the fast wakes, since the arrows mostly point in the z
direction.

Got puzzled about the fact that density is not zero inside the object.
Realized that this is because of setting it to exactly cancel the electron
density in the object. Thus one way to use the density in the object is
to say this is the electron density at the potential of the object. 

Implemented quasineutral solving for debyelen=0. Seems to work. Needed
to fix cijroutine to avoid divide by zero. However, this may not have
really reached a correct simulation because the potential boundary
condition is a fixed value on the sphere, whereas for a quasineutral
case, the BC ought to be an equivalent of the Bohm condition at the
boundary. According to Leonardo, the way to get a correct condition is
to put the density at the probe edge equal to the flux divided by the
mean normal velocity. Evidently this will require changes of cij. That
sounds very tricky. Despite this problem, the result actually does not
look too bad. How can this be? Well we are actually not using the
boundary condition on the probe, because that is embedded into the
cij, and we are not using the cij for potential solving. No difference
equation is being used. Actually we are using the cij for getting the
acceleration.  All we are doing is evaluating the density and putting
the potential equal to its logarithm. For mesh points whose volume is
set large because they are outside the domain, all that happens is
that psumtoq sets the rho=faddu(u), and then the quasineutral
potential is set to be the inverse. So, within rounding errors,
potential never changes (from zero).  That explains why we are seeing
zero potential inside the sphere.

We never use the potential from inside the sphere. Instead the cijs are
used to apply boundary potential. And quadratic interpolation of the normal
derivative is used. It might be useful to explore the extent that total
flux depends upon specified potential. It ought to be a weak dependence. 

Actually there are substantially worse problems. The flux is strongly 
anisotropic in the quasineutral case. It is also when using lambda_d less
than the spacing, e.g. -l.01. There are massive fluctuations in the 
potential, maybe up to +-.3Te, for these cases. The noise is very large.
An estimate of this is: suppose we have 4M particles on a grid 48^3, which
has about 120k cells. That is about 33 particles per cell. So we would
expect the fluctuations to be about 6 of 33. So perhaps this is explicable. 
In itself, the noise hardly explains fixed anisotropy, which is visible. 
However, perhaps the self forces are strong in these situations with 
few particles per cell, or maybe there is some other mesh-dependent effect.


13 Dec 10

Working on Media type boundary conditions. Developed the interpolation
and difference formulas.

In cijroutine, whether there is an intersection of a lattice leg is determined
by potlsect (in 3dobjects.f). This is probably the first place to work on to
implement such conditions. This is currently implemented for spheres only
through the routine spheresect. Evidently we need routines that implement
other objects. 

It might be appropriate to implement a new object type which consists of
a plane of charge-density. I realize that there is a question as to whether
objects like this should be implemented through boundary conditions or 
through the equivalent of PPPM. The point-charge PPPM is really the addition
of a screened point charge, with a radius of influence specified, over which
it is screened. In principle one could specify a charge plane by the same
sort of approach, but one would have to have a way to calculate the analytic
force quickly.

One might approximate the charge plane in pretty much any way that is
(1) convenient, and (2) capable of being accurately compensated on the grid.
But I don't really know how to do that. Perhaps should think about it. 

Analytic solutions in 2-D for a finite uniform charge strip are easily
calculated. For a strip -a<x<a at y=0, the field at (x,y) is

E_y= \sigma/4\pi\epsilon_0 [arctan(x-a) - arctan(x+a)]
E_x= \sigma/8\pi\epsilon_0 ln|[(x-a)^2+y^2]/[(x+a)^2+y^2)|

(I'm not sure what the potential is.) This could be screened in some
arbitrary way with a function that falls to zero some distance from the strip
and the corresponding screening charge density calculate. 

However, if a finite length in the third dimension (z) is needed, this
solution does not seem to be available. Another might. Actually the Wolfram
integrator gives an analytic form for the integrals required. But they look 
like this:

Integrate[ArcSinh[a/Sqrt[x^2 + 1]], x] ==
x*ArcSinh[a/Sqrt[1 + x^2]] + (Sqrt[1 + x^2]*Sqrt[(1 + a^2 + x^2)/(1 + x^2)]* (2*a*Log[2*(x + Sqrt[1 + a^2 + x^2])] + I*(Log[(4*(I + I*a^2 + x + I*a*Sqrt[1 + a^2 + x^2]))/(a*(I + x))] - Log[(4*I + (4*I)*a^2 - 4*x + (4*I)*a* Sqrt[1 + a^2 + x^2])/(I*a - a*x)])))/ (2*Sqrt[1 + a^2 + x^2])
which is rather overwhelming.

In any case, although we can come up with solutions for uniform plane charge
sheets, that does not help with the ultimate aim of implementing surfaces of
more complex shape with possibly non-uniform charge density on them. E.g.
a sphere with non-uniform charge-density.

Therefore while an analytic test might be useful, in the longer term, I think
my boundary approach is more likely to yield a profitable way forward.
By the way, it might serve my purposes to implement dielectric charged
sphere first. 

27 Dec 10

Implemented general parallelopiped pllelosect and inside_geom.

Geometry seems now to be working. The wireframe rendering is rather lumpy
because it is not optimized to draw the extrema connections. The connections
that are drawn, between intersections common to a node or on adjacent
parallel lattice legs, do not include lines between diagonally connected
cells that might be the best extrema. In particular if diagonally
adjacent nodes have just one intersection, in the same direction, then
it is likely that the whole surface is below those two intersections, 
and thus that they probably ought to be connected. It seems too much trouble
to try to fix this, since it does not actually affect the solution. 
It appears to be solving the equations in a sensible fashion and putting
the potential to the correct value on the object surface. 

Played around with thin slabs. They seem to work fine and the solution 
by eye looks plausible. If they are very thin then periodic gaps appear
for oblique slabs. These appear to be permeable, so I think it is behaving
like a wire (mesh) screen.

I think it would be possible to use this object to introduce a plane
by making the plane a surface of a big object whose other faces were
outside the computational domain. This then divides the domain into two.
I don't know off hand what the boundary condition in the Neumann case
is on the inside of the object. I don't actually think it is automatically
the continuity condition. Perhaps I haven't specified this quite
completely yet. Although I think I did for the sphere within sphere test.
Yes potlsect returns B=negative for the "inactive side". And in that
case continuity only is (normally) used. This isn't really implemented
for anything except spheres. Actually no derivative scheme is.

Implemented coordinate-aligned cuboid. Seems to work.

28 Dec 10

Implemented coordinate-aligned cylinder. Works.
Fix cijplot to be able to rotate the stick only plot. 
Now we have all the originally conceived objects working with 
Dirichlet boundary conditions. 

Needed: Neumann/Robin for other than spheres. Media/Surface-Charge.  

29 Dec 10

Committed to CVS.

Working on different boundary conditions. In the long run we probably want
to link the surface charge to the flux. Therefore when we are using a
surface charge density that varies across an object, we ought to use a
data structure to describe it that is the same as the structure 
describing the flux density. At present this structure assumes that the
number of dimensions needed to describe the flux-position on the object
is nf_posdim=2. For an object with facets, this may be inconvenient. 
For example, for a parallelopiped, the natural way to index the position
would be by face (2 x ndims) and then by (ndims-1) dimensions on the face.
Thus at least nf_posdim=3 would be useful.
Also, nf_posdim is a parameter at present, being used to define the
array sizes of numerous objects in 3dcom. It can't be variable. 
In 3-D then, it seems a reasonable compromise to suppose that objects
might have facets on which 2-d indexing of position is done, but that the
facets themselves are indexed in a single dimension. Thus nf_posdim=3,
and the third dimension is the facet number. For a sphere there would
be just one facet. For a cube or parallelopiped 6= 2x ndims. For a cylinder
there might be 3=ndims (top, bottom, curved-surface). One can imagine 
others such as cone 2, tetrahedron 4, etc. 

Currently the only flux accumulation that is defined is for sphere 
in standard cos\theta and psi. 

Make nf_posdim=3. Then fluxexamine does not plot the new flux data
properly until it is recompiled with new common. Then it appears to work.

Now we need to generalize the flux tracking routine objsect.
There are two main parts to it. The first finds the bin number.
The second adds the contribution to the bins. Only the first needs
to be different for each object. The second should become common.
Split out the sphere-specific part. 

Working on cuboid face and surface indexing. I find that my approach
that uses ofn1,2,3 with ofn3 simply the number of facets (faces) is
limited.  The problem is that it makes sense to use a specific number
of face divisions in each of 3 dimensions. So that face 2 is indexed
with n3 and n1 (for example). This does not quite work with the plan I
have set up, in which ofn1 and ofn2 are the same for all faces. That
essentially forces all face indexing to be the same numbers. What I
really ought to be using is ofn1,2,3 for the three dimensions and offc
for the face number. Then the dimensions of face 1,2,3 are
ofn2*3,ofn3*1,ofn1*2, and the total is
nf_posno=2*(ofn2*3+ofn3*1+ofn1*2) (because of two faces per
dimension). In general I regard the indexing of the surface data as
totally the responsibility of the object code. So I really just need
to correctly calculate the total size of the surface data, nf_posno. 
Any subsequent analysis has to know how the data is organized, but 
coptic doesn't except in the fluxdata accumulation (which is what I am
working on). There's no reason not to include this extra facility, 
I think. Yes there is. The problem is nf_dimlens(nf_quant,nf_obj,nf_posdim)
which is assumed to be a triple (for nf_posdim=3) in the last index,
and hence can't really change. Actually at present, for spheres, I don't
use the third dimension of dimlens. Therefore it would be acceptable 
to make it equal to the third index length for a cube. It would not break
the sphere.

Had to introduce a new nf_faceind array that gives the offsets to the 
different faces within nf_posno, because those offsets are not uniformly
spaced when the faces are not of the same side, which now they aren't
since I'm using constant array length for each coordinate dimension.

Found that using an odd number of mesh points gives faddu singularity
errors only when the total coordinate length is a factor of n_mesh-1.
Not really clear why. Seems that cijroutine is not setting the regions
correctly at the boundary of the cube. Region 0 outside of it is not
being set. Must be because of the mesh clash.

30 Dec 10

I think the problem with meshclash is that on the one hand one can make
a convention that points on the boundary are all (say) outside the object,
but on the other, one has made the convention that fraction=1 is taken
to be a non-intersection. These are inconsistent, when projecting from
inside. This probably needs to be fixed in cubesect, etc. But there's
another inconvenience with those. I should have defined the center to
be really the center and the radii to be the distance from the center
to the faces. I want to change that. 

Changed cubes to be defined by center and radii. 
Fixed the mesh clash by automatically shifting the mesh slightly.
There's a down side to this in that it is asymmetric. 
Maybe I should have shifted and scaled. Try that. Seems better.
Seem to have got the changed cube flux accumulation working.

Cubes are now working apparently consistently. 
Fixed parallelopiped to use centered definition.
Fixed cylinder to use centered definition.

Flux on cylinder. We need 3 facets: two ends and curved surface.
Rationally one should index the angle by the same number on each.
Then the radius number and the axis number could be different. 
Therefore, like the cube, we could have 3 dimlens, which we should take
to be r,\theta,z. 

31 Dec 10

Despite good progress yesterday, I am mired in the cylinder case.
It seems very complicated to determine the intersection point. 
Rationalized the spherical intersection code to some extent by
packaging the calculation. Also including the ability to do a cylinder
in the same code by omiting a particular coordinate direction. 
However, this package needs to pass some stuff back, e.g. C, D, which 
are the radii squared -1 are useful to tell which end is inside etc.
Also, one might as well pass back both intersections, not just the one
that is closest positive to x1. Then we'd have all the useful information.

Organized this all more carefully. Now cylfsect is working. And the 
coordinate-aligned cylinder case appears correct.

Parallelopiped flux accumulation is not yet done, but it ought to be
very similar to the cube.

Created a routine docwrite to convert plain text files into fortran
routines that print out the same text. Have to replace single quotes
with backquotes to make the fortran compile properly. Seems to work. 
Fixed the parameters.txt file to bring it up to date. 

Made a geommany.dat input file that illustrates many of the shapes. 

3 Jan 2011

Remaining to be done: pllelogram flux accumulation. Should be like 
cuboid. 
Implemented. Also packaged the cube to use the same code based
upon a unit cube.
Seems to work but there are some strange inconsistencies
when changing from 1 to 2 bins. The exact numbers don't add up. 
We need some diagnostics of the intersections being found.

4 Jan 11
General idea is to draw in 3-D the flux grid for an object and the
set of intersections in the form of short lines from the prior 
particle position to the intersection point. This should show if we
are finding the intersections approximately correctly. Also it 
should serve as a basis for displaying the flux accumulation. 
Flux accumulation is done for a set of surfaces that lie between
certain surface parameter limits. It makes sense to plot those limits.
One problem is that there is one more boundary than there are regions.
That might make saving in the negative index spaces of the flux data
a problem. Since arrays are uniformly spaced maybe we don't have to 
save the top limit. In any case, we need to
  Calculate and store the flux position array data.
  Develop data structures and code to do 3-D plot of intersections.

Sphere has already implemented values at the center of the intervals,
rather than the boundaries of the intervals. Might be tricky to
change, although it is only in fluxave that it is used. And there in a
non-general way. It is not too difficult to transform between bin
centers and bin boundaries when they are guaranteed to be uniform in
the quantity considered. In fact if we know the bins are uniform we
don't need to use the bin position arrays. Just the algorithm.  The
main problem is that we should not let things get too adhoc.  We
already have some stuff hard wired into the various fsect routines.
We need a programing interface turning flux index into cartesian
position if we are going to be able to draw a mesh of the flux
domains.  At the moment, even the sphere does not have this, because
what is stored in the fluxdata is the angle coordinate. The flux file
does not have enough information in itself to reconstruct the position
of the surface elements in 3-D. In fact perhaps we need to make sure
that the object input data is written back to a file, perhaps the flux
file, otherwise it will be too difficult to do post-run visualization.

Put object data into flux file at end. 

7 Jan 11

Developed plotting of objects and intersections. It shows that the 
intersections do indeed appear to be on the objects. Need a sphere drawing
object. Presumably an approach like the cylinder would do. 

Now we need the flux accumulation grid(s). It might make sense to draw
each surface element separately. Then they could be colored in (e.g.)
with the flux number color. To do this, we need a way of transforming
from the ibin number to the element position on the object, and then
to the extent (edges of) of the element. For curved elements one might
need interpolation along the curved edges. For uniform grids we can
get the edge grid values from the central values. 

8 Jan 11

Corrected algorithms for surface painting of cylinders, spheres and made
consistent with specified flux accumulations. Coloring so far just according
to flux bin.

The ppcom.f structure seems at this stage to be an unnecessary duplication
of what goes on in 3dcom.f. One could simply define the pp_ pointers 
relative to the start of the object, and then pass the object. As far
as I can tell. Got rid of it and just use the objg reference instead.

Finished implementing coloring by bin of sphere, cylinder, cube, pllelopiped.
Sphere has a fall-back to its plotting arcs if there is no flux.

Emacs register can make a register bookmark in a file by 
C-x r m <ret>
Then you can jump back to it by
C-x r b <ret>

Alternatively one can save positions in registers by:
C-x r <SPC> <char>
and jump back to that register position by 
C-x r j <char> (with the same character).

12 Jan 11

Change 3dcom.f to extend flux data addressing to one step more than
the maxsteps. That last step is for averaged data. (We don't write it out).
Fix fluxdatainit.

Change fluxave to do the averaging into that ff_data position.

We have sphereplot plotting the fluxaves but not really tested that
the are perfectly in the correct position.

pllel also plotting. This is somewhat better tested using the expedient
of putting ijbin into the value to ensure consistent ijbin. However, 
even that does not really test that what I am reconstructing as the 
position is the same as the position I started with. This is a real
headache. 

Next we need to devise a way to test that the positions are really being
plotted and tallied equivalently. 

13 Jan 11

Perhaps this is simply a matter of reducing the number of tallies till
they can be spotted individually. Yes that seems to work, and to be 
correct. It is hard to be sure the counting is _exactly_ right, but the
approximate variation of count is correct and that is presumably enough
to show the registration of the facets is correct. It does not prove
the binning of individual intersections is exactly right. And I don't think
they are exactly right for pllel.

The case 
 ./coptic -ofgeompllel.dat -ni5000 -s2 -dt.4
shows points that are plainly not being attributed to the right bin.
All the points are accounted for though. And things appear to be nearly
correct. Just that points are being moved to an adjacent bin. It's extremely
hard to diagnose, though. 

Save the ijbin with the endpoint, then draw the number on the figure.
That shows points are being moved, mostly within a face to wrong bins. 
There does not seem to be a consistent direction of motion. Some points
seem swapped. Judging by the apparent position of the intersection which
is on the face, the fraction is being returned correctly. It is ijbin that
is wrong. Both of these are determined by cubeexplt operating on normalize
xn1/2. Presumably that means that xn1/2 can't be wrong. 
FOUND. The problem was in cubeexplt using the wrong fraction balance
frac vs (1.-frac). I can't now see any errors. Wow that was a slog to
uncover the error I spotted 3 Jan.

Rationalize the facecoloring into facecolor routine.
Implement for spheres and for cylinders. 
Ok. Now all are done and all use facecolor.
Have to turn off the iosw if the plot is called for an object that
does not have flux saved. Also need to pass iobj, not ifobj, since
the latter might be zero/undefined if no flux accumulation. 
Might be better if no flux accumulation to draw a shaded object of 
all one color. There's no real reason to do multicoloring in that case
other than to see the shape of the object. 

14 Jan 11

Add in 3x3vectors after flux bug before odata end to accommodate 
gradients of the coefficients a,b,c, They are in the order c,b,a 
because usually the c will be the only thing varying with space.

Fixed some things that broke for the higher byte=3 case.

Now we are ready to implement gradient of potential. This would be 
considerably easier if the spheresect etc (in 3dobjects) operated
on specified x1,x2 rather than on mesh indices and directions.
If that were the case, we could rationalize the intersection finding
by using the routines that are in fluxdata.f. Then we'd merge the code
bases of these things. I think this probably should be done. But it's
going to take some work. 

Commit present version.

Do the swap to using sphereintersection. Gives identical flux density etc.
Do cubesect swap. Ok.
Do pllelosect swap. Interesting! The old version is actually broken in
some way and does not give the region map correctly. The swapped version
appears to work. Gives sensible region map. And a different flux. Probably
now more correct (though maybe not entirely).
Do cylsect swap. Fixed one fmin problem.
That more or less completes the swap.

Timings. Cubes and Spheres are so quick that it's not worth worrying about
them.

Cylinder shows that the new routine is a lot slower: 1.68s vs 0.99s.
This is substantially reduced to 1.1s by putting an outside-ends test
first in the cylfsect. That's probably as much optimization as is easy.
The old cylsect uses knowledge that either we are seeking 'radially' or
axially (i.e. axial difference is zero or else non-axial distance is zero) 
for field-aligned cylinder and lattice legs. That's not true in general.
But in any case the time is now nearly the same.

Spheresect is not quite right. It is that fraction needs to be set
to 1. if outside the allowable range of intersections, because
sphereinterp returns the actual fractions regardless. Done.

Cut off the obsolete routines. They might have ideas that are worth
keeping, though.

Pllel has box recuts and mesh adjustments with new routine. Not old.
But old takes longer and is wrong. So that's no test, and it's probably
not worth fixing the old. However the box recuts may be themselves caused
by an error. Apparently not by returned fractions outside the ranges.
Perhaps by not returning the minimum intersection correctly. 
There's no sign that the mesh shifting and scaling is reducing the 
box recut problem. I conclude it's not a mesh clash.
I think there's a logic problem. cubexplt returns the fraction closest
to x1, but also requires x1 to be inside the object. 

15 Jan 11

I haven't got to the bottom of this problem. It needs to be solved.
The recut arises from an SVD fit to 6 points cutting a box. Example:
 Warning: Box Recut  6 2 -1 928 18 20 17  0.978108108 Adjust mesh!
 Intersection fraction=  0.753565311 1 1 1 18 20 17 x=  0.753565311  0.  0.
 Intersection fraction=  0.0625383928 3 1 1 18 20 17 x=  0.  0.  0.0625383928
 Intersection fraction=  0.251363158 2 2 2 19 20 17 x=  1.  0.251363158  0.
 Intersection fraction=  0.25136292 3 2 2 19 20 17 x=  1.  0.  0.25136292
 Intersection fraction=  0.919081092 1 2 2 18 20 18 x=  0.919081092  0.  1.
 Intersection fraction=  0.616751194 2 2 2 18 20 18 x=  0.  0.616751194  1.
 0.754 0.000 0.000 0.000 0.000 0.063 1.000 0.251 0.000 1.000 0.000 0.251
 0.919 0.000 1.000 0.000 0.617 1.000
 6 svdsol  -0.312387854 -0.0157415178 -0.432068348 -0.477817297 -0.632711947
 -0.294701457  0.323155403 -0.0396499522  0.363257378  0.269468725 -0.239875913
 -0.794914365 -0.0854396746  0.0455816351 -0.605457604  0.0698276907
  0.624653518 -0.478513867
 1/fn=  1.11668324  0.978108108  3.25422573

These intersections appear to arise from all box points being inside the
object except 0 0 1, and 1 0 0. This is not consistent with there being
a single plane through the box. There must be two planes at different 
oblique angles, slicing out these two points. When the SVD fit is made to 
find a single plane best giving rise to these intersections, there's no
way to do it sensibly, so a stupid result is found that happens to 
cut one of the legs that really isn't cut. That's a recut. It does not
appear to be a coding bug, but rather a logic bug for a situation with
acute angles. That was not encountered before because only a pllel will
give it, and the pllel code was previously not working properly (apparently). 

What to do about it? Well the SVD fitting procedure is really not
useful right now because the whole idea of using the extra fractions
>1 to tell something about the geometry has been abandoned. Is there a
reason to try to avoid such situations by moving the mesh?
Maybe. Actually the case in point is more or less benign, and seems to be
unavoidable. I wonder if the whole boxedge thing is now a waste of time. 
It's far from clear that we should go ahead and set the intersection to 
1.01 as we currently do. Remove that for now.

Hopefully we now have a fully rationalized version that works with 
sphere, cube, cylinder, pllel using the flux intersection codes.

We now need to generalize the condition setting to account for 
coefficient gradients, if present.

This is quite simple (after avoiding a bug from variable name clash).
Implemented and working. 

Ok now we have going
   1. Flux plotting.
   2. Gradients of boundary conditions. 
we also already had:
   3. Collisions

Things still to do. 
       Magnetic Field
       Surface charges
       ...

17 Jan 11

Surface charges. The key question to decide on implementation is
whether we should immediately go to a dereferenced value of surface
charge, or whether we should proceed more simply initially. A
dereferenced charge would perhaps be stored with the flux data. It
might be a new fluxtype, although it is not obvious we necessarily
want to be forced to use all the momentum and energy flux accounting
inevitably if charge accumulation is being done too. In effect, charge
accumulation is simply integrating the charge flux, i.e. adding it
up. There might be an electron flux that also is subtracted. For that
to be calculated, we would need the (mean) potential of the surface
element each step. If we were using specified potential, that would
not be too much of a problem, but also we would then not need to do a
surface charge calculation. So there's actually a substantial
challenge in calculating the electron current to a flux element. We
need to get the potential solution, and then do some sort of
appropriate average over the facet. Since this needs to be done every
step, it must be fast.

We ought not to attempt to get the potential on a finer spatial scale
than the mesh. Therefore it would be reasonable to use every lattice
intersection with a particular facet to document the potential. Taking
all such intersections one could regard the facet electron flux as
given by the sum of the electron flux density for potentials at the
intersections, multiplied by the corresponding area. This would then
require getpotentials at all lattice intersections multiplied by fixed
coefficients representing the geometry (areas). The number of real
lattice intersections is maybe 1/3 of the total number of pointers
used, which might be up to 10% of the mesh points. Compared with a
single SOR iteration, this would therefore be managable. Does the
surface charge update need to be done during SOR iteration? Well, not
really if the charging time is long compared with timestep. But yes, if
it is short. If it is really short, there'll be various problems. 

Charging time is capacitance*potential/current. 

Capacitance (of sphere) is roughly Q/phi = 4 \pi \epsilon_0 /(1/r_p
+1/lambda_s). 

Current is of order 4 \pi r_p^2 e v_te n_e. 

So time scale is\epsilon_0/[r_p e v_te n_e(1+r_p/lambda)]
=  phi_p  \ep_0 /[r_p e n_e(lambda+r_p) omega_p]
=  phi_p  e \lambda^2/[ r_p T_e (lambda + r_p) omega_p] 

So if phi_p is normalized to Te/e the charging time is

phi_p  \lambda^2/[rp(rp+\lambda) v_t/lambda]
=phi_p  \lambda^2/[rp(rp/lambda +1 ) v_t]

In normalized time units, v_cs/r_p = 1. So v_te = smr r_p where smr is
the square root of the mass ratio: sqrt(1870). 

Thus charging time is 
     phi_p (lambda^3/r_p^2 (rp+lambda)) 1/smr 

If lambda<r_p and we are talking about ions so smr=1, then with
phi_p ~ 1 we get just the (lambda^3/r_p^2 (rp+lambda)) factor, which 
could get small rather quickly. For example if lambda=0.1 rp, it is 
about 10^-3, which will be smaller than any sensible ion step. 
What's more, since smr is maybe ~100, 1/smr is itself a small quantity
for electron charging. In order for the time to be of order 1, we need
to have lambda^2/r_p^2 ~ smr. That is lambda/r_p ~ 10.

Therefore, unless the debye length is >~10r_p, the charging time is 
less than 1 normalized unit. And unless it is >~r_p the charging time
is less than 0.01 normalized unit. (Which might be roughly a time step.)

Particles charge instantaneously on the ion timescale if lambda_D <~ r_p. 

Thus an implicit solution for the potential appears essential. The
surface potential usually tracks quite quickly to cause the electron
flux density to equal the ion. This amounts to saying that the ion
surface charge is quickly neutralized by electron currents until it is
consistent with the potential solution. In effect this amounts to what
I implemented in SCEPTIC: that the surface potential becomes equal to
the value required to make j_e=j_i. 
 
The problem is that the ion flux will be very noisy on short time
scales.  Therefore it may have to be averaged over many
steps. (Perhaps non-physical as far as transients are concerned).
Actually the ion flux changes on a relatively slow time scale, even if
the debyelength is short. Roughly on the timescale of one normalized
unit, because it is governed by the presheath, not the sheath. That's
a reasonable time over which to average the ion flux. What's more, if
the potential locally fluctuates faster than this, then it will act
mostly like random kicks: collisions.

Conclusion. To make ion simulations of transients, one needs the ion
flux to vary on roughly a normalized timescale of 1. It is justifiable
to average the flux over approximately this time scale to reduce the
noise. If the potential still fluctuates, this will give rise to an
unphysical effective collisionality. 


Given concerns about noise level of surface charge, it might not be
advisable to make the surface charge structure the same as that of
flux collection. There might be a reason to adopt a coarser grid for
surface charge than for flux collection. If that were necessary, it
would presumably be possible to do appropriate spatial averaging as
well as time averaging. So perhaps the structure does not matter so
long as it is coarser. There might be other approaches to surface 
charge approximation, though. For example one might want a surface
charge (component) on a sphere that was proportional to cos theta. 
Trouble is, low order moments (for example) are a whole different 
approach. They can be linearly mapped from facets, though. Also 
from the point of view of inputting surface charges from the object
file, there is every reason to expect that one might want to do that
with (for example) a simple gradient term. (Cos theta on a sphere). 


Actually object data already contains a reverse pointer to the object
in iinter_sor.  It looks easy to generalize that to include the ijbin
in that object.  In which case one probably has the reverse reference
required. As imagined earlier, the right place to implement new
code is in ddn_sor. At present all this does is add on the diagonal
and bdy contributions e.g.      dnum=dnum+dob_sor(ibdy_sor,ip). 
Presumably it could do far more if necessary. Actually this ibdy 
contribution is the only thing containing coefficient c. It is the sum
of -P.C/A over all directions, for Robin condition. If C were being 
changed dynamically, perhaps we could do a similar sum for adjustments.
[What happens if there is more than one contribution to this sum? 
That is not so unusual for an oblique surface. In that case we'd 
have to add up the new contributions from each intersection. But we'd
also have a different P in each direction, and we don't store it. Tricky.
The calculation of P (coef) is elaborate. Requiring two passes through
the stencil. In effect the whole cijroutine. ]

It seems as if one might need quite a lot of extra stored information
for an object that had variable boundary condition, e.g. potential. 
At the least one needs to store the Ps separately from the cij (in the 
bulk cij=P) so as to be able to adjust their boundary contribution.
Perhaps the way to do this is to implement a new type of object, and
handle the difference equation differently at its boundary. One might
chain extra object data to the end of this cijdata, when needed.

Or possibly just make more space (which would be less complicated):
Currently cijdata count is 24 total. If we add one (P) for each direction,
(x6) that would bring it to 30, a 25% increase, not too desperate. 
But that might not really be enough, since one really needs a pointer
to the variable c-data in each direction. Perhaps that pointer is what
one really needs. It could be a pointer to a total data structure which
has sufficient information to return whatever value is needed for this
adjustment. This could be embedded in ddn_sor through an examination
whether this pointer is non-zero. 

In effect, then, we need the dob_sor to include for each monitored 
point a pointer to extra structures that fully determine the additional
adjustment to be done in ddn_sor. This is more elaborate than what we
have at the moment which is simply for this point add on the denominator
and numerator adjustments, precalculated. 

What is needed in the new structure? 
     a reference to where am I going to get the variable data from
     what that variable data is in the form of
     any coefficients that are needed to add it, such as:
     the P's (i.e. coefs) in relevant directions

Do we need this for 2*ndims directions? One might assume that there 
is never an opposite intersection, so only ndims directions were needed.
But I think this is unnecessarily dangerous. So make it for every direction.
Then the actions called for in this new structure are 
     Look through all directions from point.
     If we need to for this direction
     	 get the variable data
	 multiply it by the relevant weight and add to adjustment
	 calculate the new c/a and b/a and store.

So for each direction we need 
     A flag to say whether we are doing anything here
     A pointer to the place where the variable data is (maybe same as flag). 
     Some coefficient(s). 
     At least 12 storage spaces. 24 ought to be plenty. 
Therefore we can use the same space and pointer arrangements that
are handled by objstart. But we would have a different meaning for
the data in the structure. We would only start this secondary data
in special cases where there was variable boundary value to be accessed. 

There's another issue. interpolations use the fraction, boa, and coa
information. If that is not updated, then the field finding is wrong. 
It has to be updated to include the effective c/a and b/a. (Fraction
does not change if the objects don't move.) ddn_sor could presumably
do that updating. 

Note that if the update consists of putting the boundary potential
equal to some value set by the ion flux, then it does not need to be
done every sor-iteration, only every time-step. Consequently, there
might be a better way of doing this that did the update, including the
update of the ddn_parameters idgs_sor and ibdy_sor and of coa, boa,
and then the iteration just uses those parameters for the entire
solution of this step. Given that the sor iteration is a substantial number
of steps, then doing the update over the entire cij array is a tiny
cost. This does not then accommodate iterative changes within the potential
solution to the boundary data. But it's not clear that's necessary. 
An approach that tracked the ion surface charge and then updated the 
total charge based upon electron flux might benefit from such a scheme,
but probably mostly if the potential is changing a lot in a timestep,
in which case, the potential control/adjustment seems to make more sense. 

Doing the update over the entire array would be wasteful if done every
time step, because of the additional searching for non-zero pointers.
Doing within sor updates is then less expensive (but maybe not by a
big factor). 

What happens if I want a dielectric sphere with surface charge? The two
contributions (polarization and free) to the surface charge might be
incompatible other than in the form of tracking the free. But then
I would not be tracking boundary potential. Seems an abstruse case. 

Summary. We can do the potential/surface-charge variation with a chained
structure of the same size as the existing cij extra data. It might need
to be accessed only once per time step. Quantities in the primary data
such as boa, coa need to be updated. In fact from the iteration's perspective
those parameters (and ibdy, idgs) once updated can be used identically 
to now. 


---------
Distraction. Get rid of old unused fillin. We are using fillinlin. 
Trying to figure out how to make it unnecessary to set pointer 
in all nodes of a box containing a node that really has an intersection. 
Notes say getpotential and fillinlin are broken. But I don't see how
fillinlin is broken by this. Or getpotential, which just uses it. 
They just use getfield. Maybe getfield is broken by it? 
Switching this code section off changes the flux density reported by
make (by a smallish amount so it's not clear why). That seems to 
show that there's something different. getpotential is only called
in padvnc, and setting the value called to zero in both cases does not
give an identical result. Therefore it is not the value returned by
getpotential that is affecting things. The notes of 21 July 09 describe
why I ended up with this test in. I suspect that getfield is where 
the issue actually lies rather than getpotential. Yes it is gradlocalregion
that needs the pointer information. Fix misleading comment. 
---------

Adjust the _sor data structure to include iextra_sor pointer to additional
data. 

Flux dependence encoding of the input data:
ocenter, oradius, ovec, ocontra etc remain with the same meanings.
oabc presumably have different meanings.
ofn1-3 etc remain as before. 
Perhaps ofluxtype changes currently it means the number of fluxes.
Anyway, we currently have 3 parameters (oabc) that can be used to 
specify how the BC depends on flux. They are used in setting some
of the default code a/b ... Perhaps we simply default to the same
and use the otype to tell us what to do. 

The standard thing to do is to set potential to be such that the
electron flux is equal to the ion flux. For this we need the mass
ratio over Z SCEPTIC has

         flogfac=0.5*alog(2.*pi/(rmtoz*1837.))

Actually flogfac depends on magnetization. The above is for zero B. 

           fluxofangle(j)=finthave(j)*(nthused)/
     $              (4.*pi*rhoinf*dt*r(1)**2)

           phi(imin,j)=(alog(fluxofangle(j))+flogfac
     $                    +(ncs-1.)*phi(imin,j))/ncs

So this is averaging phi over something like ncs(50) steps. Maybe finthave
has already been averaged? Yes it has. flogfac is the log of sqrt of
(2.*pi/(rmtoz*1837.)), and the solution is such that
fluxofangle = sqrt(2.*pi/(rmtoz*1837.))exp(phi)

fluxofangle is the number of counts per unit time per unit area normalized
by rhoinf. We need to know the area of our facets to get flux density. 
Fluxes are acquired by all nodes in fluxreduce. 


19 Jan 11

A key question, it is now clear, is whether one should implement new 
routines for the dynamic update of the cij parameters during iteration
(triggered by flags) or whether one should use the same cij setting 
code we currently have, but only once per time step. In effect, calling
the cij routine for a variable-BC object might be what one wanted to do
right from the get go. Doing so once a timestep might not be very 
costly especially if there were a way to skip nodes that were known not
to be affected by variability. If this view were taken, then one would
embed the variability into potlsect setting of conditions. 

The weakness with this approach is that it is wasteful to determine
the intersection on each iteration, since that is not going to change
for stationary objects. So we would definitely not want to do potlsect
exactly as it now exists. We really need a way to store at least the
reference to the flux bin for each direction. That is, the geometry 
is one calculation, the flux is another, the conditions depends on flux,
but the geometry does not change. We might however go through whatever
conditions calculation is implemented at each timestep.

Decision is to set iobj,ijbin and some other stuff for each direction
intersected within a chained code during initial cijroutine, then
reset what is needed separately.

Implemented chaining and iobj,ijbin setting.
Use spherefsect for sphere cij because we now need the ijbin.
Had to move the fluxdatainit before cijroutine otherwise the ijbin
is not calculated correctly.

It seems we perhaps ought to use the 1-nf_posdim locations to store the
facet areas, because it is not straightforward to calculate them on the
fly. I previously increased nf_posdim to 3 because of needing cube positions
in 3-d. But actually, I haven't even implemented the calculation of that.
Instead the positions are being deduced algorithmically. The areas
are significantly harder to deduce algorithmically and probably ought
to be stored.

Ellipse arc length for major and minor axes a,b as a function of angle
parameter theta is sqrt dl= (dx^2+dy^2) = sqrt(a^2 sin^2 + b^2 cos^2) dtheta
= a sqrt(1+ (b^2/a^2-1)sin^2theta)  dtheta
 is  a.E(theta,k) where k=sqrt(1-b^2/a^2),
E(theta,k)= \int_0^theta sqrt(1-k^2sin^2theta) dtheta

It's all too messy. For now just flag that the calculation is not right
if the axes are not identical. There's a calculation in my notes for
a case where two of the axes are the same length. But for now just 
divide 4pi r**2 by the total number of facets (theta and psi). That's 
correct for a sphere. Probably one could get the surface area numerically
using the same sort of scheme as used for the objplot, adding up multiple
quadrilaterals. 

Pulled the area data into the third slot of dob_sor. Seems to work. 

So now the plan is when evaluating at node ij either in the iteration, 
or more likely in a separate coefficient update step:
If cij iextra set
   for 2*Nd directions
      if iobj ne 0 reevaluate potential by
         get flux using ijbin, iobj, and time average
	 divide by area, dt, and coefficients
	 calculate corresponding potential
	 reenter the relevant main cij parameters boa, coa, ...?
      endif	
   endfor

20 Jan 11

For recalculating the cij parameters, it seems it would make sense if
we used some of the same code that is currently in cijroutine. We
don't need to do the boxedge section. And we don't need to do the
potlsect section. But we do need the dplus deff section perhaps. 
After we've recalculated the abc. No. This is too cumbersome. Really 
what we need is to have the coef for all directions stored (this is 
in cutcartesian notation P). Then most of cijroutine is redundant.
Probably we need to store coef in the third slot of dob_sor instead
of the area, and get the area from the flux location directly. If so,
then we will need to know the object number.

Implemented cijupdate (a code for mditeration). It seems to work
setting the potential boundary condition to the local floating potential. 

Time costs with cijupdate 5.423, without 4.759 for 20 steps of
standard settings. These are very fast steps with ni=100k. Costs are
not zero but hardly significant. They probably could be improved by not
iterating over the entire cij mesh, only over the stored data.

Did a test with different initial potential. It converges to the same
answer.

Did some tests with different -l and -v they look qualitatively very
sensible. These are with 5x5 flux facets.

Also tests with different flux facet numbers up to 20x20 and down to 1
seem to work.  Thus we have the ability to do floating spheres as well
as insulating.

Now I have to establish whether the absolute values make sense.

First. Rewrite this so that it does not iterate over the whole of the
mesh, instead it just iterates over the auxiliary data up to oi_sor.
This should take negligible time. It seems to give just the same 
result. Good.

Timing. Old update 5.234. New 5.201. This shows the real cost even of the
mditerate version is very small. I think the differences observed above
had more to do with changes in the sor solving because of different phi
that with the additional code execution. Still it is more elegant
to do the update sensibly rather than looking everywhere. 

With -s500 and 
1025 , 1.,0.,5., .0,.0,.0,  1.,1.,1.,   1, 5, 5, 
we get:
nrein,n_part,ioc_part,rhoinf,dt= 2394    96132    98569   100.014     0.100

 Average flux quant  1, object  1, over steps 250 500, per unit time:  1730.514
 rhoinf:  100.014366 Total: 43436  Average collected per step by posn:
    6.76    7.03    6.70    6.77    6.65    6.96    6.70    6.81    6.98    7.28
    6.93    6.93    6.80    6.96    7.27    6.96    6.95    6.77    7.01    6.90
    7.06    6.96    6.89    6.94    7.08
 Flux density*r^2, normalized to rhoinf  1.37690246

phiexamine shows the probe potential is about -2.6-7

A very profitable day. Thanks be to God!

21 Jan 11

Doing a hand calculation of the floating potential. The flogfac for hydrogen
is -5.678. If area is .5, rhoinf=100., dt=.1, count=5, then fluxdensity=1,
and so the floating potential is 0.5*flogfac=2.84. That's just what the 
phiofcount is getting. I think it's about right.

There's an issue with the whole approach for a floating object. It is
that we might well wish to obtain the angular dependence of the flux
in an object that was a floating equipotential. That could not be done
simultaneously with the present implementation. It could perhaps be done
by introducing an additional ghost object for the flux accounting. But
that's a bit cumbersome. 

Implemented a type 5 to do floating equipotential by summing the flux 
and area over the object.

The the local flux is implemented only for spheres. 
To make things work for other objects, we need to initialize their 
facet areas, which is not done yet.

Now we have going
   1. Flux plotting.
   2. Gradients of boundary conditions. 
   3. Collisions
   4. Insulating surfaces.
   5. Floating objects.

Things still to do. 
       Magnetic Field
But
       Surface charges
does not seem so pressing because of the considerations that have shown
that floating/insulating objects are not well treated by using a surface
charge. Their potential should be controlled by flux density. 

Magnetic field implementation.
I think it is essential to be able to have the magnetic field point
in any direction. The way to prescribe this is to have a vector B(ndims)
and to have switches -Bx -By -Bz (default B=0). ndims is not defined
in plascom. I guess we need to set it: nplasdims.
Ok built code to read in those components and calculate Btotal. 

The cyclonic integrator does rotation of the Larmor radius and velocity 
about the (total) magnetic field by an angle \Omega.dt. We should regard
the B as \Omega in normalized units. It has components and a total. 

For rotation, we simply need to multiply the vector by the rotation matrix.
Wikipedia says that if u is a unit vector in the direction of the axis
the rotation matrix is 
R =[ cos(t)+ux^2(1-cos(t)),ux.uy.(1-cos(t))-uz.sin(t),
					ux.uz.(1-cos(t))-uy.sin(t);
   ...;
   ...]
= uu + cos(t)(I-uu) + sin(t) u_cross = I cos(t)+uu(1-cos(t))+u_cross sin(t)
where 
      u_cross = [ 0, -uz, uy; uz, 0, -ux; -uy, ux, 0]
and this rotation when applied by premultiplication on vector v
is said counterclockwise about the direction of u. 

In sceptic3d Leo allows the B field to have a cosine angle cB so that
the following rotation supposedly makes it in the new z-direction
temp=xp(2,i) xp(2,i)=temp*cB-xp(3,i)*sB xp(3,i)=xp(3,i)*cB+temp*sB the
1-component is unaffected, so it must be being presumed that the B
field has zero component in the 1-direction. I probably don't want to
assume that. It's more straightforward to use the general rotation,
but I'm not sure how bad the extra cost will be. Also, the external
ExB drift is subtracted off before getting the gyro radius. That's
reasonable and might be needed here. 
When should one subtract off ExB drift and when not? Probably only when
it is constant.

Got most of the B-field advancing going (without ExB drift). The ions
appear to be rotating counterclockwise when viewed along the field. 
This decided by using -go and figuring out which way the particles are 
moving along their orbits.
That's correct polarity. Seems to be working. Magnetic field implemented.

Good Day!

Needs: area calculations for non-spheres.

24 Jan 11

Parallelopiped area. Face area is magnitude of vector product of the
two edge vectors. So 4 times the "radius" vector cross product. This
is then divided by the product of the facet numbers in either direction.

Implemented for parallelopiped and cube (pretty much the same).

Cylinder? Ends are each pi r^2. Side is 2pi r 2z. We have issues
with unequal radial radii as with the sphere. Don't account for
ellipses yet. Implemented.

Now we need to increase the nf_posdim again because we need to store
3 position references in addition to the area. 
Reorganized the references to be through mnemonics nf_pr nf_pt nf_pa...

Increase. Ok. Implement cylinder consistently. Done. 

Cube/Pllel positions not yet done. 
Cube done. But I'm not sure that cube is really working right.
There was a nf_posdim logical error when it was increased.

Ran out of steam. Cube may be done, but there may be an error in the 
flux read/write since it looks very different from the in-code plot.

The Average flux quant 1 printout is the same. 
The in-code flux of step plot is crazy.
Found a bug in that, which was old. Correct it.

Found that the problem with cube plots from fluxexamine is that 
nf_faceind is not being written and restored. Actually it is 
now incorrectly defined as dimension
      integer nf_faceind(nf_quant,nf_obj,2*nf_posdim)
nf_posdim is not the right dimension, since it is now detached from 
the dimensionality of the position info.  The same is true of 
nf_dimlens. This confusion needs to be cleared up. In some places 
it needs to become nf_ndims.
Then implement writing and reading back of nf_faceind. 
We get correct display from fluxexamine. 

Hopefully we are now done except for pllel positions.

25 Jan 11

Add some switches to fluxexamine.
Test coptic with two processors. Seems to be working.
Put zoomrotate into objplot to improve viewing.

Commit. 

Still to do: cylinders of arbitrary orientation.
Input: otype, ocenter (3), axisvec (incl length) (3), radius (1)
Do we need more than one radius?
Could have two radius vectors, but that would be a real headache to 
calculate correctly. Also we don't currently handle unequal radii. 

Algorithm for intersections with general cylinder:

	Get fractional distances from center in the axial direction as
a fraction of the axial length (multiply by inverse axial vector?).
Get the distances from the axis divided by the radius. This is then
rendered into unit cylinder coordinates. Implement unit cylinder code.
This could then be used for the other cylfsect too, if
desired. Actually we need both radius and angle in the transverse
plane.  (Or 2-D cartesian position). Where are we going to measure the
angle from? Need a stable and complete specification. Perhaps we 
project the x-axis on the radial plane? If x aligns with axial, this
becomes singular. Use a different axis then? y?

Might be best to construct the contravariant vectors from the input.
Such that v_contra.x = cartesian coordinates in a frame in which z
is along the axial direction and others are normal to it.

26 Jan 11
Implemented cylinit to initialize the contravariant vectors. Needs a
second vector to be prescribed.

Started to implement normalized cylfsect. It's not working. But also,
I've found a bug in the old one with intersection binning.
Fixed the bug. Improved the cylinder end facet labeling.
Found the problem with normalized cylfsect and fixed.

After further corrections I seem to have cylfsect working to the point
where the volumes and cij plots look sensible. Objplot is not yet working.
It needs a reverse transform from the unit cylinder coordinates back
to world coordinates. With pllel this was automatically given by the 
covariant vectors. In our case, the covariant vectors are not automatically
present, because the specification of the cylinder is not done by them. 
Actually the covariant z-vector is present, but in the wrong place. 
We could make the initialization routine fill in the covariant vectors,
but that would overwrite the read-in data. Would that be a problem?
Probably not. Because they are orthogonal, the covariant vectors are equal
to the contravariant vectors divided by their magnitude squared. It 
would therefore be possible to store the three magnitude but pretty 
cumbersome. 

Although installing the contravariant vectors works, it erases the 
radius. That's a problem, I think.  Maybe not.

Positioninit done just in terms of the normalized cylinder (for now?)
Now we have intersections working and giving flux quantity in bins. 
Implement world3contra and contra3world to transform from world to
contravariant coefficients and back. 
Got cylgplot working using these transformations. It is rather
duplicative of cylplot, but never mind for now.

I think that's it. We've got general cylinders implemented.
Add to geommany.dat

We have a Tallyexit error on object 5, a coordinate-aligned cylinder.
It does not happen with the old cylfsect. So there's a bug in the
new one that does transformation to unit cylinder. Must fix.
But for today, I'm tired.

28 Jan 11

That seemed to be an error in the transformation into unit cylinder code.
At least there certainly was an error and now the Tallyexit has gone
away.

The flux plotting in objplot probably would be more useful if it
colored by flux _density_ rather than flux. This is different for
non-spheres, for which areas are not all equal. At the moment, we
store the averages in maxsteps+1. It would be possible to store the
averages divided by areas in maxsteps+2. Implemented.

Actually I've been using maxsteps+1, but that's not what I want. I want
to use nf_step+1 (and nf_step+2) which is the number of actual steps+1/2
because otherwise when writing/reading one does not do so for the relevant
data. Changed that.

Implemented -gw switch to control coloring and annotation of surfaces.

29 Jan 11

Clean up. Commit.

We now need to get back to thinking about forces, I think.  At the
moment forcetrackinit is implemented only for spheres.  Stress
integrations are based upon the surfobj structure which has, for each
facet, position and area vectors. The number of facets in the theta and
phi directions are defined in 3dcom.f Other object types are
ignored. In principle that's all we really need to do for other
surfaces to work. Likely the force evaluation works better somewhat
away from the bounding surface, where the interpolation is more accurate.

Compiled with gfortran and got rid of most warnings but not all. 
Also corrected one bug found thus. It's still a factor of 2 slower.

31 Jan 11

I think my approach to averaging is incorrect. The code assumes that the
data has been initialized to zero in the new slots we are writing in. 
Hence if fluxave has been called, and has written in the current slot
that's a mistake. We need to write it just past the last step, not the
current step. This is a problem because the last step is not kept in 
common. It is determined by nsteps in the main program. Since fluxave
is only called at the end, this will not cause a problem unless we
change that.

Fix addressing bug in sphere positioninit.

Dealing with another bug in flux accumulation that shows up when
using floating sphere. There appear to be random negative flux entries
in the ff_data ahead of where we are. 

Found an error in spherefsect that only mattered when the sphere was
not centered. This is the first time I've collected flux for a non
centered sphere.

1 Feb 11

Running some test cases with fairly big domains and point charges
to try to see something about force. Found a bug in that the 
field calculation did not use the object center. Corrected. 
Trouble is that the forces seem to be down in the noise level.
These are for point charges with -.04 at r=5. Which is pretty small.
Boost that to -.4 at r=5. There there's what looks to be non-zero
field force. Much more on the back particle, which probably makes
sense given the particle repulsion. But still there's very big
fluctuation in the particle momentum component presumably because the
collection crossings are just a few per step.

Does this make sense? there are 400k particles in volume 130x50x50
so density is about 1 (actually 2 because two processes). Reported 
rhoinf 2.35 about right. At velocity 1. timestep .1 there is going
to be about pi*.1*rho ~ 1 crossing in each direction per step for a
radius 1 sphere. So yes, it makes sense. We need a smaller domain
for debugging the forces. 

2 Feb 11

The point charge influence is not currently included in the force
calculation, because addfield is not included in the fieldatpoint.
What does that mean? It means the charge ought to include only the
shielding charge, not the point charge. That's a bit puzzling in
respect of the sign of the apparent charge (negative). Ought to be
positive I think. Actually the charge is calculated using field at
point times surfobj. 


The charge is in general 4 pi r^2 times the gradient at surface
(averaged). If the Yukawa approx holds, it is phi(r) 4pi r^2*(1/r +
1/lambda). So for r=1, phi=-2, lambda=1. it should be -50.3 for an
actual object. For a point charge there's a question.

Here is the result for a series of radii: 1. 1.2 1.5, 2 3
at -l100
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  1
     0.44564    -0.10514     0.00000     0.34050   -24.09440
================== End of Object 2 ->  2
     0.34017     0.19731     0.00000     0.53748   -25.00404
================== End of Object 3 ->  3
     0.15454     0.37895     0.00000     0.53349   -25.09287
================== End of Object 4 ->  4
    -0.00017     0.09210     0.00001     0.09194   -25.04920
================== End of Object 5 ->  5
     0.09503    -1.49080     0.00002    -1.39575   -25.02007
These are correct to the analytic result 25.1 to within half a percent
except for the first object which is the radius 1 sphere itself,
and we are extrapolating to the boundary for that, so we expect it to
be less accurate, perhaps.
For -l1.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  1
     0.40185    -0.81520    -0.00044    -0.41379   -42.66593
================== End of Object 2 ->  2
     0.15460    -0.40849     0.00360    -0.25029   -41.78420
================== End of Object 3 ->  3
     0.01874     0.10315     0.00845     0.13035   -37.08116
================== End of Object 4 ->  4
    -0.04777    -0.45493     0.01364    -0.48906   -28.70999
================== End of Object 5 ->  5
    -0.02963    -0.76805    -0.05416    -0.85184   -15.78975
This is a bit less than the analytic value, but quite likely this is 
a combination of non-linearity and boundary extrapolation error.

Here is what we get with a point charge of -2. at  1. with -l100
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.28349    -5.29697     0.00000    -5.01348   -24.91903
================== End of Object 2 ->  3
     0.29647    -5.88335     0.00000    -5.58688   -24.97408
================== End of Object 3 ->  4
     0.09009    -5.08781     0.00001    -4.99771   -24.91823
================== End of Object 4 ->  5
     0.10640    -4.33492     0.00003    -4.22849   -24.87956
Looks as if the point charge IS counted in the charge. Yet force seems
pretty crazy. This is what we get with -l1. :
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.05172     0.63756    -0.00032     0.68896   -16.00428
================== End of Object 2 ->  3
     0.07639     0.63138     0.02616     0.73394   -12.89612
================== End of Object 3 ->  4
     0.02346     0.17503     0.12118     0.31967    -8.94730
================== End of Object 4 ->  5
    -0.00932     0.89757     0.13519     1.02344    -4.66365
Seems rather a lot lower than I might have expected, yet perhaps
the extra shielding inside r=1 is the reason. As an approximation,
The potential would be exp(-1) smaller from that effect:
at r=1 we have 42.6/16.0 = 2.66 ~ exp(1) so it makes sense.
Looks as if the charge is being correctly calculated including the 
point charge. 

I think this makes sense because the pointcharge I am using extends
only to radius 1. That's the place where the effective pointcharge has
gone to zero. Consequently, my calculation outside of that region should
be correct even though the extra point charge field is not accounted for.

If I were inside that radius, then I would expect the charge to be wrong.
Here's what we get for a ptch radius 2. potl -1.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.00696     0.62275    -0.01049     0.61922    -2.68375
================== End of Object 2 ->  3
     0.03578     0.47410     0.01403     0.52391    -5.96094
================== End of Object 3 ->  4
     0.02476    -0.06874     0.11036     0.06638    -8.62990
================== End of Object 4 ->  5
    -0.00941     0.87602     0.13223     0.99884    -4.60279
Basically it agrees in charge for r>=2.

Conclusion: Maxwell stresses work only outside the defined size of 
a point charge. 

Here are the force summaries for r=1.2 to 3. for  -v1. -s500 -l5.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     4.73587     6.05562     0.02025    10.81173   -24.35178
================== End of Object 2 ->  3
     3.85421     6.99056     0.04728    10.89205   -24.04704
================== End of Object 3 ->  4
     2.68490     8.25658     0.12276    11.06424   -23.23796
================== End of Object 4 ->  5
     1.17217    10.02062     0.35843    11.55121   -21.32471

Looks fairly well converged, and field is significant component
especially close. 

Here's a case with ptch r=.5, -4. with flux collection at .8, 1. 1.2 1.5 2 3
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     5.12855     4.30121     0.00359     9.43335   -22.22561
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     4.40905     5.04523     0.00984     9.46412   -21.94547
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     3.86179     5.66072     0.02109     9.54360   -21.92307
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.13189     6.43115     0.04756     9.61061   -21.57910
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.17909     7.48203     0.11928     9.78039   -20.83145
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     0.96240     8.96126     0.33762    10.26127   -19.09183
It's a bit lower. Is that reasonable? Not so clear. Mesh spacing is 
10/32 ~ .3. Not much smaller than the point charge size. Might be an 
issue. Try with larger nmesh 10/50
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.37185     4.49729     0.00322    10.87236   -24.86204
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.47498     5.41855     0.00930    10.90283   -24.67407
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.74671     6.18794     0.02015    10.95480   -24.52661
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.83298     7.12687     0.04705    11.00690   -24.05890
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.66614     8.40752     0.12212    11.19578   -23.31898
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.17042    10.17546     0.35528    11.70116   -21.41303
Yep. Looks as if we did not have sufficient resolution.

Evidence is that the force and charge is working pretty well for -l5
out to r ~ 2.5.  Possibly there are influences from the boundaries
outside that, but even so one ought to have momentum conservation,
so there is something to explain in that the force has increased
by about 9%.

Try with -dt.05 half the timestep:
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.18294     5.36787     0.00312    11.55393   -24.85392
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.33291     6.22361     0.00905    11.56556   -24.66480
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.64108     6.94924     0.01963    11.60995   -24.51566
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.75168     7.87032     0.04595    11.66795   -24.04460
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.61367     9.13149     0.11955    11.86471   -23.29948
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.14207    10.87260     0.34823    12.36290   -21.38397
Force looks substantially better converged. But also substantially higher.
Charge is hardly different. I wonder if the electron pressure term has
the right sign. 

If it were opposite, the force conservation would be very good.
Pressure force is: 
            force(i)=force(i)+surfobj(koff+ndims+i)*exp(phi)
Maxwell force is:
         do i=1,ndims
            do j=1,ndims
               stress=field(i)*field(j)
               if(i.eq.j)stress=stress-es/2.
               fieldforce(i)=fieldforce(i)+surfobj(koff+ndims+j)*stress
            enddo
         enddo
Thus the magnetic pressure is applied with a sign -es/2. whereas the 
electron pressure is applied with a sign +. These seem to be opposite.
Which can't be correct. Not sure. Need to think more about this.
The EM force density on a volume is Div.T where T= \epsilon_0[EE - E^2/2]
By contrast the pressure force density is -grad p. Thus the E^2/2 term
ought to be added to the force with the SAME sign as p. 

Try with -dt0.025.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.01259     6.14960     0.00303    12.16523   -24.84747
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.22286     7.01671     0.00881    12.24838   -24.65675
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.56712     7.71234     0.01917    12.29862   -24.50600
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.69267     8.63389     0.04504    12.37160   -24.03256
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.57710     9.83186     0.11744    12.52640   -23.28122
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.11903    11.51102     0.34266    12.97271   -21.35380
Seemingly no better converged. Modify the code sign and rerun:
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.01259     6.14960    -0.00303    12.15916   -24.84747
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.22286     7.01671    -0.00881    12.23077   -24.65675
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.56712     7.71234    -0.01917    12.26029   -24.50600
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.69267     8.63389    -0.04504    12.28153   -24.03256
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.57710     9.83186    -0.11744    12.29152   -23.28122
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.11903    11.51102    -0.34266    12.28739   -21.35380
This is consistent to better than 1% Clearly the change is correct.

It looks as if the magnitude of the step size does have a strong 
influence on the apparent force. So does the mesh size. 
Basically the force now seems to be working for spherical measuring
surfaces.

The old force paper says that the domain has to be at least twice
lambda_D to get the force right. So these domains aren't big enough.

3 Feb 11

Trying again with floating sphere. Still have a problem with negative
counts, apparently arising from spurious ff_data value entries.
I thought I'd fixed that. Yes I had. I was not excluding the floating
sphere from the region. That was an input file error.

Here's -l5 -t.1 -v1. -dt.05 on a 10x10x10 size 50x50x50 number
 Flux density*r^2, normalized to rhoinf  1.56599867
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    52.40706    32.18951     0.00168    84.59825   -36.26878
========== Object 2 ->  2 radius=  1.200 ========
    42.01913    35.39398    -0.02149    77.39163   -36.11956
========== Object 3 ->  3 radius=  1.500 ========
    34.98729    41.46753    -0.10783    76.34700   -35.83485
========== Object 4 ->  4 radius=  2.000 ========
    25.78210    50.83405    -0.44894    76.16722   -34.85485
========== Object 5 ->  5 radius=  3.000 ========
    12.82122    65.19434    -1.89569    76.11987   -31.77205
Sceptic got force 81.8, charge 37.7 fave=1.50
So coptic is getting a slightly smaller force and charge, with a slightly
higher flux, which presumably makes the floating potential a bit less
negative and might explain the force and charge differences. 
According to my calculation the floating potential in coptic is -2.39048
In sceptic it is saved as -2.44023. 

Using instead -dt.025 we get:
 Flux density*r^2, normalized to rhoinf  1.46931636
 Floating potential= -2.45420861
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    52.92072    31.25940     0.00173    84.18185   -36.16894
========== Object 2 ->  2 radius=  1.200 ========
    42.38842    34.59715    -0.02204    76.96353   -36.01539
========== Object 3 ->  3 radius=  1.500 ========
    35.25710    40.69879    -0.11040    75.84549   -35.72120
========== Object 4 ->  4 radius=  2.000 ========
    25.90463    50.27276    -0.45853    75.71886   -34.72077
========== Object 5 ->  5 radius=  3.000 ========
    12.78950    64.85056    -1.92698    75.71309   -31.56680
========== Object 6 ->  6 radius=  5.000 ========
     1.84271    80.31524    -6.17617    75.98178   -23.68394
A lower floating potential but no better force agreement. 
Looking more likely that the discrepancy is domain size.
Try instead with -dt.1 :
 Flux density*r^2, normalized to rhoinf  1.56204593
 Floating potential= -2.39300942
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    53.72910    33.49794     0.00167    87.22871   -36.49997
========== Object 2 ->  2 radius=  1.200 ========
    43.07132    36.76583    -0.02142    79.81573   -36.35361
========== Object 3 ->  3 radius=  1.500 ========
    35.86864    42.91519    -0.10785    78.67598   -36.07600
========== Object 4 ->  4 radius=  2.000 ========
    26.39515    52.41656    -0.45035    78.36136   -35.10577
========== Object 5 ->  5 radius=  3.000 ========
    13.21550    66.83462    -1.90827    78.14185   -32.04988
========== Object 6 ->  6 radius=  5.000 ========
     1.88130    82.67591    -6.19397    78.36324   -24.44571

Using a domain 15.^3 we get (-s500 -dt.1 -da8 -v1. -l5. -t.1)
 Flux density*r^2, normalized to rhoinf  1.55521822
 Floating potential= -2.39739013
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    56.00081    33.54607     0.00416    89.55104   -36.63660
========== Object 2 ->  2 radius=  1.200 ========
    44.98659    36.85350    -0.02179    81.81830   -36.48862
========== Object 3 ->  3 radius=  1.500 ========
    37.71225    43.03333    -0.11243    80.63316   -36.20971
========== Object 4 ->  4 radius=  2.000 ========
    28.08273    52.67813    -0.47333    80.28754   -35.23322
========== Object 5 ->  5 radius=  3.000 ========
    14.53309    67.52608    -2.03984    80.01932   -32.14792
========== Object 6 ->  6 radius=  5.000 ========
     2.86774    84.40074    -6.98550    80.28297   -24.58187
Thus the shortfall is to do with the domain not being big enough. 

Actually, judging by the potential plot the biggest problem is having
enough room on the downstream side. 

Try a different layout:
91,1,15,36,50,0,-10.,-3.,3.,10.
92,1,15,36,50,0,-10.,-3.,3.,10.
93,1,8,29,50,0,-5.,-3.,3.,20.
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    49.05901    30.66676     0.00020    79.72597   -39.57825
========== Object 2 ->  2 radius=  1.200 ========
    38.68083    33.98512    -0.01521    72.65074   -39.46384
========== Object 3 ->  3 radius=  1.500 ========
    31.12519    40.37292    -0.07865    71.41946   -39.26323
========== Object 4 ->  4 radius=  2.000 ========
    20.89518    50.49304    -0.33271    71.05552   -38.45021
========== Object 5 ->  5 radius=  3.000 ========
     6.01135    65.98265    -1.30534    70.68866   -35.77306
========== Object 6 ->  6 radius=  5.000 ========
    -7.66802    79.41839    -1.35357    70.39680   -29.23763
Rather unsuccessful. No! I accidentally set the -t to .05.
Here's from -n 8 with  -s500 -dt.1 -da4 -v1. -l5. -t.1 -ri70
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,8,29,50,0,-10.,-3.,3.,20.
 Flux density*r^2, normalized to rhoinf  1.51346183
 Floating potential= -2.42460632
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    54.36328    33.12401     0.00478    87.49207   -36.79696
========== Object 2 ->  2 radius=  1.200 ========
    43.79966    36.23051    -0.02060    80.00957   -36.65427
========== Object 3 ->  3 radius=  1.500 ========
    36.79764    42.14454    -0.10813    78.83405   -36.38494
========== Object 4 ->  4 radius=  2.000 ========
    27.46853    51.54213    -0.45674    78.55392   -35.42867
========== Object 5 ->  5 radius=  3.000 ========
    14.17500    66.08906    -1.97374    78.29031   -32.40749
========== Object 6 ->  6 radius=  5.000 ========
     2.64290    82.94704    -6.85779    78.73215   -24.91580
One can see that the leading edge at z=-10 is in fact significant in
the potential solution. It looks as if even the leading edge needs
to be far away. 

With
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,15,36,50,0,-20.,-3.,3.,20.
we get 
 Flux density*r^2, normalized to rhoinf  1.54259562
 Floating potential= -2.40553951
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    55.57957    33.41187     0.00557    88.99701   -36.60782
========== Object 2 ->  2 radius=  1.200 ========
    44.81544    36.56225    -0.02148    81.35622   -36.46081
========== Object 3 ->  3 radius=  1.500 ========
    37.71716    42.56567    -0.11317    80.16965   -36.18245
========== Object 4 ->  4 radius=  2.000 ========
    28.22109    52.14386    -0.47762    79.88733   -35.20405
========== Object 5 ->  5 radius=  3.000 ========
    14.76352    66.94378    -2.07169    79.63561   -32.12032
========== Object 6 ->  6 radius=  5.000 ========
     3.12687    84.30839    -7.06419    80.37107   -24.36137
Looks like we've got what we'll get. 

Increase to -l10 (floating. 20.^2)
 Flux density*r^2, normalized to rhoinf  1.58223844
 Floating potential= -2.38016534
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    82.47508    34.32501     0.00254   116.80263   -32.96266
========== Object 2 ->  2 radius=  1.200 ========
    69.27230    36.64183    -0.00902   105.90512   -32.95042
========== Object 3 ->  3 radius=  1.500 ========
    62.79748    41.54189    -0.04652   104.29284   -32.96438
========== Object 4 ->  4 radius=  2.000 ========
    54.03246    50.05941    -0.19974   103.89213   -32.73141
========== Object 5 ->  5 radius=  3.000 ========
    39.37125    65.06497    -0.96813   103.46809   -31.79218
========== Object 6 ->  6 radius=  5.000 ========
    20.33151    88.01800    -4.43408   103.91543   -28.86920
Compare with sceptic:
==> T1m1v100r20P02L1e1.dat <==  
 dt       vd       Ti  steps   rhoinf     phiinf   fave  debyelen  Vp / nr/ phi
0.05000 1.00000  0.1000  2000    190.8047    5.25125  1.5857 10.00000  -2.36912
 Charge      E-field       Electrons      Ions     Total
 -34.1119576  0.76158464 -6.75517142E-10  35.8557892  112.014252
 -3.82497454  0.0045971889 -52.9793701  163.565552  111.045898
Agreement is not great. But one can see that we have not incorporated
the whole of the positive potential peak. And not even all rho peak. 

T=.01 -l5 floating: T1m2v100P025L5e0z020x20.flx
 Flux density*r^2, normalized to rhoinf  1.6818924
 Floating potential= -2.31908631
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    73.62286    41.47784     0.00801   115.10871   -36.11523
========== Object 2 ->  2 radius=  1.200 ========
    58.49369    45.84773    -0.02990   104.31152   -35.93894
========== Object 3 ->  3 radius=  1.500 ========
    48.03215    54.48299    -0.15543   102.35971   -35.60561
========== Object 4 ->  4 radius=  2.000 ========
    34.47293    68.05175    -0.64462   101.88007   -34.51732
========== Object 5 ->  5 radius=  3.000 ========
    15.89227    88.27644    -2.70578   101.46294   -31.17728
========== Object 6 ->  6 radius=  5.000 ========
     0.27191   110.13190    -8.57866   101.82516   -22.76466
Sceptic equivalent:
==> T1m2v100r20P02L5e0.dat <==
dt       vd       Ti  steps   rhoinf     phiinf   fave  debyelen Vp  / nr/ phi
0.05000 1.00000  0.0100  2000    206.2824    5.32925  1.5893  5.00000  -2.37589
 Charge      E-field       Electrons      Ions     Total
 -37.1715889  2.51376629 -1.71099135E-10  40.6259079  103.470062
 -10.1660223  0.0073696305  47.7507477  52.3004951  100.235481
Flux agreement is not too good, but force is good. 

With the above meshes, the dz is about 6/20 at the probe, i.e. 0.33. 
Thus, the resolution near the probe edge is not great. There is a very 
steep gradient of density near the edge behind the probe. That seems 
probably to be the cause of the momentum apparent nonconservation. 
It seems that one probably needs to be at least one times dx from the
boundary for the force calculation to be reliable. 
Increase the z-mesh density to
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,15,58,72,0,-20.,-3.,3.,20.
Result:
 Flux density*r^2, normalized to rhoinf  1.67777085
 Floating potential= -2.32153988
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    65.69753    41.45311     0.00996   107.16059   -35.68893
========== Object 2 ->  2 radius=  1.200 ========
    58.64346    45.80416    -0.03127   104.41637   -35.92920
========== Object 3 ->  3 radius=  1.500 ========
    48.44096    54.44807    -0.15888   102.73014   -35.62333
========== Object 4 ->  4 radius=  2.000 ========
    34.61152    68.05910    -0.65155   102.01907   -34.54094
========== Object 5 ->  5 radius=  3.000 ========
    15.96391    88.41527    -2.73084   101.64834   -31.24056
========== Object 6 ->  6 radius=  5.000 ========
     0.26093   110.37730    -8.62403   102.01421   -22.76320
The error at the sphere is decreased by more than a factor of 2. But there's
little or no change at radius 1.2. Thus the result is not substantially 
better. But it does say that provided we avoid the object itself we
don't need to go to high resolution. 

Try to document the effect of walls. Move the particle to position x=10
in a 20^3 domain. We get
[./coptic test1.dat -s500 -dt.1 -da4 -v1. -l5. -t.1 -ri60]
 Flux density*r^2, normalized to rhoinf  1.53941882
 Floating potential= -2.40760088
    Field,       part,       press,       total,   ave over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -36.6542
     0.56419    -0.30757     0.00007     0.25669
    -0.11836     0.05651    -0.00001    -0.06186
    55.26081    33.32751     0.00551    88.59383
========== Object 2 ->  2 radius=  1.200 ========  Charge=  -36.5060
     0.50242    -0.29941    -0.00023     0.20277
    -0.10309     0.06281     0.00005    -0.04023
    44.48288    36.54395    -0.02127    81.00556
========== Object 3 ->  3 radius=  1.500 ========  Charge=  -36.2226
     0.46840    -0.23163    -0.00126     0.23551
    -0.12979     0.09512     0.00033    -0.03434
    37.31798    42.62213    -0.11201    79.82810
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -35.2406
     0.41672    -0.22586    -0.00571     0.18516
    -0.18482     0.17877     0.00205    -0.00400
    27.90334    52.16618    -0.47223    79.59729
========== Object 5 ->  5 radius=  3.000 ========  Charge=  -32.1702
     0.37379    -0.15386    -0.03115     0.18878
    -0.17052     0.15924     0.01519     0.00391
    14.49882    66.91687    -2.04443    79.37125
========== Object 6 ->  6 radius=  5.000 ========  Charge=  -25.6056
    -4.11204    17.45297   -12.43861     0.90232
    -0.11563    -0.01325     0.08904    -0.03984
     5.31671    81.13351    -6.06404    80.38618
There is an x-force but in the reliable regions it is only about 0.2
compared with the z-force of 80. This gives a feel for how much the 
proximity of a wall might affect us. About 0.2%.
Not a lot when we are half way out. Incidentally, the last object is
centered at x=12, not on the particle center. That's the reason for the
big field and part components separately (presumably).

12 Feb 11
Tried a long run on sceptic machine. Got tallyexit error with -v1
(not with -v1.5)
0091 149-0.001| 0092 150-0.076| 0093 147-0.094| 0094 145 0.056| 0095 152-0.012| 
0096 148 0.109| 0097 158 sd problem, fraction,type,No  0.  1. 1 6
 Tallyexit error 1 721545 6 113
 xpart,r=  4.45770645 -3.3805542  52.4564285  5.59457731 128
 xp1 -0.0105438232  0.00775885582 -0.114601135  0.013090915
rank 7 in job 54  sceptic.psfc.mit.edu_59249   caused collective abort of all ra
nks
  exit status of rank 7: killed by signal 9 

Tracked this down after a great deal of effort to a problem in spherefsect.
We are getting fraction =1+ from the sphereinterp, but this is actually 
a situation that inside_geom has decided is a crossing because a point
is exactly on the boundary. Then I was not setting sd to 0 but I was 
returning without calculating ijbin. The unset value of ijbin was being
used for an address with an overrun. The simple fix is to put sd=0.
Then tallyexit decides there's an error and I've now made it discard the
particle if such an apparent error arises. This works to prevent crashes
but with 8 processes 30M particles total, the number of such tallyexit
errors is annoying. One every 20 steps maybe. I'd prefer a better solution
that did not seem like an error. It seems that the rounding differences
arise because of the differences in inside_geom and sphereinterp 
calculations. 

Got some cases to run. There seems to be more y-force coming from the 
particles than is easily accounted for. It ought to be zero by symmetry.
On the second run this doesn't happen. Hmmm...

Made adjustments to spherefsect so that it permits fraction up to 1+tiny,
but only if sd is not equal to zero. Thus it errs on the side of allowing
a slight over-run of fraction when deciding whether to calculate the ijbin.
It is in principle possible for calling programs to reject cases that
are allowed through on that basis, because fraction is left slightly 
higher than 1, even though ijbin is calculated. This is not debugged
because it is only on major runs that this problem arises.

29 Mar 2011

Idea is to calculate the force on point charge object(s) from the field
(and any non-local contribution perhaps if any) within coptic and put it
out in the flux file.
 
At present obj_geom(ofluxtype) is zero for point charges. And we do 
no flux accumulation, which means that they are not accounted for in
the mf_obj list. One would wish to leave the point charge masked out
of the region calculation, but have it listed in the mf_obj list.
Also ns_flags needs to be set to something. 
When read back, we need to set the address so this is null:
      read(23)(ff_data(i),i=1,nf_address(1,1,nf_step+2)-1)
Now the obj_geom(ofluxtype) doubles as the number of flux quantities
mf_quant. A point charge for which we wish to calculate the force 
should have mf_quant = 0 so there is no flux accumulation, but a value
of ns_flags that tells it to get the field force and multiply by the
charge. In order to leave the point charge out of the calculation, we
retain the idea that ofluxtype=0 simply turns it off. However we implement
a test in the flux initialization that detects a point charge and if the
ofluxtype is 1 we turn on force calculation but still set mf_quant to zero.
Implemented in fluxdata.f. 

Seem to have got this going including writing and reading back. However,
although the force is in the right ballpark, it seems too low, by up
to a factor of 2 in some minimal runs on small domains with T=1.
At lower T, results look better. Probably need some big-domain runs
to check consistency more thoroughly. I find that the point field force
estimate is in fact much less susceptible to noise than the others.
That's a good sign. It isn't enough to run 100 steps with 3M particles.
This is neither steady nor sufficiently overcoming the noise. 

We need to do major tests on sceptic machine.

Here's the v=1 result for r_p=2, r_p\phi_p=1 on a 110x25 size mesh.

    Field,       part,       press,       total,   ave over steps 750 1500
========== Object 1 ->  1 radius=  2.000 ========  Charge=  -12.5664
    -0.13769     0.00000     0.00000    -0.13769
     0.00107     0.00000     0.00000     0.00107
    20.98632     0.00000     0.00000    20.98632
========== Object 2 ->  2 radius=  5.000 ========  Charge=  -10.7190
    -0.06480     1.21881     0.02813     1.18214
     0.00021    -4.02067    -0.00296    -4.02342
     4.49055    27.66972    -3.72163    28.43864
========== Object 3 ->  3 radius=  4.000 ========  Charge=  -11.0850
    -0.07555     1.25581     0.01572     1.19599
     0.00655    -4.03701    -0.00099    -4.03144
     6.79924    24.02388    -2.30635    28.51677
========== Object 4 ->  4 radius=  3.000 ========  Charge=  -11.9277
    -0.08995     1.28759     0.00703     1.20467
     0.00827    -4.05602     0.00034    -4.04740
     7.48934    20.37780    -0.95641    26.91072
========== Object 5 ->  5 radius=  2.000 ========  Charge=  -11.9285
    -0.09585     1.31110     0.00195     1.21720
    -0.01691    -4.00246     0.00022    -4.01915
    10.30984    16.43853    -0.25337    26.49500
========== Object 6 ->  6 radius=  1.000 ========  Charge=   -3.7371
    -0.03220     1.30913     0.00019     1.27712
    -0.00306    -4.01847     0.00001    -4.02153
     4.50617    11.54300    -0.02968    16.01949

This seems to show that the local field calculation is very significantly
lower than the integration over spheres. I'm not terribly happy with the
agreement across the radii: from 26.49 to 28.51 either. We know that 
r=1 is going to give a wrong answer, and maybe r=2 is suspicious, but the
others ought to agree. 

Looks like the mesh spacing outside the inner region is as great as 
1.6 or so. This might be a bit big. But in any case the inner region is
better resolved. I don't understand. There are comparable discrepancies 
at v=1.5.

Went back to laptop and using geomforce and geomforce2 which has twice
the resolution, I seem to get agreement between the point measurement
and the surface measurements at -v1. -t.1 -l10 that improves with resolution. 
At 64^3 we get (200 steps on a +-5^3 domain).
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.23548     0.00000     0.00000     0.23548
    -0.12161     0.00000     0.00000    -0.12161
    14.90358     0.00000     0.00000    14.90358
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -12.2314
     0.12728     0.33798    -0.00012     0.46514
    -0.15539    -0.46727     0.00017    -0.62249
     7.63848     7.31926    -0.01387    14.94387
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.8205
     0.01426     0.46699     0.00000     0.48125
    -0.01897    -0.54087     0.00001    -0.55983
     3.31843     4.22225    -0.00121     7.53946
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -11.9920
     0.11133     0.40623    -0.00188     0.51568
    -0.02513    -0.51600     0.00148    -0.53965
     4.09672    10.80441    -0.11957    14.78156

-l5:
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.02662     0.00000     0.00000    -0.02662
    -0.07211     0.00000     0.00000    -0.07211
    13.33594     0.00000     0.00000    13.33594
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -11.9781
     0.00245    -0.02488     0.00016    -0.02227
    -0.03551    -0.38794    -0.00005    -0.42350
     5.79478     7.75847    -0.04958    13.50367
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.7635
    -0.01770     0.04770     0.00002     0.03002
     0.01322    -0.46171    -0.00002    -0.44851
     2.68749     4.63474    -0.00459     7.31764
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -11.0783
     0.01740     0.04902    -0.00096     0.06546
    -0.02187    -0.42372     0.00293    -0.44265
     2.55158    11.36046    -0.39078    13.52126

-l2:   Not so good!
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.08077     0.00000     0.00000    -0.08077
     0.03787     0.00000     0.00000     0.03787
     8.46597     0.00000     0.00000     8.46597
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -10.6575
    -0.00736    -0.14493     0.00136    -0.15093
     0.00222    -0.00104    -0.00155    -0.00037
     1.70747     7.27180    -0.16739     8.81188
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.4301
    -0.01574    -0.09479     0.00024    -0.11029
     0.01126    -0.05549    -0.00014    -0.04437
     1.09785     4.89843    -0.02017     5.97612
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -7.3739
     0.01402    -0.06781    -0.00556    -0.05934
    -0.00764     0.00118     0.00398    -0.00247
     0.16782     9.49809    -0.81142     8.85449
-l1
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.01964     0.00000     0.00000    -0.01964
     0.04107     0.00000     0.00000     0.04107
     4.86549     0.00000     0.00000     4.86549
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -8.0949
    -0.01932    -0.01404     0.00764    -0.02572
     0.01614    -0.05005    -0.00875    -0.04266
     0.21513     5.27831    -0.26057     5.23287
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -2.5923
    -0.00825    -0.00650     0.00088    -0.01388
     0.01075    -0.06147    -0.00104    -0.05176
     0.27169     3.98918    -0.04810     4.21277
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -3.6314
     0.00393    -0.00332     0.01446     0.01507
    -0.00176    -0.07535    -0.00118    -0.07829
    -0.05562     5.75820    -0.54000     5.16258
-l.5
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.03778     0.00000     0.00000     0.03778
     0.07824     0.00000     0.00000     0.07824
     2.57617     0.00000     0.00000     2.57617
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -4.2711
    -0.00236     0.23551     0.01142     0.24457
     0.00280     0.05457     0.00158     0.05895
    -0.02972     2.93894    -0.21933     2.68990
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -0.6572
    -0.00026     0.27244     0.00152     0.27369
     0.00268     0.06801    -0.00461     0.06607
    -0.01624     2.56229    -0.09923     2.44682
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -1.5672
    -0.00067     0.28145    -0.00817     0.27261
    -0.00131     0.05740    -0.00729     0.04880
    -0.00465     2.72331    -0.04260     2.67606

Switching to uniform grid in geomforce2 we get -l1 rather worse.
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.01859     0.00000     0.00000     0.01859
     0.01352     0.00000     0.00000     0.01352
     4.62026     0.00000     0.00000     4.62026
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -8.1152
    -0.02191    -0.04323     0.01097    -0.05417
     0.01764     0.06283    -0.00849     0.07198
     0.20519     5.45752    -0.26392     5.39879
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -2.6212
    -0.01194    -0.01557     0.00079    -0.02673
     0.00766     0.06462    -0.00086     0.07143
     0.29040     4.08004    -0.04873     4.32170
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -3.7099
     0.00518    -0.03094     0.02029    -0.00547
    -0.00061     0.03528    -0.00171     0.03296
    -0.05509     5.89466    -0.52665     5.31291

Examining the raw ug, in the case of -l1 it has a negative peak of magnitude
-1, and not very smooth in the vicinity of zero. Therefore its electric field
is actually varying very quickly in the vicinity of the axis and just 
picking the r=0 value off is not going to be very accurate. It's a bit 
surprising. Even with -l10 the negative potential peak on ug is very big.
Nearly -2. Why is that? 

Run on sceptic with 33 cells across +-6 gives result that has good agreement
between all the full-sphere forces and about half the error with the 
point force (~10% versus ~20%) cf the much coarser mesh. Thus discrepancies
do arise in the force estimates when the resolution is insufficient and or
there are mesh size changes in the interesting areas. Also the point force
calculation is not reliable even with a cell size of 0.36 with charge size
2 and -l10. 

This explains some of the strange results during the runs we've done for
transverse force. It's basically a problem with mesh resolution. 

4 Apr 11

To explore this further we need a routine that reads back the field file
and then uses the force structures to calculate the field average over
a series of spheres of different size. I think it would be a mess to 
use phiexamine itself. We should create a new analysis program. This can
use some additional routines in stress that are designed for this case.

A problem is where we should get cij from, since it is not stored in the 
phi file. Actually cij is only needed near an object boundary. We ought to
be able to use a simple field evaluation getsimple3field. Yes we can
but that means we need to change the routine in stress.f not to use
fieldatpoint.

Seem to have got this going but the results are all over the place.
We get completely different answers from pha and ua files. None of this
makes much sense. I think there must be bugs. Yes wrung them out. 
Now measuring the field force on at 4pi charge by averaging the field
over a sphere of radius r gives from both ua and .pha files:
r=    0.0100 Fieldforce   -0.043290    0.041318  -14.475240
r=    0.0200 Fieldforce   -0.041516    0.039458  -14.473969
r=    0.0500 Fieldforce   -0.036141    0.033882  -14.470119
r=    0.1000 Fieldforce   -0.027171    0.025364  -14.436084
r=    0.2000 Fieldforce   -0.016220    0.019686  -13.873970
r=    0.3000 Fieldforce   -0.025746    0.032109  -12.923309
r=    0.4000 Fieldforce   -0.035169    0.044168  -11.885883
r=    0.6000 Fieldforce   -0.037832    0.068202  -10.156130
r=    0.8000 Fieldforce   -0.041531    0.080158   -8.874148
r=    1.0000 Fieldforce   -0.052499    0.093228   -7.850270
This is a case that gives an average point force of 14.6507,
and at r=1 a field force of 7.50117 on charge of 12.1702. 
The latter is quite close to the force 7.85027 when charge is 12.5664.

I conclude that there is not a big interpolation error in the fieldforce.
Instead the fieldforce is actually too low by a significant amount.
This appears to be because the asymmetric charge that gives rise to 
the drag force is actually very significant close to the point. 
It actually peaks at about z=0.3. Therefore if one has cell size of
0.15, the charge is not being well resolved there within the Poisson 
solver. The charge-to-mesh spread is smoothing it substantially. 
There is also, by the way, a structure in the charge on this small scale. 
This is quite alarming if one wanted to use the field directly to 
estimate forces. Of course I don't. I use the particle momentum flux.

Interim Summary

Direct field evaluation for the point charge does give a force that is
less susceptible to fluctuations than the particle momentum flux
included in evaluations at larger r. However, it is instead
susceptible to substantial systematic errors apparently arising from
lack of resolution of the mesh charge in the vicinity of the point.
Perhaps if one had a finer resolution in the vicinity one might be
able to escape this problem, but I have a gut feeling one might at the
same time get back the noise problem.

2 June 11

Finished first draft of RefManual.tex and cvs added it.
Moved the fieldave.f file responsible for the above data to /testing
and cvs added it. 

Added dealing with a perpendicular velocity to the Bfield section
by subtracting and adding the perpendicular drift around the cyclotron
move.

Also discovered a bug in the cyclotronic mover. Sign error fixed.
Implemented a fall-back to summed acceleration if the Bfield is very weak
otherwise rounding in the subtraction of gyro radius becomes too large.

Profiled the costs. Adjusted placement of sin and cosine calls to make
sure they aren't done unnecessarily, since they otherwise would tend to
become a dominant factor in the costs. As it is, they are now much smaller
than the field interpolation costs.

Seems as if we have working cross-field (FxB) drifts.

Moving back to wakesmalllambda on sceptic. The initial runs were done
with -l.2 which is resolved by the mesh. We might be able to go another
factor of 2 to -l.1, but then we are going to have to go to quasineutral.
So tried quasineutral.

There's a display problem in that the potential inside the object is set
to 0, even when the object's boundary potential is non-zero. 

Actually it's not clear that there's much advantage in going all the way 
to quasineutral. Once -l is smaller than the mesh size, we are close to
getting the same result anyway. The number of iterations is small, so 
there's no big advantage in going to QN. 

The boundary condition at the probe is something of an issue. Provided the
probe is attracting, relative to the local potential, then probably there
is some place between the boundary and the first set of nodes where the
effective boundary position is located. Thus one can think of the BC
position as being the uncertain point. It seems unlikely that one can do
very much better than this by some sort of square-root shape assumption. 

26 Jun 11

Improved the testing built into make to compare prior potential file
if one exists. 

11 Aug 11

Fix reinjection outside particle region simply to retry.

Implement "infinite" magnetic field case (>1.e5) which amounts to
one-dimensional dynamics along B plus a fixed drift velocity perpendicular
to B.

There's a problem concerning reinjection with this implementation. It
is that no particle ever moves perpendicularly out of the region, but
there are particles that are reinjected perpendicularly. That
perpendicular reinjection must be stopped. I think. When B is finite,
we follow the Larmor orbit and constant escape and reinjection happens
across the field. Therefore the old cartreinject is ok (except maybe
for the drift velocity, must check, it seems to be ok).

As far as reinjection is concerned, in the infinite-B case, the drift
is purely along B, but so also is the random motion. Therefore the 
statistics of reinjection do seem to be rather different. 1-Dimensional.
It looks as if this needs changes only to cijinit which sets up the
statistical arrays. These are based upon (possibly) shifted maxwellian
flux and distributions from the functions ffcrein and fvcrein, which
assume the drift to be in the z-direction and have equal temperatures
in all dimensions.

Instead we want functions where the drift consists of two parts:
parallel and perpendicular to B projected in the coordinate directions:
vpar*Bfield(j)+vperp(j).
Also the thermal velocity (spread) must be the parallel velocity
projected into the coordinate direction. I.e. it is multiplied by the
direction cosine Bfield(j). 

Implemented new functions ff1crein and fv1crein and put in calls to them
if Bt is "infinity" meaning >1.e5


Plotting orbits we seem to get sensible results both with high B (<1.e5)
and with infinite B (>1.e5). Perhaps the issue with reinjection was not
so bad, since the orbits become nearly straight with high B well before
the new code is triggered. Of course I am not actually looking at reinjected
particles for the most part. 

I think the problem is that if the timestep becomes longer than the gyro
period, then during each step the particle actually makes (multiple) cycles
that when it is near the edge are in and out of the region. These exits
are not captured. Each of them would give rise to a reinjection of the 
particles that are close enough to the edge that their Larmor orbit 
intersects it. There are then multiple issues:

(1) Because the timestep is too large, even a particle whose Larmor orbit
intersects the boundary might statistically be able to move into the region
because it might take several steps with gyrophase such as to be inside
the region. In other words, just looking at the initial and final positions 
of a timestep is badly mistaken about whether or not one leaves the 
region during the timestep. 

(2) If particles are placed "just" inside the region, where the
distance inside is bigger than the Larmor radius, then they are artificially
prevented from making the multiple entrances and exits. 

If all this is right, then it appears that the reinjection scheme is 
already erroneous when B>~1/dt. I wonder if there's a way to correct it
in intermediate cases, rather than immediately going to B=infinity.
If not then evidently infinity ought to be considered to be a lot smaller
than 1.e5. More like 1/dt.

15 Aug 2011

One way to think about this for the edge of the computational boundary
is to do a calculation of whether the Larmor orbit intersects the
edge.  If it does, then during a time step of dt, a total of Nc=\Omega
dt/2\pi transits in and out of the region occur. There's no point in 
_actually_ reinjecting the particle Nc times, because we are trying to
model a region that is part of a bigger plasma. However, one might 
count the times that the particle would have transitted, and consider
that many reinjections to have occurred. 

The problem is that some surfaces have many of these type of
injections, and some don't. Consequently, the probabilities for edge
face at which to reinject need to be adjusted in the light of the 
reinjections already taken into account by the cyclotron process. 

One ought also to distinguish between solid surfaces, which would remove
the particle immediately in the first cycle, and computational boundaries
which would not. 

Clearly the effective statistics depend upon the timestep length dt. 
Perhaps one could set up the probability distributions to account for
this in the first place, but they would then depend upon dt, so 
acceleration would not be appropriate. 

18 Aug 11

Worked more on the infinite-Bt case, because the analytic efforts on the 
intermediate cases yield only rather uncertain interpolations.
Had to implement also the rhoinfcalc and ninjcalc appropriate for the
projected parallel distributions. 

Tests with -gi flag show plausible results. Particles are injected with
a spread of parallel velocities and with perpendicular velocity essentially
equal to the drift. The spatial distribution of injection appears to be
inconsistent with expectations. Zero on faces with no v- or B- component, 
but insufficient on the z-face into which the flow is directed when
B is in an orthogonal direction.

That's because I omitted to scale the distribution when projecting 
it so that the total area is constant. Did that. Then get sensible
result. Tried some vt2min adjustments. Smaller is better, but if you
make it too small then it can lose the integration entirely. 1e-5 is
safe and gives only about .002 flux through the side walls. That's
unlikely to be significant. The minimum v-difference with 2000 points
in the integration (starting at -2,2) is <1e-3. When vt2min=1e-6,
then the maximum value of arg is <1 so certainly there will be a nonzero
position. I think 1e-6 is safe. It gives flux .0007. That's pretty much
negligible.

NOW: we seem to have infinite-B reinjection working.

Start working on copticthin which is going to be a thin domain to
do effectively 2-D calculations with infinite Bt. We can't currently
make the z-direction thin because the drift velocity is purely in that
direction. So make the x-direction thin. 

Even with a mesh of four nodes in the thin direction there are big
problems with No Good Vertices if the object is as big as the domain.
And there are boxinterp zero weight everwhere errors. That's with a
sphere. Change to a cylinder, as is my plan. Still get those boxinterp
errors. It looks as if I am not rejecting ions initialized inside the 
cylinder, since I am using only 20650 tries for 20000. This is a 
radius 1 cylinder in a box 10x10. So that fraction is pi/100, which
is about right. So no. The fraction of rejections is reasonable.

Found the problem to be the special particle-1. It was being set outside
the mesh. Moved it in pinit so as to catch this sort of error and give
a warning.

Now we can use down to 4 cells in the thin direction. 3 gives a
sorelax error. But we ought not to need to use sorelax for
quasineutral, so setting -l0 this runs too. 2 won't initialize.
Recall that the first and last nodes in each dimension are ON the boundary. 
Their volumes are set to zero because the presumption is that they 
are going to be set by boundary conditions, not by solving Poisson's 
equation. psumtoq works only on non-edge nodes.

Seems to be working with quasineutral 3 nodes in x-direction up to 
velocity of 5. Trying 10 gives Getlocalregion excessive error and
field corruption. Arising because a potential is minus infinity.
I suspect that must be because the density happens to be zero. 
Probably we need some sort of truncation in the quasineutral 
calculation. Yes, added a floor to the density before taking its log
and the error goes away.

Tests with 400k particles on a 3x32x64 mesh give fluctuations of density
that are roughly 0.07. Particles per cell are something like 400k/32x64
=200 which seems to be statistically correct. 

With mesh allocation 4x100x100,   time is 43s. For 100 steps of 0.4M
With mesh allocation 100x100x100, time is 46s. Not that much
different!
4x200x200 allocation 44s.
Hardly changes when the used mesh sizes are doubled.
4x200x200 used: 1m6s. Slower.

Clearly it is possible to run with large grids without much explicit
penalty other than the need for more particles. 


21 Sep 2011

Took a long detour based on the realization that the stability calculation
is in many ways better done by analytics, than relying on PIC. Did not
come to a definitive answer but 

Trying to get back to this thin case for cylindrical operation in
general.  For the infinite-B situation there was no need to solve the
problem of ions leaving along an axis of translational symmetry,
because the drift velocity would normally be set to zero in that
direction. However, for finite-B if one wants to do a cylindrical
calculation, then periodic reinjection ought to be used for exits
through the symmetry faces. Otherwise one loses lots of orbits in a
perpendicular direction if the domain size is literally thin. An
alternative is to regard the domain as extremely large in the symmetry
direction (even though having only few cells). If large enough then
very few particles leave across it. This is almost equivalent to
reducing the symmetry-direction velocity of the particles. This sounds
like an easier way to solve the problem. 

Change the nameconstruct to use the y-length as the second parameter
that way the long x-domain does not screw name up. Seems to sort of work
with a long (100) x-domain and shorter in other directions. The inherent
inefficiency of using 4 x-nodes is that the ends take up 1/3 of the charge
which are then not used because the potential there is set from the 
boundary condition. So even if we average the potential in the x-direction
thereafter, we will have lost some efficiency. However, this does not 
seem a very serious problem. I therefore conclude that the thin version
is actually able to do cylindrical problems reasonably well. The bigger
inefficiency is probably in the particle moving where three rather than 
two field components are used and the interpolation over three dimensions
is probably a non-trivial cost. 

Did a commit. Big mistake. This version was not correct with a standard
copticgeom.dat. Mditerate error. Mditermults error. Ok this seems to be
caused by using ifull too small. Yes. Fixed by increasing to a decent
number. Ought to have a more sensible check. Implemented in meshconstruct
and in readgeom.

4 Jan 2012
Fixed some bugs in pex file writing where arrays were not being 
properly reinitialized for subsequent writes after the first.

6 Jan 2012

Rationalize the boundary setting code in bdyroutine.f. Add slpcom.f
and the ability to set BC type from command line.
Encapsulate the command-line argument handling code and remove it from
the main program to cmdline.f.

CVS commit.

14 Jan 2012
Trying out second derivative style boundary condition at trailing edge.
Doesn't seem to converge during SOR, although does not blow up, when
lambda=1, vd=.2. but dxk2 is too big: 2.6

16 Jan 2012

Struggling with segfaults with the pathscale compiler on loki.
Using no optimization -ffortran-bounds-check -DEBUG

Found some bugs in getfield that cause array bounds overrun. 
Circumvented.

Found an error in obj_geom zeroing. 
However, there's a big problem with pathscale bounds checking. 
It gives false positives for implied array subscripts during read
or write. Without the -DEBUG switch those mostly don't happen. 

Now we have an apparent error in the unit number for writing. 
That implies either a compiler bug or some serious overwriting. 

Actually I have a prior comment that pathscale segfaults from 
flush. Commenting that out stops it. Should have noticed.
Oh well, found some bugs as a result.

20 Jan 2012

Fixed bug in collisional code arising when a particle slot is set to 
empty. I had never used the collisional operation.

Realize that I have not got compensating Eneutral in code at present.
So program to put vneutral equal to vd by default. Can override with 
the -vn switch.

23 Jan 2012

For collisional cases I need another force variable colnforce(i,j,k).
This would be the rate of change of momentum in each dimension (i) for
each object (j), and step (k). This is equal to \nu.Sum_iparticles
(vd-v(i_p)). This is a bit tricky, since it is not evaluated by object
intersections or surface integrals like the other forces. It has to 
be evaluated by examining each particle and finding if it is in the 
object under consideration. It could perhaps be incorporated into the 
particle advance, to minimize the computational burden. The place to 
put it is right at the end of each particle advance, using iregion.

Updated the fluxdata to read and write the extra colnforce, and account
for the version number to accomodate old versions.

Implemented in the relevant places.

I realize now that there's a booby trap for these diagnostics. It is
that if I have millions of particles, and I add each one's
contribution separately, then I end up adding roughly speaking 1. to a
total of over a million. Therefore there's potentially lost accuracy,
using reals. Eventually the increment will be lost. Do I have to think
about using doubles for these? Wow, that's an issue for some other
things too! But all the other diagnostics apply only to particles
crossing boundaries, so they are presumably far fewer than the total
particles. And the bulk force will also apply to far fewer particles
provided the region of measurement is much smaller than the whole.
But it might not be (especially in sceptic). This problem is partly
alleviated by keeping the number of particles per node smaller than
about a million. Have to think about this in the sceptic context as
well. Sceptic seems to bundle up the particles into the cells first, I
think. If so, that's a big help, provided the number of cells to be
added together is not too large. OR provided one does preliminary sums
in (e.g.) dimensional directions, followed by summing the sub-sums.
The problem can in general be alleviated by making sub-sums. 

Working on makefile I seem to blow away coptic.f. I've lost the additions 
to it. I think that was mostly the addition of call to bulknorm. 
Put it back in. I think that's all needed.

How to fix the rounding problem. Probably implement an intermediate 
summation stage around 3000 long.

24 Jan 2012
The sceptic code uses partsum and vzsum diagnostics to calculate the 
collisional force. Therefore the most particles they sum is one cell's 
worth. Then cells are added. Hence there's no problem.

8 Feb 2012
Working on fluxexamine and related plotting and printing of fluxes and
forces.

Changed object plotting routines to have a leading argument iq which
refers to the quantity to be plotted. By default it has been taken 
to be one (flux) before now, but perhaps one might want to surface-plot
the particle momentum or energy.

11 Feb 2012
Change nameconstruct to use 100*vd unless vd is very large. 
Do the same for P the probe potential.
Fix makefile to refer to the new code.

22 Feb 2012
Various upgrades to philineread to improve plotting and analysis capabilities. 

Checking on the functioning of the collisional bulk force with 
./coptic -v1 -ct1. -ofgeomforce.dat -s200 -ri400
gives
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  1.000 zcenter=  0.000 Charge=   -8.5655 =====
     0.01101    -0.01232    -0.00235    -0.09590    -0.09957
     0.00600     0.32662    -0.00069     0.00334     0.33526
     0.22893     1.79832    -0.15775     1.22717     3.09667
===== Object 2 ->  3 radius=  0.500 zcenter=  0.000 Charge=   -2.7139 =====
    -0.00124     0.19713    -0.00001    -0.03246     0.16343
    -0.00070     0.39909     0.00046    -0.00257     0.39628
     0.20808     1.76245    -0.02563     0.57719     2.52208
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.0945 =====
     0.00624    -0.07546    -0.01251    -0.13619    -0.21792
     0.00681     0.23792    -0.00955     0.02475     0.25994
     0.05635     1.62745    -0.33953     1.73184     3.07611
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -4.0442 =====
     0.00080     0.05888    -0.02615    -0.24646    -0.21293
     0.00534     0.42357    -0.03260    -0.00527     0.39104
     0.01015     1.56578    -0.47526     1.94861     3.04928

Showing good z-force agreement for radii bigger than 1, the point charge
radius. That seems to indicate that we can in fact measure the force
for collisional cases.

27 Feb 2012
If colntime .ne. 0 then append an extra section to nameconstruct to tell
the colntime. 

7 Mar 2012

In force calculations, especially with collisions we seem to be
getting single steps with enormously large force contribution. So
large that it biasses the average over 100s of steps. How can this be?

My idea is that it arises from the point charge treatment. If a step
takes a particle to a position extremely close to a point charge, then
it will experience a massive acceleration equivalent to a hit (in a
practically random direction) out of the park. It will leave the
entire region (and domain) in the next step with a monstrous
contribution to momentum in some quasi random direction.

If this is in fact the cause, then it arises because the leap-frog
finite steps are too big near the point charge to correctly conserve
momentum and energy. The point charge really ought not to be allowed
to do these big hits.  Close collisions, well inside the shielding
radius and b_{90}, ought basically to be 180 degree scatterings.
Those give rise to the maximum possible momentum transfer. Twice the 
prior momentum. 

These problems were observed with time steps -dt.025. and point charges
-1. at .2 in a domain -4to8x-4to4, quite severe at low drift velocities
0.1, 0.2. At velocity of 1, a step of .025 is a length of .025, which is
1/10th of the point charge extent. Therefore, if out of the park hits is
the problem, they are occuring deep inside the point charge extent. Thus,
the ion will have stepped inside the extent, and acquired a velocity 
mostly from the potential drop: 

1/2 v^2 = r_p\phi_p/r

at that point it steps to the charge:  dr = dt v = r so that

1/2 r^2/dt^2 = r_p\phi_p/r

r^3 =2 dt^2 r_p\phi_p = 2 dt^2 P   where P is the particle charge strength.

This defines the volume for which hitting occurs. Solution is

r = (2 0.025^2 .2)^{1/3} = 0.06.

rhoinfty is 128k, so the number of particles in this region is. 
4\pi r^3  /3 rhoinfty= 134 There are about 100 particles in the hitting 
region (of a total of about 100M) one particle in a million. 
Their velocity is approximately r/dt = 2.5.

An integral of the momentum discrepancy will be of the form

\int dv n 4\pi r^2 dr = \int  dt (P/r^2) 4\pi r^2 dr

which converges fine at the origin. So it's not obvious that there's 
anything other than a noise problem associated with hitting. 

b_{90} = r_p\phi_p/(v^2/2) is kind of self-defining when the major
part of the energy comes from potential. 

What will constitute a hit out of the potential well? A step where the 
impulse is such as to produce escape velocity in one step. I.e.

	dv = dt P/r^2;   (1/2)dv^2 = P/r. 
So
 dt^2 P^2/r^4 = 2 P/r   =>   r^3 = dt^2 P/2.

This is essentially the same criterion as above for stepping to the charge
in one step, except with a different (smaller) coefficient.

Solution of this is r = (.025^2 x .2 /2)^{1/3} = 0.040
There are about 25 particles in this region.

What constitutes a ridiculous velocity? Perhaps one that leaves the
region in one step. I.e. roughly v = 1/dt = 40. To produce that velocity
requires impulse dt P/r^2 = 1/dt. I.e.

	 r^2 = P dt^2    =>    r= P^{1/2} dt  = 0.01

We certainly ought not to use accelerations as great as arise from this 
distance. 

Alternatively, one might limit the impulse to of order unity: r^2 = Pdt.
For which r = 0.07

Options to prevent. 
1. Limit the maximum acceleration somehow.
2. Subcycle if we are close to a point charge.
3. Remove a particle if it comes too close to point. 

Actually I have some code for 2 in padvnc but commented out at
present.  Activate this code to subcycle for particles with very large
accelerations.  Subcycle is the maximum permitted ion impulse per
step. If it is exceeded, then the subcycle's time step is set short
enough to limit the impulse accordingly. A reasonable value for
subcycle is 1, which would force subcycling within about r=0.07. One
probably should not go less than 1.  Bigger might be sensible. One
needs to worry about the possibility of self-force arising.

The runs don't look like completing on loki. I suspect there needs to be
a limit to the degree of subcycling that is done. Did some trials on
tp400 with subcycling and printing the numbers nsubc. Limiting to 10
the degree of subcycling reduces the nsubc. 

The loki runs give zillions of ptch field overflow corrected errors when
subcycling was turned on. This indicates that an ion is within about
1.e-12 of the point charge. Therefore it looks as if the subcycling is
causing some particle(s) to spiral in to the point charge. No other runs
show these errors. This would presumably be less likely if the degree
of subcycling were limited.

The one run that got far enough to print it says Subcycled: 16721.
So there are obviously a lot of subcycling steps. 

In any case, if advancing steps are taken that are too large for
energy conservation or momentum conservation, then an error will occur
in the force calculation if that ion subsequently crosses the
momentum-measuring surface.

Probably I need at least to remove ions that come too close to the
point particle. This could be done in response to a return from 
getadfield, and bypass the flux collection stage. That would be arguably
better than an arbitrary ptch overflow correction, and might be worth
doing even at smaller accelerations. 

9 Mar 2012

Implemented particle dropping if impulse exceeds 5. [Probably ought to make
that parameter adjustable, but not yet.] Also report out the number of 
drops by master node periodically. 

11 Mar 2012

Modify fluxexamine to allow printing out of vd and a component of the force 
from a masked set of objects. This is to ease developing plots versus vd.
Committed.

5 Apr 2012

Modify diagexamine to do an overplot of velocity arrows on a final plot
of the density. Seems to work. Discovered one needs the right number of
arguments if there's a character array to be passed, else the character
array is not handled correctly, and the called routine looks for its
length in an incorrect storage location.

13 Apr 2012

Revisiting subcycling because there seems evidence that with collisions
the effects of proximity to point particles are in fact impacting the
results with slow flow. It might be possible to alleviate these by getting
subcycling to work better. 

The basic problem with current subcycling is that it does not
substantially reduce the acceleration duration at the first reduction
of movement duration.  That's because the kick has duration
(dtprec+dtc)/2. And if we've been moving fast so that dtprec is big,
then reducing dtc doesn't reduce the acceleration duration (kick) by
more than a factor of 2. What that says is that if we obtain an
acceleration that is too large, it is already too late to reduce much
the kick-duration because the that duration has been set by the prior
dtprec. We should not have allowed the ion to step into so high an
acceleration region with the previous timestep. 
However, we have the information to back-up the move. It is in the form
of dtprec and the current position and velocity. 

What we are really saying is that for subcycling we want to interrupt 
and reduce the standard KICK-timestep. This is different from collisions
where we want to interrupt the MOVE-timestep. So simply adopting the same
approach for subcycle and collisions is insufficient.

Now when backing-up, the prior kick has already assumed a step dtprec
and made its kick-duration equal to half of it plus half of the
previous step.  We need to change dtprec. But we can (I think) adopt
an alternate view of where the step starts and stops. Instead of
defining that through the move-step, we can define it through the
kick-step. So if we back up by half of dtprec, we are at the end of
the prior kick-step, and we can then change the current kick-step to
something new and smaller. That means the prior move step was half of the 
dtprec plus half of the new kick-step. This is illustrated by the following
diagram for a step-size-reduction occurring at *.

Kick Steps ----|-------|-------|---|---|--|
Move Steps |-------|-------|---*-|---|---|-|
                               a b c d e f g
Unchanged Moves ---|-------|-------|-------| 
                            dtprec

That can be contrasted with the step reduction occuring at a move-step
end (which is what is used for collisions) illustrated below:

Kick Steps ----|-------|-------|---*-|---|
Move Steps |-------|-------|-------|---|---|
                            dtprec c

Either one of these approaches has some error associated with the
transition in step-size. I don't think there's reason to suppose that
one error is worse than the other. We represent the values of position
at move-step ends (|). The velocities that are stored are those
corresponding to the middle of the prior move-step. I.e. at the prior
kick-step end. 

Backing up (the first of the above diagrams) corresponds to returning
to that prior kick-step end. The position is then half the dtprec
times the velocity back (*, a). We then want to take a smaller
half-move step forward to the adjusted move-step end (b), and make the
kick step based upon the acceleration at that position (b) and the new
duration.  We want to store as dtprec, the new kick-step duration,
half of which is the half-kick-step to the current position (b). Then
when we attempt to take the new step, the kick will be correctly
calculated using (dtprec+dtc)/2. 

If we then find the new kick is still too large and we want to reduce
the step-duration further, we can simply repeat the backing-up process.
We need to keep track of where we eventually have to get to,
using dtremain.

In neither case do we adjust the potential. So the field calculation
is done for a charge density that corresponds to the end of the prior
move step. In the prior approach to subcycling, that meant we use
advanced particle positions to obtain the acceleration, leading to
accelerating self-force. In the case of backing up, the field is
instead evaluated at a position (b) behind that (c) at which potential was
solved.  It should then give decelerating self-force at kick b, but
accelerating at d and f. The time g is where the subcycling catches up
with the rest of the simulation, and we move to the next time-step.

To implement backing up we need to package up particle moving into
a subroutine and then just do it backwards for specified negative dt.
Did the encapsulation.

14 Apr 2012

Seem to have subcycling working as planned. When a sub-step has been taken
the length tries to double or go the full length. That way if we substep
out of a high acceleration region, then we'll lengthen the step again.

22 May 2012

Trying to run some moon-relevant cases. Can do 3-d but then one gets a
(physical) peak on axis with no B-field.

2-d equivalent case is the "thin" 4-mesh in the x-direction. But this is
found to have boundary points that have stupid potential values. This is 
probably not affecting the result much, since we have taken the length
to be very large in the x-direction. But it reminds me that we really need
to develop better boundary condition control. In fact the old stuff is 
so ad-hoc that it probably can't be properly cleaned up. May be better to
develop a whole new system in parallel. 

There are 3 main categories: Spherical symmetry, Cylindrical Symmetry,
and Rectangular symmetry. The ad-hocs are mostly applying spherical and
cylindrical BCs to the rectangle. What we now need is rectangular control.
This means we can apply a different BC to each of the 6 box faces.

Proposed 1st-order scheme is that we allow ourselves to apply a Robin type
condition in the form:
	  A phi + B dphi/dn + C = 0
at each face: A(6), B(6). Probably the normal derivative ought to be 
OUTWARD, so dn is opposite in sign for opposite faces.

But we would also like to enable C to vary with space. In first order it 
might be 
      c = c_0 + c_x x + c_y y + c_z z.
However, we wish to be able to bypass expensive x,y,z calculations 
if their respective coefficients are zero. That might be done by making
c a function, but in any case, we need to save the c_coefficients, and
possibly a few other logicals for bypassing. If the boundary potential
is going to be continuous, then we require the c's to match along edges.
One way to ensure this is for all 6 faces to have the same c's. But that's
not the only way.

The degree of evaluation of the Robin condition would call for a third
phi value if we made phi and dphi/dn centered on the boundary, so one
might make it centered on the half-lattice position: 

phi = (phi_i+phi_b)/2              dphi/dn = (phi_b-phi_i)/|dn| 

But if B=0, it makes more sense to simply set phi_b. 

Then c ought perhaps to be evaluated at the same place (if it varies).
It's better to do the adjustment of c externally for spatial position
internally. Not in routine.

So:
	 if B=0 phi_b=-C(b)[/A]
	 else phi_b = - [C+(A/2-B/|dn|)phi_i]/[A/2+B/|dn|]

If A/2+B/dn=0 there's a singularity. One could verify initially that 
that's not the case. One ought also for speed to save the values of
A/2+B/dn A/2-B/dn for each face. 

	 logical LF,LCF(6)
	 real AF(6),BF(6),C0F(6),CxF(6),CyF(6),CzF(6)
	 real ApBF(6),AmBF(6)
	 common /FaceBC/LF,AF,BF,LCF,C0F,CxF,CyF,CzF,ApBF,AmBF

Programmed the mditerate routine. bdyface
Programmed the initialize routine. bdyfaceinit.
Added CFpack to the cmdline argument list. 
But in all honesty its going to be verbose to do this through
cmdline flags like this 
-BFidn,Ain,Bin,C0in,Cx,Cy,Cz

Implemented anyway. One can omit the Cx... if unused. Seems to be setting
the arrays correctly, but the BCs are not tested under solution conditions.
The way to do that is with -gs -gd -l1000 and the small cube.

As soon as one of these switches are set, we are using the new BC 
setup. But not yet implemented as actually calling the bdy routine.

23 May 2012

Now got a version of the bdyset routine working that uses the new 
conditions. After debugging a bit they seem to work.

Implemented setting in geometry file. Currently lines in copticgeom.dat
will override cmdline switches. It's a judgement call which prioity is
the most useful. This is the easiest to program. 

Periodic Potential Boundary conditions would be interesting. 

Within the present scheme, they are equivalent to setting the boundary
value equal to the first non-boundary value at the opposite face. 
I.e. phi(n) = phi(2), and phi(1) = phi(n-1). 

However, it would be much more elegant, and possibly faster, to implement
a truly periodic difference scheme in mpibbdy. At present the face
information is passed from adjacent mpi blocks. If the end block information
was passed in a similar way, one then would not have to call a separate
bdyset routine. This actually looks pretty easy, since all the infrastructure
for this is built into the mpibbdy. All one needs is to specify the 
cartesian comm world as having periodicity in the interesting direction(s)

As far as I can see, all we need to do is change the value(s) of lperiod(nd)
(that are set in sormpi) to true rather than false. I could pass information
in ictl to control that. Tried to get sortest going. It gives nans in u.
Drat! However, coptic appears to be working with the geomsphere and the
test routines. So ignore sortest which has not been used for at least a year 
and just work with coptic.

Implemented ictl switches to make periodic. Does not work. Just seems to 
hang. Seems to be a problem with the separate mpi_send and mpi_recv. 
How can one send to oneself like that? But actually, I'm running a 
serial process here. So maybe that's the problem. Under mpiexec it
still hangs. So that's not really it.

Replacing the separate sends and receives gets one through to the second
step. And replacing the second with a sendrecv fixes the hang.
Result looks plausible, but I've still go the separate bbdy setting the 
final result to something different, which is not what I want, of course.
Still I now have a working periodic BC version of coptic working with
only the need to turn off the separate bdyset. Done that. 

Now we can use periodic BCs on potential. (Don't yet have particle
reinjection periodicity). There seems to be a problem with multiple
processes and periodic BCs. They don't seem to be working correctly,
at least on the thin domain. I don't see the problem with the test
cases. 

24 May 2012

The symptom is that in the x-y plane, with z in the lower half, only
the bottommost y-block has anything other than zero on the boundary.
While with z in the upper half, none do anything except zeros.
This can be seen in dimension 1 plane 1 very clearly. Actually its the
lowest block of y and z that has non-zero. 

Changing the default face bc to a different value, one finds that the 
erroneous faces stay equal to zero. So presumably they are never being
set. Perhaps this makes sense. I only pass the values from the other
blocks that correspond to non-ghost cells. Therefore the ghost-cells 
that belong to the other blocks, around the edge, never get set in the 
master node's version of the potential. That's the problem. 

Each process in sormpi is looping on [bdyset, bbdy, sorrelaxgen.
Each has an array for the whole potential, but updates only part of it.
The information for the boundaries of each block is passed, so the 
inner solution is correct. The only problem is that the boundaries of
the master node's arrays belonging to other blocks never get set. 

For some reason, bdyset is called twice at the end of the iteration.
Why did I do that? But anyway that is when the boundary is supposed
to be set to be correct for the final result. With periodicity that
does not work for blocks not already set. Probably the correct way to
do this is to do reverse communications after the mpialltoall, to 
send the information back to the ghost (edge) cells. But this needs
some careful thought. I don't see how to do it.

Instead I just fixed bdyset to explicitly set the boundary values in
accordance with periodicity with a periodic direction. This is
unnecessary for iterations, and so is a possibly important
inefficiency.  Profiled to check. bdyface is using about 12% of time
in a solution-dominated case with a thin 4-element
x-direction. Compared with sorrelaxgen 35%.  So it is an inefficiency
but only roughly a 20% maximum hit, in a bad situation like the thin
geometry. However, this inefficiency will become very bad once we
have lots of processors and the sorrelaxgen costs are shared. It 
does not scale at all. In the end, for a periodic system the boundary
points are irrelevant: they correspond to the adjacent point of the
opposite face. So it isn't really necessary to set them for anything
other than final aesthetics.

This is pretty much a universal problem with the way that sormpi is 
written. The bdysetting costs don't scale because every processor sets
all the bdy points, regardless of their relevance. If all boundaries
were periodic, though, it would be unnecessary to set any of them. 
One approach to solving this issue (which would not solve the prior 
problem of sharing the boundary with the master node) would be for 
each process to detect when it is not required to receive information
about one of its faces. When this happens, it is because the face 
has been set by the bdyset routine. So the _process_ ought to set that
face according to the boundary condition, and do so for any face that
it does not receive from neighbour. In that case the bdset routine becomes
redundant. 

The challenge for this is that the bdysetting routine is currently global
and defined in terms of a mditerate process. The bbdy routine setting 
would be different and more local. It would be a subface-filling
procedure. But it would be local to the process's block. The hard part
is probably how one tells how to do the setting without entanglement
into the sormpi mpibbdy code. The information would have to be gained
other than through sormpi. We'd need an api.

It looks pretty tricky given the density of the mpibbdy code to figure
out how to do this. I think it's time to commit what we have. 

I think that in sormpi the information returned from bbdy is sufficient
to figure out the location of the block and which boundary points to
be responsible for. So it would be possible to parallelize the boundary
setting. That would overcome the non-scalable bdy effects.

25 May 2012

Trying to parallelize the bc setting. 
First try to get sortest working.
Actually, I find that sorserial works, while sortest doesn't. 
Update sortest.f to use the new cmdline setting code.
Found that the sortest error arises because plascom is not in 
sortest. Put a trap in bdyset to prevent divide by zero debyelen.
Why do I not put plascom.f into sortest? Just for independence, I suppose.
If it is included then sortest works like coptic. If not then 
there's more independence, I suppose.

sorserial replaces the bdyroutine with a null so it gives a different
solution.

OK sorserial and sortest are working.

Implemented an mditerate over the boundary of a block of thickness 1. 
bdyshare.

Got it to work with explicit setting of u to a hard coded value. 
With mpi processes, the boundary value is not actually set in the
master frame, as expected. If needed, this can probably be fixed 
afterwards by one call to the bdyshare routine which tells it that
there's only one process. [No that does not work because bdyshare 
never actually sets boundary values for periodic dimensions, it just
relies on the mpi communication. That means that sorserial won't work
properly with periodic BCs. 

Now working with -bf face boundary condition setting and -bp periodic.
For the old -bc settings the global code is still used. This is also 
called last thing in sormpi so that the boundary is set for the whole
of the master node's mesh. This is purely for aesthetics with periodicity.
It might be worth considering implementing explicit boundary setting
in periodic directions so that it would work with sorserial. Then the 
final call could be to bdyshare routine telling it one block. 

26 May 12

Fiddled with partaccum and related codes to allow one to choose a 
block configuration in the postprocessing of particle data. 
That way we can specify e.g. 1 in the x-direction and more in the 
other directions.

Found a bug in the binfinity ninjcalc and fixed. 
But there are other bugs. One seems to return particles which have
purely z-velocity at least when reading back from the partfile.

B-field is not working properly yet.

28 May 12

Fixed partaccum to do sensible things if all the particles have the same
velocity. They do in infinite-B cases in a coordinate direction perp to
the field.

Fixed the particle moving to make the perp velocity just the drift. 
(For some reason I'd had it set to zero.) Now infinite B seems to be
working.

Succeeded in getting a magnetized moon case to run. Looks sensible.

1 June 12

Cleaned up the makefile and that of accis so that it is substantially 
more logical and robust in making decisions about drivers and compilers.

2 June 12

Discovered that the serial version of coptic does not communicate the
periodic boundary conditions properly. I had thought about this but
came to the conclusion that it did. That was wrong. But I don't know why.
What bdyshare currently is supposed to do is to set the guard boundary
values when the block boundary is actually a domain boundary.
However, it only does that when the boundary is NOT periodic. 
When it is periodic the bdyshareroutine does nothing, because it is
assuming that the boundary has been set by the bbdy communication. 
Thus, when there is no bbdy communication, the boundary is not set 
properly. That explains my observations. 

Ideally, we would like a call to bdyshare that changes its behavior and
instead does periodic setting rather than nothing. 
Currently we pass offset into the bdyshareroutine and this points to the
adjacent position. For periodic setting, we could perhaps pass a different
offset that pointed to the corresponding point at the opposite face, and
have A=0,B=1,C=0 (zero slope) which sets the boundary point equal to the
adjacent point, and temporarily set LPF off for this id.

Instead simply increased the information passing capability by using 
more of idone. If idone(2)=1 then the periodic conditions are set 
explicitly, using the ABC coefficients.

Also fixed the object file setting of periodicity so it works.
It's possible to get an inconsistent result by setting periodicity 
then setting _just one_ of those faces back to face conditions. The
other face then uses zero derivative condition left over from the
periodicity settings. But it then seems to get the boundary set wrong
at the end as if it were periodic. Hmm. There's an error in logic.
Fixed that.

Implemented an option in partaccum. If ivproj=1 then we project the
accumulated velocities: id=1 parallel, id=2 perp, id=3 z-component.
Added -p switch to activate in partexamine. 
Added default Bfield setting in partexamine if not already set. 
But that needs some better switch to prescribe override.
Implemented ability to prescribe Bdirs after -p.
It overrides if it was set with the full three components. Otherwise
it is used only if Bfield was not read from the partfile.
Seems as if this option is working: finding projected particle distributions.

Needs some work on the annotation of the plots.

Thinking about rationalization of the ndims argument and parameter mess.
What are the causes of problems.
1. Local Scratch arrays can't be created legally unless ndims is a parameter.
2. Consequently, if ndims is _passed_ which means we are prevented from
   using it as a parameter, then we do duplication with modified name. 

How do we avoid the crazy multiplication of ndims parameters?
Perhaps have an include file that does nothing except define the
ndimsparam which we use for all such dimension definitions.
Unfortunately this mess is very bad. Plus, in quite a number of places
the coding is implicitly assuming 3d. For example in some of the surface
and flux tracking code. There are literally hundreds of places that ndims
appears in the code (with various prefixes and postfixes.)

Converted all the objcom.f _sor labels to _cij. 
Found that there's a call in sorrelaxgen that references ddn_cij.
So that there's some cross-linking even here. That's a bit of a problem.
Actually it is unavoidable. It is the place where the adjustment of the
cij stencil takes place. It uses extra information stored elsewhere.
Only the cij code knows the structure of that extra information.

But anyway. I seem now to have rationalized one naming convention so
that _sor really does refer to the sor code. While cij refers to the
objcom object boundary code.

Made bdyshare an argument passed to sormpi.

7 June 2012

Constructed github repository COPTIC and successfully exported from cvs
and imported to github. Clone it via

git clone https://github.com/ihutch/COPTIC

Seems to be ok.

14 June 2012

Getting back to this after week of CAP and physop duties. Next steps to 
incorporate some instability calculation into the particle analysis.
For this purpose, we need the projection along (and potentially across)
the field. That's implemented above.

Then we need to be able to do the stability analysis of the resulting 
distributions. This presumably involves the chicomplex code that has
been developed. Probably this can use the pex file?

First trying git out on loki, sceptic get a certificate error problem.
It is fixed by doing:

export GIT_SSL_NO_VERIFY=true

Apparently 
git config --global http.sslVerify false
also does the same thing.

This is a problem with the older operating systems on loki and sceptic.
Their certificate bundles are out of date. 

15 June

Found a bug preventing convergence in the benchmark runs. If only x
has multiple processes then iterations don't converge. E.g.

mpiexec -n 3 /home/hutch/src/coptic/coptic -bf7,1,0,0 -ri800 -v.5 -t.01 -dt.025 -rx.0 -l1 -s3 -w-99999 
# Point charge at origin. Spanning a radius .2. At which its potential is...
513 , 0.,0.,0., 0.,0.,0.,   .2,.2,.2,   0,-1.,0
# Measure forces at .3,.5,1.
1, 0.,0.,0.,   0.,0.,0.,  .3,.3,.3,     4,5,5
1, 0.,0.,0.,   0.,0.,0.,  .5,.5,.5,     4,5,5
1, 0.,0.,0.,   0.,0.,0.,  1.,1.,1.,     4,5,5
#
91,1,33,68,100,0,-4.,-1.,1.,4.
92,1,33,68,100,0,-4.,-1.,1.,4.
93,1,15,82,100,0,-4.,-1.,4.,8.

This is purely a point charge. Also there's a bug with the test solution
not converging because it is zero everywhere. That's easily fixed.

The other bug is present for pure coptic make runs with mpi, and with -bf
boundaries, but not without.

bf BCs don't actually seem to be working.

Tracked down this major bug. It seems that if you pass a logical array
(lperiod) whose dimension is also passed as an argument (ndims) then
within this routine, the logical array is incorrectly aligned, and also
other arguments after it in the argument list are misaligned or otherwise
misinterpreted. If however, the dimension of the logical array is a
parameter within the routine, then it works correctly (actually not). 

This problem exists with gfortran as well as g77.

But here's a major worry. I used this approach with bbdy too. So how 
come that is not broken?

No the thing is still broken even with the declaration. I don't think I've
found the real problem yet. Tried a bunch of things without success.

Moved the lperiod to the end of the argument list of bdyshare. Seems to work.
But there's still a worry about bbdy. 
Also works reverting to passing the ndimsdecl argument.

Because I am really worried about this problem. I move lperiod to the 
end of the argument list in bbdy() too. This may have affected some
routines in testing/

bbddecltest.f tries to reproduce this error in microcosm. but fails to do
so. Therefore I am still worried, because perhaps there is another 
problem in bdyshare that was the real cause and I've just bypassed it 
for now.

23 Jun 2012

Changed the partaccum to a septic profile to get rather more points
further out on the velocity range. The results are slightly better for
the moon case. But surprisingly still don't seem to put enough points
far out. It may be we need a different algorithm for such high speed
cases with big dynamics.

24 July 2012

I need periodic boundaries for particles as well as for potential. 
For example, in trying to set up a plasma with 1-D gradients to mock
up (e.g.) a presheath. This ought not to be too difficult. But it 
needs changes to padvnc and to the reinjection routine.

I think the natural place to put the periodic particle relocation is 
in partlocate. There are in that routine, tests for each dimension to
determine whether the particle is in the mesh (linmesh). Those can be
readily extended to relocating the particle periodically. 

Create a new set of flags ipartperiod(ndims) in partcom.f that tells
if particles are to be periodic in a dimensions (!=0 yes). Initialize
and set in copticcmdline.

Implement in partlocate the relocation in periodic directions. 
partlocate returns the region of the relocated particle. It seems
this is sufficient to do everything that padvnc needs. Notably, it seems
that this will satisfy all the region tallying etc. at least for objects
in an unbounded mesh region.

However, we now need to fix the reinjection statistics etc. 
This only really applies to cartreinject. But ought not to be as
difficult as the strong magnetic field correction.

In the initialization of gintrein, it is constructed by multiplying 
grein by area(id). It looks as if one can eliminate injections on
faces in one dimension by simply putting that area to zero, or putting
grein to zero. Area is also used in ninjcalc. That's another place to 
correct. It does not use grein. rhoinfcalc too.

Renamed area in cartreinject to fcarea to disambiguate.
Added fcarea to partcom.
Removed fcarea local declarations and now rely on partcom declarations.
Now they should be calculated in common. So once this is initialized
it ought not to be necessary to recalculate in rhoinfcalc, nijcalc.
Actually ninjcalc it is necessary because that is called before we do
any reinjections. That's the place where it is first initialized.
But that's not called unless ripernode is set. So we still need to do
initialization in cinjinit. Unfortunately rhoinfcalc is also called
before the first padvnc. So initializing in cijinit is irrelevant.
The fact that making this common has not changed the result seems to
show that the recalculation of it in rhoinfcalc does not change the 
value. No that's irrelevant. fcarea is only used once in cijinit, 
and it's calculated there. Bottom line. Attempts to rationalize fcarea
calculation are not worth it. The uses are effectively disjoint.

Put in 3 places fcarea(i)=1.e-6 (factor) for periodic directions.
What do we do if all three directions are periodic? In ninjcalc and
for reinject this ought to be ok without an absorbing particle because
we ought never to reinject.  For rhoinfcalc, we ought to get the
fallback for nrein lt 10 to just divide the number of particles by
volume. 

But what happens when we have an absorbing object present? Then we'll
get some reinjections, and these might well be more than 10. Comes
down to the fact that calculating rhoinfinity on the basis of
reinjections makes no sense with fully periodic particles. Perhaps one
ought to track the number of relocations as well. That's the thing 
that includes the same quantity. A relocation is practically equivalent
to a reinjection. If we treated a relocation exactly as a reinjection,
then we would not have to do different calculations for ninjcomp or
rhoinfcalc. The area modification ought to be removed.

Add an extra argument nreloc to partlocate. Increment it when relocated.
nrein is passed when it is called in padvnc. Remove the adjustments from
ninjcalc, rhoinfcalc, because now nrein includes all relocations.



25 July 2012

Ok, I think we should be done. How to test this? One way might be with a 
uniform plasma and no objects. Set up a geometry file uniform.dat for this.
But actually on the way implemented extra interpretation of the object
file that allows to put partperiod information into it. Also did some
improvement of the -h documentation of values, and adjusted the order
of things and the use of copticmdline to enable commandline arguments
to override the values set in the geometry file. 

Running a no-objects geometry with periodic particles and potential in
the x and y directions and fixed potentials on the z-faces.  If
potentials are equal to 0. (-l.01) the plasma is simply uniform for
all (subsonic) velocities. If they are different, then the plasma
potential floats up a bit and there are sheaths adjacent to each end
of the box.  This is with nominally fixed injection count. The actual
nrein exceeds that count giving e.g.  2651 2572 2546 vs 2457. This
might explain why the potential rises above zero. I'm not crystal clear
why this excess arises. I guess that it must be made up of periodic particles
after the reinjections of end particles have stopped. It's dropping only
very slowly towards the right value (after 300 steps). It can be made closer
by using acceleration up to -da20. Then the systematically high average 
goes away. 

Perhaps one ought to discard relocated particles if nrein is already 
exceeded. That way presumably one will approach the right density 
more rapidly. May not be necessary with acceleration.

Now, can we implement collisions in such a way as to give a systematic
gradient? Initially collisions don't do much. This presumably because 
they are implemented at exactly the drift speed, so they don't actually
drain any momentum from the ions. This can be fixed by doing -v.5 -vn0.

Find that yes rather consistent results are obtained with velocity profile
depending on collisionality.

26 July 2012

Implemented periodicity ninjcalc and rhoinfcalc that ignore periodic 
boundaries unless all are periodic. Also partlocate increments nreloc
only if all ipartperiod are non-zero. This causes the potential rise
to be less than with the earlier approach which always increments nreloc.
However, it also shows instability which drives the potential to large
negative values under some conditions. So it's not so obvious this is a
good plan. The velocity profile obtained looks similar to the other
count case.

27 July 2012

I realise there is an inconsistency in the implementation of particle
periodicity. The potential is periodic by virtue of the fact that the
edge potential value corresponds to the nodes _adjacent_ to the edge
on the opposite face. This is part of the guard-cell approach to block
decomposition. Consequently, the join between the opposite faces
really takes place half-way between the adjacent nodes and the edge
nodes (on both sides). However, particles are currently allowed to
travel all the way to the edge node location before being considered
lost. Then in the periodic particle situation, they are moved to a
periodic position on the opposite face, using the full domain. In
short, the periodicity length of the potential and the particles are
different the way I have implemented it so far. This is definitely
unsatisfactory. The charge on the edge nodes is never used in the
potential solution (I think). The edge potential nodes are set purely
by the boundary condition. However, the charge on adjacent nodes IS
used and in non-periodic cases requires charge to be deposited from
the full cell out to the edge nodes, so that they have full contributions.

If, therefore, periodic particles were implemented consistently with
the potential domain, i.e. out to the half-boundary (half way between
the last two nodes), then one needs additional contributions to the
adjacent node charge. Presumably this contribution is from the
opposite edge nodes. So one way to implement this consistently would be

1. Particles occupy only out to the half-boundary. Then they are relocated 
to the opposite half-boundary. 

2. Charge is deposited between boundary and adjacent nodes in the usual way.

3. The total charge on the adjacent node is the sum of its directly deposited 
charge PLUS the charge deposited on the opposite edge node. 

[4. This can also be considered the charge on the opposite edge node.]

This requires changes to the charge deposition for periodic faces compared
with non-periodic (as well as relocation etc). For consistency it also
requires the last cell to have the same size on both (opposite) faces.

An alternative would be to retain the full particle and potential
domain but to implement the potential periodicity differently. Namely
that the periodic boundary is exactly at the edge nodes (on opposite
faces). AND the potential on those nodes must be determined by a
difference stencil that includes the adjacent nodes from both faces.
This is a bit troublesome, but probably not infeasible. It would not
require changes to domain tracking or charge deposition, but would
require summing both edge charges. I don't like this option much.


28 Jul 2012

To implement the changes to chargetomesh, run up against the need for a
general iterator over indeterminate dimensions again. Could use the 
mditerate scheme, but that is really cumbersome. There's got to be 
a better way. It is much more transparent if the code one wants to 
iterate is able to be placed in line, surrounded by the equivalent of
nested do loops. It ought to be possible to create a more general 
iterator that works like the beginning and end of loops. This is appropriate
in chargetomesh, because there, the arrays are in abstract form.

I think I need to think harder about the minimal requirement of an iterator.
It seems that the fundamental requirement is to calculate an array index
that corresponds to the next position, and to determine when to stop.
How the next position is determined might be variable, but could perhaps
be specified in terms of a logical step size that is converted by the 
iterator into an actual step. 

If the iteration is specified in terms of a step in the lowest dimension,
things are easy. But if instead there is a fixed value of a higher dimension,
how do we do that? 

One abstraction that might help think about this is the idea of a "view"
of a data set. In NumPy, this can be in the form 
 A(start1:end1:stride1,start2:end2:stride2,...)
In other words, each dimension can be strided and truncated.
In a sense I already do truncation by having ifull and iused. iused
defines the end for each dimension. In a sense I can define the start
when passing an array like A(2,2,2,...) or the linear equivalent. 
I lack the striding ability, except by specific incrementation. 
The NumPy discussion conceives of a general iterator as:

	set up iterator
	  (including pointing the current value to the first value in the array)
	while iterator not done:process the current value
	point the current value to the next value

which is consistent with my conception. 

A general structure for specifying NumPy equivalent striding would be
  iview(3,ndims) where for each idim we store start:end:stride.

Illustrating
1. if each iview is 0:n_idim-1:1 we iterate the whole array.
2. if for dimension id iview is X:X:1, but the others are as in 1, we iterate
   a slice in dimension id.
The following is wrong. Using a stride of N-1 gives a diagonal slice.
3. if for dimension id iview is 1:N-1:N-1, we iterate only the id boundaries.
Boundaries would have to be done with two iterations 0:N-1:N and N-1:N-1:N.

In current implementation, I am generally passing end (iused) and full (ifull).B
ut not start or stride.

The general iterator has to move the pointer and decide if we are finished.
The conversion from multidimensional to linear pointers is 

    ipointer=indi(ndims)
    do id=ndims-1,1,-1
       ipointer=indi(id)+ipointer*ifull(id)
    enddo

which gives 
  ipointer=indi(1)+(indi(2)+(..indi(ndims)*ifull(ndims-1)...)*ifull(1))

Implemented as integer function indexcontract. Then the iterator does
not need to know about ifull, because only indi is used.
Created an integer function mditerator to be used as a generic 
iterator.

30 July 2012 

Implemented periodic partlocate that does relocation of particles that
are outside the outer half of the edge cell.

Implemented chargetomesh periodic summation of psum (not yet diagsum)
to compensate. 

Got to run, but realize that there are several other problems seemingly
all associated with xmeshstart/end. 

1. pinit: we don't want to initialize particles outside the periodic domain.
2. reinject: we don't want to reinject particles ditto.
3. reinject etc: we want the area of faces to be correct.

These all seem to be dependent on xmeshstart/end. So if we simply change
meshconstruct to define xmeshstart/end for periodic dimensions to be at
the half-cell position, then probably these are fixed. 

The only other place that xmeshstart/end are used is in partaccum for
particle diagnostics. Again that seems to be corrected by using the adjusted
values. 

Actually, the partlocate could have used this redefinition, except that
it operates in mesh-space rather than real space.

So modified meshconstruct to make xmeshstart/end different for periodic
directions. Code runs at least.

---------------------------

Hummm. MPI does not seem to work for multiple processors. Perhaps it does
not work to put this exchange in the chargetomesh. Each processor only 
has its own charges until after psumreduce and diagreduce. 

Separate out the psumperiod and diagsumperiod parts into their own 
routines that are called after the reduces. Still doesn't seem to fix
the mpiexec -n 2 result. Actually I don't think there's really a problem
with mpi and particles. Each processor has particles everywhere. So
the separation wasn't really necessary (but did not hurt). 

Running with high Debye length shows this bug to good effect. The 
density on the positive z-edge for x<0 progressively increases. But 
only with two processors. It is the x-direction that is split.

It happens even with no periodic particles! Also without collisions, but
not so bad. DOESN't happen if -bf6,1,0,0 instead of -bf6,1,0,4 
(but that seems bizarre.) This is pretty puzzling.

Moving aside my cartreinject.f and doing git checkout -- cartreinject.f
brings back the old scheme. It shows the same error.
This seems to show this is not a new error in cartreinject.

The 14Jun version which is mostly from 6 Jun does not show this problem.
Cloned the current git repo from tp400 which is later than the github
version to copticcopy. It does not show the problem with
mpiexec -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5 -l100. -ct20. -gs -gp -bf6,1,0,.5

There are many files that differ. I've shown it isn't cartreinject. 
Files 3dobjects.f and ../coptic/3dobjects.f differ
Files cartreinject.f and ../coptic/cartreinject.f differ
Files chargetomesh.f and ../coptic/chargetomesh.f differ
Files cmdline.f and ../coptic/cmdline.f differ
Files coptic.f and ../coptic/coptic.f differ
Files facebcom.f and ../coptic/facebcom.f differ
Files interpolations.f and ../coptic/interpolations.f differ
Files mditerate.f and ../coptic/mditerate.f differ
Files meshconstruct.f and ../coptic/meshconstruct.f differ
Files padvnc.f and ../coptic/padvnc.f differ
Files partaccum.f and ../coptic/partaccum.f differ
Files partcom.f and ../coptic/partcom.f differ
Files pinit.f and ../coptic/pinit.f differ

The issue did not arise from the bbdy fixes, therefore. But it is hard to
guess how to figure out where the problem arose. 

Copied 3dobject.f from ..coptic. No problem.
Copied cmdline.f segfault. Not tested. 
Copied partcom.f. No problem
Copied facebcom.f No problem
Copied cartreinject.f No problem
Copied interpolations.f No problem
Copied mditerate.f No problem
Copied partaccum.f No problem

So we've narrowed it down to 
Files chargetomesh.f and ../coptic/chargetomesh.f differ
Files cmdline.f and ../coptic/cmdline.f differ
Files coptic.f and ../coptic/coptic.f differ
Files meshconstruct.f and ../coptic/meshconstruct.f differ
Files padvnc.f and ../coptic/padvnc.f differ
Files pinit.f and ../coptic/pinit.f differ

Copy meshconstruct segfault. This is longer argument list. git
checkout meshconstruct.f

We need now to change to the new coptic.f calls. First commandline.
In copticforward directory.
$ for file in coptic.f ; do diff -u $file ../coptic/$file > patch; done
Then patch <patch makes both coptic.f the same, but git checkout coptic.f
does not restore the old one in copticforward. Thatmeans I must have
patched the ../coptic/ version. This is corrected by  patch -R <patch
which seems to reverse the patch.

$ diff -u ../coptic/coptic.f coptic.f >patch.forward
$ patch <patch.forward
patching file coptic.f
Reversed (or previously applied) patch detected!  Assume -R? [n] y
That patches the whole thing. But I just want to do the beginning. 

So just patched the first hunk which should be cmdline and meshconstruct.
Copied cmdline.f PROBLEM.
git checkout cmdline.f No problem. 

Thus the problem is definitely involved with the new cmdline.f
It has a more complicated structure to do with first and second calls.
Patched the leading part of the old cmdline.f to pass arguments,
ipartperiod. Then perhaps there is a problem but partly obscured by 
rounded values. But that problem is there with old cmdline too. 
Consequently, I am not sure I've really localised the problem. 

I need to quit for now. I am not really to the bottom of this. 
Save the partly edited coptic.f as coptic.stage1, and cmdline as cmdline.stage1
Just using cmdline.stage1 does not introduce the problem. It appears
that coptic.stage1 is where the problem lies. The double call, and
some repositioning of readgeom. 

1 Aug
I'm not sure that the conclusion I made is true. Certainly there's some
combination of coptic.stage1 and cmdline.state1 that seems to produce 
a problem. But I don't think I can isolate it yet. But now running
with the patched coptic.f, but old cmdline.f, AND commenting out the
second call to cmdline, I get ok results, while with the second call I don't.
Without second call the return from cmd gives:

Two cmdline calls with the fully updated cmdline.f gives:
0:  T F 0 1  0. T 1 T F F -1 0  0.100000001 1 100 0 1  100.  0.5 0  20.
0:   0.100000001  5.  0.  10.  1.  0.  0.  0.  0. 0 300 3000  0.  1.60000002 0 7
0:   100.  0.100000001 99999 0 F
0:                                                                                                      
0:   0.  0.  0.
0:  unif5.dat                                                                                           
0:  F  0.  0.  0.  0. 3 0  1.60000002  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.
0:   0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
0:   1.  0.  0.5  0.  0.  0. 5 T T F 0 0 0
Gives asymmetry.

1 call with cmdline.stage1, (new coptic.f)
0:  T F 0 1  0. T 1 T F F -1 0  0.100000001 1 100 0 1  100.  0.5 0  20.
0:   0.100000001  5.  0.  10.  1.  0.  0.  0.  0. 0 300 3000  0.  1.60000002 0 7
0:   100.  0.100000001 99999 0 F
0:                                                                                                      
0:   0.  0.  0.
0:  unif5.dat                                                                                           
0:  F  0.  0.  0.  0. 3 0  1.60000002  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.
0:   0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
0:   1.  0.  0.  0.  0.  0. 5 T T F 0 0 0
No asymmetry problem. 

There's a difference in the last line, item 3: 0.5 That seems to be the last
CFin. Without it there's no asymmetry problem. 

Is that being reset to zero by readgeom? Seems so. Yes. That's because
there are lines in unif5.dat that set it. Confusing.

Make all the arguments in the commandline to avoid confusion.
Then it seems that the original version of copticforward gives the
asymmetry.

mpiexec -l -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5
-l100. -ct20. -gs -gp -bf6,1,0,.5 -bf3,1.,0.,0. -bp1 -bp2

coptic14Jun does not give asymmetry. Seems my previous tests were
incorrect. Have to backtrack.

partaccum.f
mditerate.f
interpolations.f
cartreinject.f
And all the rest.

It transpires that there IS an asymmetry with the copticforward
version.

Ok I confirm that checking out the prior commit gives no asymmetry.
It was that last commit that caused it. One can go back and forth
between known versions by 
git checkout bc8df1863f8e0a8faa698e70cb03d52db0fc9dab
and  
git checkout 8c25d5581d646f0dec2efde1ffc07edb45f5261d

But don't do this in the coptic directory!!!!

Summary for today. 
The asymmetry bug arose between the above two versions. They differ
by the discovery of the argument problems passing lperiod and the 
reordering of the arguments of bbdy and other places, to try to 
avoid bugs. Seem to have introduced bugs instead. What is needed next
is to return to that prior bugged version and find out where the
asymmetry really turns on. Given that the prior bugged looked like
a compiler bug. I am in a very sticky spot. Might try a different 
compiler on Loki???

Return to the backward version 8c25d558. 
 mpiexec -l -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5 -l100. -ct20. -gs -gd -bf6,1,0,.5 -bf3,1.,0.,0. -bp1 -bp2
shows incorrect boundary setting. Periodic boundaries are not working.
But periodic are working with -n 1. However, -bf6 does not show the 
value -0.5, although the C0F does show 0.5.

Put a write on entry to bdyshare and of arguments just before call.
They don't agree. Arguments are not being passed correctly into bdyshare.

Constructed a bbdecltest.f that works and calls bdyshare. 
Made bdyshare implicit none. FOUND THE PROBLEM! It was an incorrect
length first line of argument list that happened to have the final
comma in column 73. It therefore concatenated that with the first
argument of the next line and misalinged all the succeeding arguments.
That was the issue. The passing of logicals etc was irrelevant.

Now how to back-track? I'd like to get the reorganization of the
argument order out of the picture to eliminate that possible cause 
of problems. However, the reorganization of the argument order did
indeed remove the column 73 error. So possibly one could accept 
that reorganization. 

Actually with the fixed copticbackward. I find that the asymmetry 
problem is present. Therefore, the asymmetry problem was in the 
copticbackward version but was masked by the error in the bdyshare
call.

I need to go further back. Try commit c1301209920104a0df8c3946e3a0aa3b3126f9cd
doesn't seem to show asymmetry.

Try 08ed85fb4c8e298 May 29th ditto.

Try d829957c4c6c515e9fd Jun 3rd has asymmetry.

Try 4383ebc59c3145 Jun 2 has no asymmetry.
Call this the new copticbackward 

Call d829957c4c6c515e9fd the new copticforward

Files ../copticbackward/3dobjects.f and 3dobjects.f differ
Files ../copticbackward/bbdydecl.f and bbdydecl.f differ
Files ../copticbackward/bdyroutine.f and bdyroutine.f differ
Files ../copticbackward/bdyshare.f and bdyshare.f differ
Files ../copticbackward/cmdline.f and cmdline.f differ
Files ../copticbackward/coptic.f and coptic.f differ
Files ../copticbackward/partaccum.f and partaccum.f differ
Files ../copticbackward/partwriteread.f and partwriteread.f differ
Files ../copticbackward/ptaccom.f and ptaccom.f differ
Files ../copticbackward/sormpi.f and sormpi.f differ

Copy partaccum ptaccom to backward. No asymmetry.
Copy partwriteread. No asymmetry.
Copy cmdline. No asymmetry.
Copy coptic.f. No asymmetry.

Copy sormpi.f Big changes and strangeness, so put it back.  Now

Files ../copticbackward/3dobjects.f and 3dobjects.f differ
Files ../copticbackward/bbdydecl.f and bbdydecl.f differ
Files ../copticbackward/bdyroutine.f and bdyroutine.f differ
Files ../copticbackward/bdyshare.f and bdyshare.f differ
Files ../copticbackward/sormpi.f and sormpi.f differ

Copy bbdydecl.f. Have to remove the ifull declaration from sormpi and bdyshare.
Then no asymmetry.
Copy bdyshare.f no asymmetry.
Copy 3dobjects.f no asymmetry.
Copy bdyroutine.f no asymmetry.

Files ../copticbackward/sormpi.f and sormpi.f differ

Doing ediff. Update idone to an array. No asymmetry.

Current differences:
232,234c232,243
< c Boundary conditions need to be reset based on the gathered result.
< c But that's not sufficient when there's a relaxation so be careful!
<          call bdyset(ndims,ifull,iuds,cij,u,q)
---
> c Boundary conditions need to be fully set based on the gathered result
> c at least for the master node, for aesthetic plotting reasons.
> c Call the parallelized boundary setting routine but lie to it that 
> c it is the only process. Also insist on explicit setting.
>       idone(2)=1
>       idone(1)=0
> c Actually it would not hurt if every process did this.
>       if(myid.eq.0)call bdyshare(idone,ndims,ifull,iuds,cij,u,q
> c     $    ,iLs,idims,lperiod,icoords,iLcoords,myside,myorig
>      $        ,iLs,ones ,lperiod,zeros  ,iLcoords,iuds  , ones
>      $        ,icommcart,mycartid,myid)
> c Old global setting. Obsolete.

Yes this is the difference that causes asymmetry.
The offending item is if(myid.eq.0). All processes must call the 
global boundary setting, otherwise they do not have the full potential
grid set properly. Since particles use that grid, it is not optional to
have it fully set. 

I think I ought to be able to fix this whole thing by removing if(myid.eq.0)
from the current version. Done. 

Commit Push
------------------------------------------------------------------
3 Aug 2012.

It would be quite easy to introduce collision time that varied as a
function of position or of velocity. All that would be required is
a call to some scaling function at the time of deciding whether a
collision has occurred. 

One possible use of this would be to make more efficient use of the
code in application to a collisional presheath. Since the scale length
of the presheath is the collision mean-free-path, one could scale the
collision length to be shorter, at positions further from the sheath. 
That would allow the distant solution to be well converged within a
shorter length, and mean that one did not spend so much of the PIC
effort calculating what is effectively the background problem, rather
than the local problem (e.g.) surrounding a grain in the (pre)sheath. 

It's rather important to have -rx0. for these full domain cases with
periodic conditions, otherwise instability occurs induced by spurious
attempts to adjust to the edge potential. Put in an exception to prevent
such operation.

In particle-free collisional gradients, there appears to be a z-flux
gradient (diag(4)) that depends upon the drift velocity specified. I
don't understand that. It is not reduced by smaller mesh spacing.
Or by wider transverse dimension. Actually it seems to correlate with
a time derivative of n_part. So perhaps it is simply correct that there
is a divergence. I'm going to adopt this view for now.

5 Aug 2012

Reimplement mditermults and mditeradd using the new mditerator.
They are much easier and more transparent.

Plan for rationalization.
1. Turn all calls to mditerate into mditerarg. Arg just allows more
   arguments on the end. Consequently straight replacement ought to work.
   Right now only bdyroutine and bdysetsol call mditerate.
   Do replacement. Nothing seems to break.
2. Reimplement mditerarg.

There's a problem reimplementing mditerarg. It is that the routine 
gets called with essential inconsistent indi sets 
For example with all lengths 6, the old mditerarg goes
  4  1  1  20104    1
  5  1  1  20105    1
  6  1  1  20106    1
  1  2  1  20201    1
  2  2  1  20202    1
versus what a straight forward reimplementation gives:
  4  1  1  20104    1
  5  1  1  20105    1
  0  2  1  20200    1
  1  2  1  20201    1
  2  2  1  20202    1
where the arguments have wrapped (correctly) at the 1-boundary.
This appears to be because in mditerarg:

c Wrapping occurs at the _iused_ dimension
c length relative to the starting indices.  In other words, if the
c incremented indi(id)-indinp(id)+1 exceeds the used length iused(id),
c then the next higher dimension is incremented by 1, and indi(id)
c decremented by iused(id).
I have not implemented the starting index part of this yet.
Fixed. 
Also had to implement iLscom setting. Then finally we get the same
answer.

Time testing on mditeratetest shows the new mditerarg is slower by 
perhaps 50%. Profiling shows that the costs are shared between the 
three routines rbroutine, mditerator, indexcontract; as one might
have expected. However, the speed is very high. The total time
for 1000x100^3 rbroutine calls is about 8s. So that's about 8ns per
point. The only place where that might be important is within the
sormpi iteration loop. I.e. boundary setting. And even there it is 
not likely to add up to much.

iLscom is a total hack. Several routines use it but it is set only in
mditerarg (I think). That's a major problem. Most of the usages arise
because of mditerarg argument conventions. Therefore the sensible
approach to getting rid of it would be to moved towards getting rid
of mditerarg. Alternatively we could change the argument list of 
the routine called by mditerarg to include it. 

There are ~15 different routines called by mditerarg. All would
have to be changed. A bit of a pain.

There would be residual problems with reduce because iLs is not included
in mditcom.f. Still, since it isn't used in those routines it probably
wouldn't be too bad. I don't think there's a problem.

git commit -a

-----------------------------------------------------------------------
fgrep 'mditerarg(' *.f | sed -e "s/^.*mditerarg(//g" -e "s/,.*$//g"

bdyface
bdyslopeDh
bdyslopescreen
bdymach
bdy3slope
bdyslopeDh
bdy3slope
bdyshrroutine
bdyshrroutine
cijroutine
volnode
cijedge
psumtoq
quasineutral
routine
ucrhoset
iteradd
iteradd
cijroutine
cijedge

Purged out the iLscom from all files, by adding to argument list.
Also in phisoluplot.f calculated and passed to needy routines.
mditerarg still needs to calculate it and that fixes reduce.

6 Aug 2012
Fixed an error in the new mditeradd.

git commit -a

Fixed bulkforce to use vneutral to calculate the momentum loss.
This should work correctly provided there's no driving Eneutral, which
is not (yet) implemented in coptic.

 mpiexec -n 2 ./coptic uniform2.dat -s400 -v-.6 -vn0. -t.03 -da10 -ds1. -l1. -ct20. -md4 -gs -gd -rx0.
produces a bulkforce that is still evolving (getting larger) with time during
the run. It also does not give agreement between different spheres.
Not clear what the problem is. 

Compare ./coptic -v1 -ct1. -ofgeomforce.dat -s200 -ri400 with new coptic 
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  0.500 zcenter=  0.000 Charge=   -2.6908 =====
     0.00883     0.01075    -0.00072     0.01021     0.02907
     0.00612     0.00070    -0.00070    -0.00032     0.00580
     0.19566     1.17729    -0.02441     0.56033     1.90886
===== Object 2 ->  3 radius=  1.000 zcenter=  0.000 Charge=   -8.5194 =====
     0.01088     0.04131    -0.00717    -0.01681     0.02821
     0.00498     0.05935    -0.00304     0.01155     0.07284
     0.22069     1.23143    -0.15176     1.19035     2.49071
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.0372 =====
     0.00744    -0.00210    -0.01884    -0.04048    -0.05398
     0.00531    -0.03313    -0.01100    -0.02103    -0.05985
     0.05078     1.07863    -0.32375     1.65627     2.46194
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -3.9712 =====
     0.00473    -0.05140    -0.04172    -0.05803    -0.14642
     0.00031     0.04795    -0.01667    -0.08757    -0.05599
     0.00546     1.02033    -0.43453     1.96473     2.55599

and Jun14th version:
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  0.500 zcenter=  0.000 Charge=   -2.7091 =====
    -0.01209    -0.01049     0.00118    -0.00415    -0.02555
     0.01653     0.00276    -0.00164    -0.00548     0.01217
     0.19764     1.06559    -0.02442     0.55081     1.78963
===== Object 2 ->  3 radius=  1.000 zcenter=  0.000 Charge=   -8.5595 =====
    -0.01439    -0.02641     0.00924    -0.02864    -0.06021
     0.02715     0.00463    -0.01409    -0.04490    -0.02721
     0.21429     1.16204    -0.14992     1.24086     2.46728
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.1374 =====
    -0.00737     0.02001     0.02408    -0.08309    -0.04637
     0.01199    -0.05386    -0.03684    -0.06926    -0.14796
     0.05046     1.00457    -0.31815     1.72937     2.46625
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -4.0830 =====
    -0.00037    -0.02474     0.03537    -0.17158    -0.16132
     0.00638     0.04012    -0.06936    -0.12916    -0.15202
     0.00812     0.75871    -0.43419     2.02579     2.35844
Not identical but pretty good agreement.

That is with point charge radius 1. potential -1. which is less than
what I am running with sheath model: radius .4, potential -1. The
mesh spacing is ~.25.

Did a new run with sheath model and -ct2 still does not seem to make any
consistent sense. Still evolving substantially.

3 Aug 12 Non-uniform sheaths. See hand written notes.

22 Aug 2012

Implement separation of Tneutral from Ti, in cmdline setting.
In padvnc change tisq to be sqrt(Tneutral) instead of sqrt(Ti).
Thus we can make the temperature of the reinjected ions Ti different
from that of the collisional ions.

Thinking about giving true drift distribution input to reinjection.
Functions ffcrein and fvcrein are the crucial ones. They are
Maxwellians (at present) that are integrated using cumprob to give
the flux and velocity distributions of various dimensions. The idea
is that we change these just for the drift direction (z). At the
moment these are unnormalized. This does not matter because the
same functions are used for all the faces and so the relative probability
is all that counts, and the relative unnormalization is the same. 
If, however, we substitute a different function, the collisional drift
function, for one dimension, this must be correctly normalized relative
to the Maxwellian function. Those functions are given in terms of the
standard dimensionless velocity (i.e. not normalized to the ion or neutral 
temperature thermal speed; the T_i is explicitly accounted for in the 
function). And they are equal to unity [times abs(v) for ffcrein] at the
Maxwellian's peak. It seems that if we normalize them so that the 
area integral \int f(v) dv = 1, then we can normalize the drift distribution
the same and we'll be able to use both simultaneously for different
faces. This means we have to divide by sqrt(2\pi T_i). Multiplying
each by 2 gives no change to test output. Dividing by  sqrt(2\pi T_i) gives
a change. I'm assuming this is a rounding problem.

Implemented drifting distribution in the reinjection. In general, this 
will revert numerically to drifting Maxwellian distribution when vneutral
is equal to vd. Therefore, the code is left as the default for all cases.
The code for 1-D distributions with strong magnetic field not fixed
implemented with the collisional drift effects yet. 

Tested using testing/creintest.f with some development to normalize the 
routines and compare them. 

It would be helpful also to have an absorbing boundary choice such that
particles are never injected from some boundary, e.g. because it is a
wall. The natural way to do this is to set fcarea to zero for this 
face (as I do with periodic conditions), but this has only 3 dimensions,
so it does not work when a face is opposite a non-periodic face.
Therefore I have to set grein(id) equal to zero for this face, prior
to the calculation of gtot and gtrein. 

How to denote this? Probably use different values for ipartperiod. 
We could use the first two bits to denote non-injection for the lower
and upper faces in a specific direction. In that case, 1 would denote
non-injection lower, 2 non-injection upper, 3 non-injection both.
Perhaps we therefore make 4 periodic particles. In that case 3 can
be used for two absorbing faces opposite one another. 0 means nothing
special, open boundaries. Need then to correct other routines for this
change of meaning. 4 <-> periodic.

There's still an issue of ninjcalc rhoinfcalc etc. This must be fixed
in fonefac. 

24 Aug 12
Implemented new section in fonefac that represents the fluxes in either
or both faces for possibly different vd and vneutral. This is supposed
to give the flux from the general drift distribution, which can vary
from drifting maxwellian (when vd=vneutral) to pure drift when vneutral=0.
This section gives the same result as the old explicit form for a 
drifting maxwellian when vd=vneutral (after corrections!). However, 
it is not fully tested for situations when vd != vneutral and the drift
distribution is fully in play. 

25 Aug 12
Seems to be in good shape now. BUT proper treatment of collisions with
magnetic field has not been implemented. So the drift-collisional 
distribution is not used with infinite field, and it is not correctly
field-aligned with a finite B-field. 

Collisional sheath simulations work much better using phi derivative equals
zero on the upper face. We now can also set the lower face to be absorbing
so that the physics is much better defined. One can run these without
collisions too and the potential tends to be much flatter in the presheath.
The diagnostics don't seem to be what we expect at the z-boundaries. 
I'm not sure the injection is right. Found the problem. Caused by my
non-intuitive order of grein chosen long ago. Fixed.

Now one needs rather slow injection speed (relative to zero neutral
speed) to avoid a non-monotonic phi. About -v-.1 for -ct10. I don't
understand that. Can make the -ct number high (e.g. 50) without much
change.  Actually mpiexec -n 2 ./coptic uniform3.dat -v-.5 -rx0. -da10
-s200 -vn-.0 -t.1 -md4 -ct40.   does not give non-monotonic, except
just at the end, so maybe the non-monotonicity was a different
problem. I can't reproduce the non-monotonicity immediately.

28 Aug 12
Changed the -gd -gp switches to be more rational, and removed -gs switch.

12 Oct 12
Problems of data storage and restarts. 

At present on loki the end of the run is bogged down if one writes 
the particle data to disk because each processor tries to write to the 
central (head) disk and the NFS has a problem with all the pressure on 
the ethernet with all trying to write at once. The result is that it can
take ten minutes to write the data (files of maybe 100MB per node). 

Actually loki has local disk space to which the writing could be done.
Then after the fact one could go back and grab the written files if
one wanted, or a script could copy them sequentially to the head node
(thus avoiding ethernet collisions). 

Rewrote datawrite, namewrite etc to prefix files with the path kept in
restartpath, which can be set with or without restarting.

Reorganized restarting to use an integer switch. It can then indicate
whether or not we are reading flux (bit 3: 2). If we are just restarting
from a saved state in order to initialize the particles and potential, 
then we normally don't want to read the flux file. For that we would
use -fs1. A full restart is -fs3, which shows up by the fact that the
step number adds to the previous steps instead of starting from 1. 

13 Oct 12
Added ability to use the name 'restartfile' for the restartfiles.

18 Oct 12
Stop slave nodes warning about special particle error.
Added collision settings to write/readfluxfile. New version 2. 
Fix coptic -h call when coptigeom.dat file is absent.
Fix diagnostics ipartperiod(i).gt.0 error in meshconstruct.
Fix bug in diagfilename generation arisen from name code changes.

I realize that COPTIC does not have an Eneutral. So when -v and -vn 
are different, we need an actual potential gradient to cause that.
We can't just take it to be the case and do reinjection accordingly.
In sceptic Eneutral=colnwt*(vd-vneutral). In COPTIC colntime is the
inverse of colnwt. So presumably Eneutral=(vd-vneutral)/colntime.

Another issue I find is that in low collisionality sheaths, the density
evolves very slowly, and is not in equilibrium by the end of the run.
We ought to have a clear diagnostic that this is happening. Perhaps
simply the npart (and rhoinf) or their ratio.

Implemented saving of n_part. And fluxexamine gets it back and plots it.

19 Oct 12
Understanding the shark-fin behaviour of n_part on restart. Did an experiment
multiplying dtprec by bdt on restart. This is to find out if it is something
to do with dtprec that makes the sudden jump. Shows very little difference
with or without this multiplication. I conclude it is not just a transient.
This conclusion is consistent with the fact that the sharkfin looks like
the timestep, not like its derivative. 

Try running cases with different -dt. .1, .5, .03. These seem to show
that there is a systematic increase of n_part as dt becomes larger (in
near steady state). For -dt.03 about .96e5, for -dt.1 about .963, for
-dt.5 about .97. However, these do not seem to be fully consistent
with the enhancements associated with acceleration. Those might be
dominated by the initial transient. So one probably needs to look at
restarts for real consistency.

20 Oct 12 

Problem with the collisional part of the force for the new
collisional treatment.  As of now, the bulkforce is taken as given by
the sum over particles in the measurement sphere of
(vz-vneutral)/(rhoinf*colntime). Seems to be correct. But in runs it
is way out of whack, giving values of about 4 for spheres of radius
1.5, when rhoipernode= 500 for 8 nodes. Total rhoi is about 4000, so
there are 4\pi 1.5^2/3 ~ 10*4000 in sphere.  If collision time is 2,
then the normalized would be about 5 (check) but perhaps the problem
is that dt is not accounted for. It seems correct because the force is
the momentum difference per unit time. So we should not multiply by
dt. The particle force is obtained by adding all the particles
crossing and dividing by dt and rhoinfinity. The collision force is
the momentum (difference) times dt/colntime normalized by
dt*rhoinfinity which is what I've got.

I think the issue is the absence of Eneutral, in COPTIC, versus its
presence in SCEPTIC. If we consider a uniform quasineutral plasma, in
which the ion drift is driven by Eneutral, then within a measurement
sphere the loss of momentum to neutrals is made up by Eneutral. In
sceptic Eneutral is explicitly added to the ion acceleration, and the
colnforce is collf=-colnwt*(vz-partsum*vd) which is -(vz-vd) per
particle, while Eneutral=colnwt*(vd-vneutral). Assuming vneutral to be
zero, this makes force per particle=-colnwt*vz+Eneutral, which cancels
out when the vz is equal to vd. The remainder of the electric field
is explicit and acts on both the ions and the electrons. Eneutral does
not act on electrons (effectively). 

In COPTIC there is (as yet) no Eneutral. All the drift comes from the
explicit field. The explicit field acts on both ions and electrons.
There is then no steady uniform state with Boltzmann electrons,
because the potential is varying, so the electron density is
non-uniform.  The electron pressure force then balances the electric
field force on the electrons. If electron and ion densities are equal
so the charge density is zero, then the field force on the ions is
equal and opposite to that of the electrons. The ion electric field force
is balanced by the collisional drag force. Therefore in the absence
of a grain, the collisional drag force ought to be equal and opposite
to the electron pressure force. (That's not quite true if there's
any imbalance of charge density, but it ought to be close.)

This does not seem to be happening. Looks like a bug.

At z=20 the potential gradient is 0.13 in 2 i.e. 0.065.
Also the value is -0.6. 
So grad(n)/n_infty is (e/T)grad(\phi) exp(e\phi/Te) = 0.065*exp(-0.6)
= 0.036
At z=10 it is ~0.4 gradient and value -2.0. grad(n)/n_infty=0.054

For a volume of 4/3\pi ~4 (radius 1) we expect a pressure force
roughly .14 or .2. We actually get .17 and .19, I think that's near enough.

The collision force is about 10 times larger. Can't be correct. Is it 
possible that I am not dividing by the summed rhoinfinity? Doesn't seem
so. However, I am running with -rx0. which means there's no accounting
for the potential of reinjection in calculating rhoinf. That might be 
a significant effect (but not a factor of 10). 

There are at the end about 270000 particles in one process in volume 
40x5x5=1000. So 270 particles per unit volume, ~1000 in unit sphere.
For 8 processes there would be 8000. If colntime is 10 which is what I
am examinining, then there would be 800 collisions per second. Each would
lose a momentum of about 1 if the flow is roughly v=1. So the momentum
divided by the rhoinf (~2000) would be about 0.4 whereas I'm getting
about 1.5. This doesn't quite resolve the problem. We are sort of half
way between. Actually at z=20 the velocity given by diags is about 0.5.
And if I took that, I'd get force around 0.2 which is about equal to the
pressure force. At z=10 v=1.5 but there the density is substantially 
lower, so probably the collisional force would be substantially reduced.
Conclusion: it looks as if the value of the collision force is too large
in the code for what it ought to be. 

Running with 2 process produces a collisional force only < factor of 2 less
than the 8 process case. There is not an obvious problem with node accounting.

Doing a restart for a second 1000 steps and then averaging over 1400-2000
produces far smaller collision and residuals. Looks as if it is working.
Maybe it was working all along but just not converged.

There's another problem with the x/y boundaries causing some sort of density
rise. With 8 processes the rise is about 8%. With 2 processes, about 6%.
Not clear that this is significant. But I need to figure out why it's there.

One problem is that the mesh is visibly asymmetric. 

That's a particular problem with periodic domains because the presumption
is built in that the cell-size is the same on either side. But fixing it
does not fix the particle excess near the boundary. It does not show up
on the density, only on the particle distribution. That's peculiar.

22 Oct 12

Tracking down the edge particle excess for the sheath domains.
First do particle accumulation in restricted z-ranges. There's no
obvious variation with z. That suggests it cannot be caused by 
nonuniform injection across the z=40 face, because that would 
quickly be smoothed out by subsequent transverse travel.

Note, this 6% ion density variation for transverse temperature of .02
would arise from the ion response to an effective potential such that
exp(d\phi/T_i)=0.06, i.e. d\phi = 0.06*0.02= 1.2e-3 really small!

Looking harder at phiexamine, I see there really is an edge potential
drop of order 1.e-3 between the second and first cells from the boundary.
(The boundary itself is periodic and there's not much difference between
it and the adjacent cell.) Thus we have a physically consistent result
in that the ions are responding to the presence of a (very small) 
attractive well at the boundary. This is for the 8-processor run which
has uneven cells on either side. Looking at the 2-processor runs with
symmetrized cells, they still have some potential effect but it's nearer to the 
noise level. There's a sense that perhaps it is the corners that are
more systematically low. Running a 8-processor symmetric case does not
get rid of the particle excess, and confirms the 1.e-3 edge potential drop. 

The lower-potential edge is counter to the higher ion-density there. 
In other words, the potential value is inconsistent even though the
particle density is consistent. The problem must lie in the determination
of potential. 

The question is, where is that well coming from? 
The objectdata file has
####################################################################
# Domain BCs
# Periodic phi-BC in dimension 1 and 2.
111
112
# Specified phi on face 3 (lower z-face)
103,1.,0.,10.
# Specified zero derivative on face 6 (upper z-face)
106,0.,1.,0.
# partperiod: particles periodic in directions 1 and 2, absorbing lower 3.
98,4,4,1
# It's important to end the last line.
####################################################################
This appears to be correct. 

The cell size is 2/10 at the edge, which is 0.2/5 = 0.04 of the full
width (5). If there were a second order error arising at the edge
perhaps it would be the square of this which would indeed be 1.6e-3.
Thus we might need to look very hard at the difference scheme.

Run a case with uniform 48 cells in x and y. Reduces the cell size to
5/48 ~ 0.1 (vs 0.2). If the problem is second order in cell size,
this ought to drop the effect by a factor of 4. In fact the particle
excess at the edge (partexamine) is 0.3 in 5, i.e. still ~6%. It has
not changed. Similarly, the potential dip at the edge is ~1.e-3.
This seems to show decisively that it is not a resolution problem. 
The error is independent of cell size. 

I notice that the potential drop gets smaller as we go lower in the
sheath, so it is nearer 1.e-4 at the bottom. This is probably consistent
with there being a perturbation that is a constant fraction of the ions.

What might make the corners more susceptible to this drop?

Try increasing the timestep to .2 to see if that changes the magnitude.
Makes no difference. 6% particle excess, ~1.e-3 edge potential drop.
It's not a timestep scaled error.

Checked the volumes at the edge of the mesh. They are
0.  0.00121212041  0.00121212006  0.00121211959  0.00121212006
showing that rounding is only at the 10^-7 error level.

Looking at denexamine, we see that the density appears low at the
edges.  By about 0.04 near the top (much less, ~0.01 near the
bottom). This contrasts with the particle distribution which gives it
6% high. Seems to show that there must be errors in translating from
particles to density at the points adjacent to the boundary.
This density effect is NOT visible in diagexamine. That's interesting.
The particle boundary exchange for diagnostics is done by diagperiod.
Meanwhile the psum boundary exchange is done by psumperiod.
Perhaps there's a bug in psumperiod. There is a difference, in that
psum on the boundary is not set to zero, while diagsum is, in the
process of particle transfer. 

Test whether using diagperiod in place of partperiod gets rid of the
problem. It's too hard to tell with 2-processor case. In the 8-processor
case it is clear that the replacement of psumperiod with diagperiod
has NOT fixed the problem. 

However, I note that psumreduce does not reduce the boundary nodes.
Moreover it is called prior to psumperiod. Therefore, it appears that
there's a bug in that not all the boundary values are going to be 
reduced to the final result. So try exchanging the order of psumperiod
and psumreduce. 

YES!!! This was the bug. Because of incorrect order of period cross
sum and psumreduce, the multiprocessor exchange was not being
correctly evaluated. With them swapped, it is and the particles are
uniform.

Conclusion for commit:
---------------------
The particle non-uniformity for periodic faces was a bug arising from
incorrect order of psumperiod and psumreduce. Because psumreduce does
not reduce the boundary values, the periodic sums psumperiod must be
done BEFORE the psumreduce. Now fixed.


Rationalized the restart code so that it does not matter if the potential
is not correctly set. In any case it is simply calculated from the particle
data, so it does not matter if one fails to read the potential file.

There's a problem, though, with chargetomesh, the first time it is called
after a restart, if the mesh spacing has been changed. It is that the
top 3 components of x_part, which give the mesh position, are inconsistent
with the position in the context of a changed grid. Chargetomesh uses
these in preference to partlocate. Fixed this with a new initialization.
locateinit.

Improved partexamine to allow summing subcells over dimension x,y, or z.

Fix restart so that it does not read the potential file if it fails to 
read the flux file. The reason is that the potential file contains 
ixnp, xn etc, which clobber the mesh setup. So if a potential file
is read then one can't restart with a different mesh spacing. Arguably
this could be fixed by simply ensuring there's no potential file to
read. But I think it is probably safer to avoid the read in all situations.

Edit the RefManual to reflect recent changes.

Tracked down the problem with titles in denexamine. It is that there's
a bug in the extension of slicegweb to have optional array arguments.
That can't be done because one of the arguments is a character string.
It's length is invisibly added on to the arguments at the end. If the 
call has fewer arguments than the definition, then that character 
length specifier is in the wrong place and the subroutine gets confused.
Actually it grabs something beyond the argument stack, which might 
cause a segfault. This needs to be fixed. One way to fix is to give
all the calls the full argument list with two dummy real arguments
after the utitle. This is what I've done with all the analysis routines
and coptic routines for now.
Possibly we should change the subroutine name of the extended version
and wrap it in the old name with dummy arguments. Not done.

9 Nov 2012.

Running check shows that restarts are not now correct. That's a problem.
Commenting locateinit does not fix. Bad. It's not clear how long this
problem has existed. Things do not look so bad on loki. But there are
in fact differences with that version. I need to track back.
Coptic 14June has same differences. So this has not been introduced 
by the recent changes. Looks as if it might even predate the git
repository move. Even so, I ought to be able to get a version out
of the repository if the cvs import kept track of all versions.

Using copticforward to bisect to where the error occured.
2012-01-04 ok.
2012-04-17 broken.
2012-02-09 broken.
2012-01-15 broken.
2012-01-05 ok.
d4661836cc2e035fbd83aece5b6b0bdaa25cf1d4 09 broken.
8d8b068bd920ffdeea22f6508a286f590ba1a811 04 ok.
c22bbfa3aa8ae99c9cf0528267bb8f85802c405c 05 ok. 
627bf5af9ea6b1ea5858c65cacef01f6f734dee1 05 ok.
--This is where the problem arises.
d4661836cc2e035fbd83aece5b6b0bdaa25cf1d4 09 broken.

This is the place where the command line argument setting was encapsulated
into cmdline. That gave me big strife before too. Also boundary setting
was changed.

10 Nov 2012

Went into copticbackward. Deleted some files so that 
git checkout 627bf5af9ea6b1ea5858c65cacef01f6f734dee1
would work.
Then ./check is broken. This seems to contradict the above results.
That's very puzzling.
Cloned the home COPTIC to copticfix. 
If finds 627bf5af9ea6b1ea5858c65cacef01f6f734dee1 checks ok. 
copticbackward is somehow broken. Delete. 

copticforward is the broken version
copticfix is the unbroken version.

In copticforward, put aside coptic.f copy coptic.f from copticfix.
./check is unbroken. Hence the problem is in the changes to coptic.f.
These are almost entirely the encapsulation of the arguments.
Things become confused again in copticforward.
Do the same cloning process to get copticbreak version. It is broken.

copticbreak and copticfix give different solutions on make regardless
of the restart. break:0.875556827 fix:0.875226319. Must be some
effect of the reorganization.

Tried doing the initializations. Did not help.
Tried redoing the commandline interpretation in line. No improvement.
Put in initialization and inline cmdline. Comment cmdline call.
Fixes! Uncomment call. Unfixes. Moved the call till after the
initialization. Still broken. 
Ah! The difference is in crelax. With old version it is 0. With new
it is 0.5. Putting back to 0. gives old result. 
Taking the broken version, setting crelax to zero after the cmdline
call fixes both the difference in value and the restart check.
However, setting it explicitly to 0.5 causes the restart to break.
Non-zero crelax, which only happens by default in the new version is
the cause of restart errors. crelax multiplies phirein and averein in
cartreinject.

The following statements in cartreinject are both causes of check
errors when crelax is not zero.:

in avereinset
      averein=crelax*phi+(1.-crelax)*averein

in rhoinfcalc
         chi=max(crelax*(-phirein/Ti)+(1.-crelax)*chi,0.)

Currently averein is not stored. So avereinset is definitely going to
fail to set averein correctly, since it depends upon the prior value.
Averein is used in only one place in cartreinject: 
	x1=(x2+2.*max(0.,-averein))/x2
which is where the reinjection energy is scaled up by averein. It's
of dubious value. That checks out as the place where the averein 
error enters. Also chi is a saved variable not read back. So 
both of these averaging problems arise from non-writing of needed
information if crelax.ne.0.

Fixing this is awkward. averein is hardly used, but lives in reincom.
Really reincom ought not to be used by cartreinject because it is
some extra stuff in reincom.f left over for orbitinject. reincom.f
cannot be widely used because it has spurious stuff in it. The other
variable in reinextra is adeficit, also not used except in orbitinjnew.

Probably all cartreinject usage should be divorced from reincom.f and
hence from averein. Get rid of averein from cartreinject and use
a variable caverein instead. Also get rid from coptic.f. Put caverein
into partcom and write/read it. Then we solve the restart check for
the caverein. 

Do the same for chi: put in partcom, read and write. That fixes the
phirein problem. Done. Checks without error. 

15 Nov 2012

Upgrade diagexamine to enable averaging over transverse directions and
plotting the profile of the result in a particular axis direction.
Also upgrade reading of additional potential and density files, 
with the simultanous profiling of them.

23 Nov 2012
Allow a region to be _ex_cluded by a non-field-object in the object file.
This is to allow a point charge to be specified with a finite-radius 
region around it that does not specify potential boundaries but just
ensures that it acts as an absorber. This ought to enable us to have
finite sized small objects too small to be resolved by the mesh, but
with a specified charge, using the PPPM technique.

28 Nov 2012
Changed the handling of excluded finite grains represented by point charges.
If a node is outside the particle region, but inside a point-charge region
(which is the case if we have concentric point charge and finite sized
grain) then do not set its charge to compensate the electron density by
setting rho=faddu. That is problematic in such a situation because the
rhoci of the point charge gets included in faddu. Instead simply set the
(ion) charge to zero. 

6 Dec 2012
Idea about drift distribution reinjection for collisional cases with 
different velocity dependence of cross section. Riemann shows, that 
for (stone) cold neutrals, one can obtain a closed form solution of the
Boltzmann equation for constant cross-section, when the potential gradient
can be approximated as uniform. This is a good approximation for large
distances from the wall in a sheath, because we are into a regime where
there is only logarithmic dependence on distance. 

This works only because the transverse velocity dependence of the solution
disappears, because the transverse velocity is zero. In other words the
cold neutral approximation turns the problem into a perfectly 1-d problem.
Finite transverse temperature cannot be accommodated in that way because
any collisionality other than \nu=constant couples the transverse and 
longitudinal dependencies of f(v). However, if the neutrals are fairly
cold, T_n << v_d^2, then it will still be quite a good approximation to
take the longitudinal distribution to be the 1-d solution and just
add on the transverse Gaussian. This approximation ignores the coupling,
but that is justified if the T_n is small enough. [It's not clear whether
one can improve the approximation by getting a 1-d solution whose 
collision rate accounts for the transverse velocity at low longitudinal
velocity.] 

At any rate, it seems not unreasonable that such an approximation would
work rather well for low neutral temperature reinjection with more 
general collision cross-section form.

8 Dec 2012
Implement command line argument to specify the cartesian topology.
Fixed bug in dims_create.c of mpich2, so it finds the right distribution
of dimensions when all the factors are the same.

12 Dec 2012
There are required corrections to moveparticle and accelerate particle
for magnetized cases. It would be better to rationalize that whole
code region of padvnc.f

15 Dec 2012

Doing a big clean up with help from ftnchek. There is a major issue
that is a violation of fortran standard but probably does not make any
problem. It is in places where I pass e.g. objg into pllelplot. objg
is a particular object element from the array obj_geom, which is defined
in common objgeomcom. When objg is modified in such a routine, this is
a violation of aliasing rules in fortran, which state that if two dummy
arguments are aliased to one another, they may not be changed in the 
routine.

Working on this in objplot.f Fixed pllelplot. Had to change fluxdata.f
too. Fixed cubeplot. That fixes objplot.

Unfortunately we now have an error in the rhoinf on make. Which was
introduced just before
79fcaa3b74dd2580326d5c465e6086e9c5e08ff5
which was a lot of the rationalization of testing directories etc.
33dd358882930a31799b5e2debaef4069e241490
was the one before it.
The differences are enormous.
Our current version is a7e4ac405bf95938940c97e91e267f60ac7d92bc

Error appears when fluxdata.f is updated.
Found the error and corrected.
Commit.

Now remove nreloc from the pinit and padvnc because it is aliased to 
nrein inconsistently.

Remove partcom from reinject, and instead pass caverein from reinject
as an argument.

19 Dec 2012

Trying to understand forces in the collisional sheath case, I go back to
uniform plasma calculations. I have forces when the neutrals are drifting.
However, I am not in a position to do proper force calculations the same
as with sceptic when -vn0 in a uniform plasma, because that requires an
Eneutral. In coptic there is no automatic eneutral. I need to implement one.
In sceptic, Eneutral is added to accel(3). There's also something in fvinject.
And there's an issue with collisional force calculation. 

In sceptic collf=-colnwt*(vz-partsum*vd) which gives the collision force.
In coptic currently we have the following contribution in padvnc.f
c Inside object i->igeom which is a flux measuring object.
c The scalar drift velocity is in the z-direction.
               do id=1,ns_ndims
                  vid=0.
                  if(id.eq.ns_ndims)vid=vneutral
                  colnforce(id,i,nf_step)=colnforce(id,i,nf_step)+
     $                 (vid-xpart(ns_ndims+id))
               enddo
When neutrals drift, vneutral=vd, but when not, if there were an additional
Eneutral force, then it would have to be added on here. The normalization
takes place by calling bulknorm from coptic for all objects.

I need a way to specify to what extent the drift difference vd-vneutral
is driven by external E-field Eneutral, and to what extent it is driven
by internal Efield grad(phi). In the sheath calculations it is all internal.

If it were all driven by Eneutral, then the force calculation ought to
be vid=vd, not vid=vneutral. So if Eneutral=f*(vd-vneutral)/colntime,
then vid=(1-f)*vneutral+f*vd = vneutral -f*(vneutral-vd) = vneutral-
Eneutral*colntime. 

However, this is incomplete for obliquely magnetized cases. At the
moment vperp, the component of vd perpendicular to B, is interpreted
as being entirely due to external ExB (or equivalently to a Galilean
transformation).  If there's a difference between vd and vneutral with
collisions, then this has to be rethought. However, with B and perp
drift, the distribution function is much more complicated and requires
the full solver which is not yet implemented.

22 Dec 2012

Add facility to put command line arguments line into the object file.

Add an automatic target geometry/*.phi which runs tests to see if the
phi output is the same as expected. Added target geometry, which tests
all the geometry .phi targets.

Git Added the geometry/*.phi files. They then serve as the test cases.
Git push.

Still need to test the Eneutral code.

Testing the Eneutral code finds incorrect argument list with bulkforce.
Fixed. Then the Eneutral distributions look correct with partexamine.

Removed the necessity to have the number of dimensions on the first line
of the object file. That was a pointless relic of ambitions to make the
code variable dimensioned. It can't be sensibly done.

23 Dec 2012

We really need a particle initialization to drift distributions not just
to shifted Maxwellian, which is what is currently done. It does not seem
to be the case that we can just use the reinject routine, because that
excludes some faces under some conditions, and hence distorts the distribution 
that it represents. 

Probably particle initialization ought to be a part of the implementation of
a more general collisional reinjection solution based on integration of 
orbits and storing of representative particles.

Had a problem with git push. It was because I was in detached head state.
http://stackoverflow.com/questions/5772192/git-how-can-i-reconcile-detached-head-with-master-origin
explained how to fix that. Create a branch from current; force master to be it;
checkout master; delete temporary branch; git push.

Eventually got new version on loki. But some of the geometry tests fail.
(Some pass). Perhaps this is because of different architecture(?)

26 Dec 2012
Implement a collisional particle distribution calculation that is used
for pinit.

27 Dec 2012
Completed the implementation of the particle distribution. 
The idea is that this would be used for a sample-based reinjection
scheme too. Only for B-field free cases at present.

Tested using crein and demonstrated excellent quantitative agreement
with the old version (by turning on and off Eneutral).

28 Dec 2012

Implement collisional particle initialization, reinjection, and moving
for non-zero Bfield and non-zero Eneutral. This requires a change of 
meaning for vperp to be the velocity of the frame in which the total
background electric field is zero (both vdxB and Eneutral are cancelled).

Created analysis/partv2d.f to display the results of initialization etc.
It seems to show that the distributions are doing what is expected.

29 Dec 2012

Bulkforce in padvnc.f is not yet correct for magnetized cases.
I'm not entirely convinced about bulkforce implementation. At present
it is called for every particle the end of every step. But really
the bulkforce accounts for collisional loss of momentum from a region
whose momentum is being tracked. Why don't we literally count the 
momentum loss only for collided particles?

That could presumably be added to the postcollide routine. It would
probably be more accurate, and might reduce fluctuations. Might save
some time too. 

The problem is that the Eneutral effect needs to be subtracted from 
all particles, not just the ones that collide. If we don't do so,
then the momentum conservation will be violated. 

The bulknorm approach is problematic in that it is assuming the 
collision frequency to be the same for all particles. If we simply
subtracted off the Eneutral for all particles and removed the collision
effects in postcollide, then we'd solve this problem and not need
bulknorm. Also we'd need a simpler bulkforce and only when Eneutral!=0.
How do we conveniently compare two alternatives? Save old version.

Implemented a version in which the Eneutral is subtracted from colnforce
at the same place it is added to velocity. And postcollide is the place
where the collisional momentum loss is accounted. Not yet tested.

Done some testing to try to verify the new collisional code. Some
bugs killed. Others still present. Seems I need to do something with 
the rhoinfinity calculation. It doesn't seem to be consistent.
Yes there's substantial code that does not get called for the 
new collisional scheme that leaves the rhoinfcalc and ninjcalc
high and dry, I think. However, it's not so clear because the same
number of injections is called for as for the old code. Also there
are systematic potential differences. Looks like things are different.
Result is similar for the bulk case.
Actually I see there was an error in the Eneutral acceleration. 
Corrected and that problem goes away.

I think for constant collision frequency, the rhoinfcalc is still ok.
However, for other cross-sections it might not be. 

lbulk determines whether the old bulkforce implementation is used or the
new implementation. There's no noticeable difference at -ct100. But that's
because the collision force is a tiny fraction of the total. At -ct1.
the lbulk=.false. is still small, but the lbulk=.true. is big and gives
a dominant error. That means there's either a bug in the bulk treatment
or else it is far less accurate. 

I tried fixing by importing an older version of bulkforce, from before
I started modifying it.  Did not fix. Looks instead like a sign error
of the value of Eneutral added on. That just gets me back to where
I started but with sign reversed. I don't think I've really solved
the lbulk=.true. problems. But tiredness is setting in.

31 Dec 2012

lbulk=true means
In coptic
               call bulknorm(1./(rhoinf))
In padvnc
   Don't add Eadd to colnforce.
                     if(btest(iregion,igeom-1).and..not.lbulk)then
                     colnforce(ns_ndims,iob,nf_step)=colnforce(ns_ndims
     $                    ,iob,nf_step)+Eadd
   Call bulkforce
         if(lbulk)call bulkforce(x_part(1,i),iregion,dtaccel,vneutral
     $        ,Eneutral)
In postcollide
   Don't add dv to colnforce:
               colnforce(id,j,nf_step)=colnforce(id,j,nf_step)+dv(id)

This seems to indicate that I really do need the old version of bulkforce.
It gives far larger offset of force.
                  vid=0.
                  if(id.eq.ns_ndims)vid=vneutral-Eneutral*colntime
                  colnforce(id,i,nf_step)=colnforce(id,i,nf_step)+
     $                 (vid-xpart(ns_ndims+id))

Notice the sign difference of Eneutral. 
The colnforce is added to other things to give the net force on the 
object. That force is the net influx of momentum. A positive collision
force gives a positive net influx of momentum. The Eneutral is in the
direction that net momentum is being added to the ions. Therefore it
should appear positive in the colnforce. Looks as if the old version
has a sign error. 
Reverse the sign. That gives a drastic reduction in the colnforce, 
but not to quite as low as the non.lbulk. There's some systematic
growth of the colnforce as the collecting sphere increases.

Artificially enhancing Eneutral by 10% gives a big imbalance (14.2 cf 3)
reducing it by 2.5% removes the imbalance (but a bit too much).
Thus the levels we are seeing correspond to roughly 2% imbalance in the
collisional forces. The non-lbulk does not have such a subtraction issue.

Reducing dt seems to reduce the lbulk Eneutral error (maybe). 
lbulk looks ok at larger -ct e.g. 5. Worse at 0.5 

I think I see the difference. In the new nonbulk scheme I have multiplie
Eneutral by dtaccel and added dv. Those are consistent with each other,
but not with the normalization, which divides only by rhoinfty, not by
dtaccel. The force is the Eneutral or dv/dtaccel. So I am getting a
colnforce too small by a factor of dtaccel~0.1

So correct the colnforce to refer properly to force dv divided by dtaccel.
But that does not seem to be correct, because we are dividing by a lot 
of different dtaccels. Need to think harder. A collision is decided by
a random comparison of dtpos. Consequently, the number of momentum exchanges
by collisions is proportional to dtpos, and therefore the momentum transfer
by Eneutral ought probably to be obtained multiplying by dtpos. However,
that does not seem to be consistent. The transfer of momentum from Eneutral
that is actually obtained, is certainly Eneutral*dtaccel. In any full step,
we accelerate the particles by (dtpos+dtprec)/2 and advance the particles
by dtpos. dtprec is twice the acceleration time of the previous step that
needs to be applied to this step. If it's smaller than dt, that's because
we're considering the previous step to be shortened, e.g. by collision.

The problem seems to be that the acceleration step is the duration over
which Eneutral effects are being accounted, whereas the position step
is the duration over which actual collisional momentum exchange is 
accounted. I'm not sure whether we should keep the Eneutral*dtaccel
or change it to Eneutral*dtpos. Try division by dt. This looks extremely
promising. The variations in total force are less than those in particle
force. Looks as if this time ambiguity is not affecting things much when
we take the normalization to be by dt.

Get bulknorm out of coptic.f and into padvnc.
We now have a situation where the nonbulk gives very consistent-looking
results but lbulk gives suspicious results with small -ct. I suspect
but have not proved that the lbulk problem is associated with the 
smallness of dt compared with ct. 

The difference between the schemes is that lbulk makes the collisional
force equal to the sum over all particles in the measuring sphere at
the end of the step of vneutral + Eneutral*colntime - vparticle
(/colntime); whereas non-lbulk makes it the sum of the actual Eneutral
accelerations during the step minus the actual collisional momentum
changes from the postcollide routine (/dt). The latter advances the
particle self-consistently up to the collision position and velocity.

If we multiply Eneutral by 1.1 in non-bulk, then we get the same force
as before, more or less, even though the density drops by 10%, because
of the inconsistency. If, instead, we multiply Eneutral by 1.1 only 
in the colnforce contribution, not in the actual acceleration, then
we get up to 22 mismatch in the colnforce. Thus, with the non-bulk
scheme, it is really mismatch between actual acceleration and Eneutral
accelerations that causes spurious forces.

When vd=vn the two schemes give highly similar results.

------
Bottom line: non-lbulk is working very well. lbulk has significant
errors at low -ct that are probably caused by slight misaccounting for
the Eneutral momentum when the timestep is not much smaller than the
collision time. lbulk should not be used.

1 Jan 2013

Issues. Restarting and the new pinit may be broken.
pinit called before the restart code. Consequently, the initialization
of the reinjection scheme ought to be the same as the prior run.
If so, then perhaps this is not broken. I hope not.

Reinjection and En fraction ne 1. If we are running a case where the
drift distribution is partially driven by explicit phi-gradient and
partially by Eneutral, which is the case when the En fraction is 
non-zero but not unity, then there's an inconsistency.
I want the injection distribution to reflect the entire driving field
sufficient to give the drift velocity specified. But in the simulation
region, I want only part of the driving field to be En. 
During the colninit routine, Eneutral is adjusted to be what is really
needed to give the drift set if colpow ne 0. 
So, I haven't quite done the right thing. The right thing is that
Eneutral is the full driver in the reinjection calculation, but not
necessarily in the padvnc moving. Consequently, I need to maintain the
distinction between Eneutral and the fractional En.

The natural thing would be to make pinit the place where the Eneutral
is finalized. Trouble is that Eneutral is being used as the sign that
colinit is required. Stop using Eneutral in cartreinject. Instead use
if(colntime or copow) cartreinject else colreinject.

Removed Eneutral from coptic.f and cmdline.f (Enfrac instead).
Eneutral is set only in pinit.f. During particle initialization it
is the entire required Eneutral. Then it is multiplied by Enfrac
prior to return. 

Improved file arguments to accept multiple lines and use them all.

Installed velocity scaling of collision time into padvnc.

Fix a bug in the cmdline handling of the end of the Arguments parsing.

14 Feb 2013
Wrote a new colinit function based upon the above work, but incorporating
the Null Collision Method to give proper account of neutral velocity 
distributions.

Some level of testing done to show that things are reasonable, but not
tested very quantitatively yet.

25 Feb 
Corrected a bug in the new null-collision code with partially drifting
neutrals.



25 July 2013

Noticed in some sheathparticle runs that n_part seems to exceed ioc_part.
I don't know how that can be so. Investigate. Can be reproduced with 
the settings being used, regardless of restarts. Not caused by -da 
acceleration, or -ds subcycling. It IS caused by collisions.

For -ct1 and -dt.1 the n_part is greater by about 10%. That seems to say
that every collision is enhancing n_part artificially by 1. 

Found the error. npart was being advanced on partial collision steps.
Corrected.

Fortunately, n_part is not used for anything significant except
default initializations, at which time it is not incorrect. Therefore
there's no impact of this now-fixed error on any prior results.

30 July 2013

Another problem appears to be associated with sphere intersections.
This needs documenting.

When in padvnc we change region (cross a boundary)
call tallyexit(i,inewregion-iregion,ltlyerr)
This assigns the "exit" to a particular object and bin on that object
if it is a mapped object. This is done by objsect for each object
that has been crossed.
Objsect:
For spheres, calls 
         call spherefsect(npdim,x1,x2,iobj,ijbin,sd,fraction)
with
      do i=1,npdim
         x1(i)=x_part(i,j)-dt*x_part(i+3,j)
         x2(i)=x_part(i,j)
      enddo
so x1 is the previous position, and x2 is the current (approximately).
fraction is used by objsect only for intersection diagnostics, 
not for tallying.
ijbin, and sd are used for tallying only to ijbin. 

Spherefsect:
Returns ijbin,sd,fraction. It uses only the (fraction=) f1 return from:
      call sphereinterp(npdim,ida,xp1,xp2,
     $     obj_geom(ocenter,iobj),obj_geom(oradius,iobj),fraction
     $     ,f2,sd,C,D)
and sd refers to that intersection. There are always two such intersections.
But here there's an implicit assumption, that the second is not actually
reached. In other words that the second corresponds to a fraction f2>1. or <0.
That's not always true. If 0.<f2<1. then the particle passes right through
the object. If it's not absorbing, then two tallies are needed, but are 
not provided. 

Actions:
Calculate ijbin2 as the address of the bin of the second intersection
if >0, <1. or set ijbin2=-1
Add ijbin2 to the arguments of spherefsect.

This does not actually find any pass-through cases. I realize this is
because we don't test an object that we don't move from inside to outside,
which we don't if we pass through.

But we actually DO find some passthrough objects. Why? 
I think it is because taking the difference to get idiffreg is 
a logical mistake. I want to detect changes in individual bits of iregion.

This is not accomplished by an arithmetic difference. So I am not 
actually executing objsect on the right objects. tallyexit expects really
an integer containing bit flags each of which is set for an object crossed.
This is the bitwise xor of the two regions, not their difference. 
This either xor(i,) [Gnu extension] or ieor(i,j) [F95] The latter exists
in g77 also, so we should use it. 

Correcting makes a big difference.
Prevents passthroughs, as it should. Removes (most of) the apparent 
dependence of one object's results on presence or absence of others.
So this correction ensures that passthroughs are not counted for any
fluxes. But true transitions are.

Values of forces in the sheathparticle test case are promisingly low 
for empty spheres. Net forces observed for 50 steps of 2 processors
are less than 1/20 of the particle force, for -dt.2. Cancellation
is nearly complete. 

Perhaps we ought to include absorbing objects in the search regardless
of whether they show difference in region. At the moment we are allowing
particles to pass through solids in a single step. 

It's hard to tell how important this error was for previous results. 
Might have to do spot checks to find out. I certainly think that 
all the sheath cases should be redone. I'm not totally certain that
I've got rid of all bugs.

2 Aug 2013

Updated psumtoq to cancel _just_ the electron density when outside 
the particle region but inside a point-charge region. This ought then
to represent the region inside the grain-sphere that is just the 
shielded point-charge solution, with rhoci corrections but no electrons
or ions.

13 Aug 2013
Tidying in the testing directory.
git rm world3.h. Slicesect actually uses the accis/world3.h version.

-------

24 Aug 2013 

Preparing to make the direction of plasma drift general (like B).
Add variable vdrift(nplasdims) that is the direction cosines of the
drift velocity, relative to x,y,z. (Like Bfield).
Add code in cmdline interpretation to allow setting of vdrift(x,y,z)
and to default it to 0,0,1.
Add code to calculate vpar and vperp in cmdline. So that subsequently
(for nonzero Bt) vd(i)=vpar*Bfield(i)+vperp(i).

Now we need to address mostly pinit.f, cartreinject.f, padvnc.f. 
The order is not obvious. However, pinit.f is responsible for initializing
the fxvcol distribution that gives the reinjection statistics for a
drift case. So probably that's the place to start.

If colntime.ne.0 pinit calls colninit and colreinit, which are responsible
for calculating Eneutral. It appears that Eneutral is presumed to be in
the z-direction. That's got to change. But vperp is taken to be the velocity
of the frame of reference in which Eneutral (perp?) is zero.

Removed colninitoff which was obsolete.

Before we do more, we need a test case that will establish whether we
broke the prior collisional initialization. One way is with colndistshow.
Which plots the drift distribution. Add to that a write of the center
of the distribution so plotted, and run
./coptic -v1 -vn0. -Ef1. -ct100. Saved original as fvxvydist.init

Editted the unmagnetized parts of colnstats to reflect general flow
direction. (But those aren't the hard or necessary parts).

run ./coptic -v1 -vn0. -Ef1. -Bx1. -By1. -ct100.
save as fvxvydist.bxy
Editted the magnetized parts of colnstats. Still getting consistent results.
In so far as Eneutral/vpar/vperp is correct, this ought now to be correct.
The previous version seemed to presume that Eneutral was in the z direction.
The new version presumes it is in the vdrift direction. That's equivalent
once we reimplement vzave as the vdrift-component sum.

There is no "infinite"-B version of colnstats. I assume it is unnecessary.

Eneutral is set or iterated so that Eneutral=(vd-vneutral)/colntime,
before each colnstats call. However, it does not appear that vperp is
updated as Eneutral is iterated. This seems to be an omission.
Or maybe I don't quite understand. In cmdline, vperp is set to be the
drift velocity perpendicular to B. There is some description in Jan 2013
but I think my calculation is in notebook. 28 Dec 2012 refers to the change
in meaning of vperp. It is the frame in which both vdxB and Eneutral are
cancelled. 

So ... during colninit, colnstats, vperp is such as to "cancel only
vdxB".  That's what cmdline sets it to be. The perpendicular part of
vd.  After that's finished, but still in pinit, vperp is changed to
refer to the velocity of the frame that cancels (the perpendicular
part of) Eneutral as well. There does seem to be an inconsistency
during iteration of Eneutral, if it has a perp part.

27 Aug 13
Not quite finished with the magnetized case. I'm thinking that I'll perhaps
leave it when I've got the BGK case done. The other case seems harder.
Made some hand notes.

31 Aug 13

Rerunning sheathparticles cases. I want to understand the passthrough 
situation. I seem to be using updated coptic but still getting passthroughs
despite using ieor in tallyexit. I'm not sure I understand that.

Comment out colndistshow that I was using for the collisional distribution
stuff.

Get sheathgeom.dat from loki. It is simply one of the sheath cases.
Add the Arguments into the file. Now I can get passthroughs routinely.

Found the cause of the passthroughs. It was that during acceleration
dt is not equal to the actual step duration dtpos. Therefore in 
tallyexit and objsect, the prior position obtained by backing up by
dt is not correct. One ought to have backed up by dtpos. Therefore
dtpos needs to be passed to tallyexit and objsect. Fixed and it gets
rid of the passthroughs. 

Increased nf_maxsteps to 6000 from 3000.

Finalize magnetized case (I think) by changing the pinit section to 
add generalized EneutralxBt to vperp in the transition to the working
values (removing assumption that vdrift is in z direction). That means
it ought to work for constant-nu collisions (but not for colpow nonzero).
Not thoroughly tested.

Purge lbulk from padvnc and everywhere. 
Replace it in colncom.f with ldistshow. Add switch to toggle distshow.

5 Sep 13

Fix cmdline interpretation loop so that it works for Arguments in the 
object file even if there are zero arguments on the actual command line.

Change the limits of cijplot plotting to be the mesh limits conserving
aspect ratio.

9 Sep 13
Trying to run magnetized presheath cases. I find the vd has to be set to
a ridiculously low value to get the rhoinfinity to give n=1 at the upper
computational boundary. I believe this is because rhoinfty is not being
calculated correctly for magnetized distributions and general vdrift 
direction. 

In cartreinject, fonefac assumes that the drift velocity is in the
z-direction.  Now that is incorrect, and it needs to be fixed.
fonefac also takes no account of magnetic field. Probably it ought to
do so rather than just using the drift distribution in the z-direction,
as it does now.

The problem is that this requires a forward-flux calculation in which
the normal vector is other than the drift direction, which has no
known analytic solutions. In SCEPTIC and orbitinject it is accumulated
numerically. 

When there are no collisions, the background distribution is a drifting
Maxwellian which is equivalent to taking vn=vd. Such a distribution
can be integrated to get forward flux in non-drift direction. See hand
notes 12 Dec 2001. In effect, it is the same as obtained by just taking
the normal component of the drift velocity, and putting it into the
Maxwellian expression. 

However, in current rhoinfcalc the setting of a in response to
partperiod appears to be completely superceded by the calculations
done below it that perform the equivalent of this setting. The 
determination of absorbing or not is done within the H-setting code.
However, that's currenly only for dimension 3.

The obsolete code setting via maxwellian integral is not correct in 
fonefac because it does not account for absorbing surfaces. That's true
also for infinite Bt. But it is needed for the non-z surfaces.

Implemented code corrected for general vdrift direction, but only for
drifting Maxwellians. Designed to abort with message for cases that
would have the drift distribution. 

Fixed the cinjinit code, especially fonefac and the routines it passes
to cumprob so that shifted maxwellian distributions are correctly 
represented by reinjection. Collisional drift functions are not yet
implemented because they are not separable except in the direction of
the drift. Therefore for oblique drift, it is not straightforward
to obtain the distribution normal to a face. 

Desiderata: An output from diagexamine in vtk format of the velocity
vector able to be plotted by visit as streamlines etc. Implemented
14 Sep 2013. 

Some struggles with makefile that arise as a result of trying to
compile routinely on NERSC. I don't understand why on T400 make
returns from the initial recursive make, makes dummyreduce.o and then 
DOES NOT remake coptic, while on NERSC it DOES remake coptic. 

25 Sep 13
Corrected pinit to use the full drift velocity direction, not just
the z-component.

23 Oct 13 

Ideas for Te_gradient.
faddu is the place to change. Although it is passed as an argument to
sormpi, faddu is also called in psumtoq which is passed to mditerate.
Therefore it is not really possible to replace faddu as the name of the
called routine.

Implemented a cmdline argument for giving gp0 and gt the tempr gradient.
Need a test to warn if this is a sensible direction for gradient. 
Ought to be perp to Bfield.

Either we precalculate the electron temperature everywhere, the way 
we do for the point-charge data, or else we have to calculate it 
for every faddu call. The former sounds more efficient (provided memory
access does not dominate). It also involves a process that is just
like ucrhoset. Pretty easy to do. Might as well store the info in the
same common as rhoci.

Implemented Tec/i added to the ptch common and set via setadfield and
ucrho setting iteration.

Implemented modification to faddu to use Teci factor.

Seems to be working but not properly tested yet.

5 Nov 13

Problems seen with fluxexamine giving ridiculously large forces for the 
first step. These were run with -l0. Put a trap into pressureforce to
make it stop if the potential is too large. Is tripped on (some of) the
cases that give the error. Hence it appears that the problem is not with
reading back the data, it is with forming it in the first place. 
It appears that potentialatpoint is returning unphysical values.

Problem seems to be induced by potentialatpoint, which calls
      potentialatpoint=getpotential(u,cij,iLs,xff,iregion,2)
The parameter 2 tells getpotential to use fillinlin to fill in the
interpolation points that are missing from a cell. However, for -l0
cases, the initial potential is discontinuous at the edge of the 
object. Therefore doing linear extrapolation, as fillinlin does, is
very risky, and appears to be giving crazy answers. I don't understand
exactly why the answers are so crazy, but certainly the big potentials
are avoided if one instead does
      potentialatpoint=getpotential(u,cij,iLs,xff,iregion,1)
which attempts no correction for region, and just does a multilinear
fit anyway.

This change will alter values of pressure force even for finite lambda.
However, generally the force values have not previously been found to be
reliable immediately on the region boundary. So there is probably not much
loss in making this change universally.

8 Nov 13

Planning for implementing nonuniform ion parameters.  The hard part is
the reinjection scheme. This is controlled by the parameters in
creincom.f: hrein,grein,prein,gintrein,idrein.

For density gradient, if we ignore any alteration in the distribution function
arising from the gradient, then it seems that we simply need to weight the
position choice so as to reflect the density gradient. Assuming the gradient is
constant, the weighting for a face is determined just by the centroid of the
face, and position on the face can be simply determined by the displacement
of the centroid in the gradient direction. 

The change of the distribution function due to density gradient is, to lowest
order, a shift equal to the diamagnetic drift. We could either (1) consider
that shift to be already incorporated in the drift velocity, in which case
we would need to adjust the Edrift for ExB. or (2) Add the diamagnetic 
drift to the ExB flow velocity specified from the commandline switch during
the calculation of the probability distributions. If we choose (2) then 
perhaps an extension to a higher order correction would be easier to 
incorporate. For that reason it might be a better long term choice.

Thus we need to make changes to ffdrein, the distribution function
from which the cumprob is found, and fvdrein for the transverse velocity.
These functions already depend upon creincom, plascom, and colncom.
Therefore it does not seem very difficult to do this change.

18 Nov 13

Implemented density gradient switch in cmdline.f. Data in plascom.f.
Also implemented checks to ensure that there is no density gradient
that is not perpendicular to a magnetic field.

Implemented vdia calculation and correction in ffdrein and fvdrein.

Now to re-weight the faces, all we seem to need to do is adjust ginfty=grein.

Actually there are alternative places to implement the flux correction:
fcarea, grein, fonefac. 
Given that the central position of density measurement may not be in the
center of the domain, there needs to be correction in the vicinity of 
fonefac, which governs the ninfty calculation. But it could be fcarea. 
No it couldn't because that refers only to three dimensions not to 6
faces. It may not be logical to use fonefac itself, because that is dealing
purely with the velocity distribution effects. It is complicated by the
magnetic field (especially infinite field) cases. Actually fonefac uses
0.,.5,1. to specify whether faces have input or not. But it is only 
3 dimensions, not 6 faces.

fonefac is called only in rhoinftycalc and ninjcalc. 
It has the Bt dependence, but cinjinit also does.
grein constructs gintrein and gtot. gintrein is used for deciding face.
cinjinit accommodates absorbing faces by setting fcarea to small.

There are corrections to be considered for infinite Bt.
First the values of fonefac and grein are consistent for non-infinite Bt
while they are inconsistent for infinite. This proves to be an
improper scaling of ff1crein. Fixed.
Maybe fv1crein also needs fixing. Applied the same fix.
fv1 and ff1 do not include vdia. I don't think it makes sense to have
a diamagnetic drift when B is infinite.

Now for all B we are getting consistency between fonefac and the sum
of greins for the faces, provided that fonefac is not zero. 
Particle absorption/periodicity seems still ok. Therefore I think we
now have all the fonefac information coded into grein. Periodicity 
is a bit puzzling. Why are the grein values and fonefac non-zero?
It should not matter in rhoinfcalc because the fcarea is set to small.

Got rid of fonefac calls by using grein. This gives tiny changes to the
ninfinity calculations because of rounding.
We therefore fail the geometry tests etc. We can update the stored
phi files by simply deleting them (I think). But that would be a major
step in terms of code check evolution. [Did it anyway below.]

Now that we've got everything in grein, we can do the density gradient
implementation through that. Implemented grein adjustments. 
That means we now choose faces appropriately. However, we have not
adjusted the choice of position within the face. Nor have we checked
whether the corners lead to negative density.

I realize at this point that the constant gradient choice is highly
inconvenient. Choice within a face is not separable under that condition,
but it is if we take constant gradient scale-length. Constant scale-length
is not substantially more difficult for forming the average density across
a face than is constant gradient. Therefore constant scale-length is 
clearly the better choice. I change the code to adopt that, and then
the choice within face implementation is easy.

Now I think I have the implementation completed and working. 
Fonefac is totally disabled.
It would be good to do some tests e.g. using injection diagnostics.

rm geometry/*.phi
for file in geometry/*.dat; do make ${file%dat}phi; done
make geometry
renews the geometry test files. Then subsequent make geometry tests are
passed.

Commit.

20 Nov 2013

Now we need to compensate faddu for the density gradient. faddu is the
effective electron density (and gives its gradient wrt u) as a function
of u (and position). It is currently n_\infty exp(u). We want it instead
to return n(x) exp(u). Where n(x) is n_0 exp((x-x0).gn). 
This can be accomplished using the storage uci, which is added to u before
it is divided by Te(local)=Teci and the exponential is taken. If we make 
uci equal to (x-x0).gn.Teci that will do the trick. Actually we need to
add this amount to uci if there are point charges that also contribute
to it. Currently, uci and Teci are set in padvnc.f routine ucrhoset ...

Implemented that. And it seems to be being set from the setup diagnostics.

Now there is a question about particle initialization with a density 
gradient. This ought not to be too difficult because it is similar to
the choice of reinjection position which was implemented by the following
change in cartreinject:
c         fr=ran1(0)*0.999999+.0000005
c Or Accounting for density gradients:
         fr=ranlenposition(iother)
In fact, since ranlenposition automatically initializes itself, it
seems we just need to replace the pinit position decision with this.

Implemented the change. Unfortunately, since ranlenposition contains
code to prevent getting exactly equal to 0., or 1., there are tiny 
changes (again) in the solution for uniform density.
Recreated the .phi files again.

21 Nov 13

After a detour because of being misled by partperiod, I find that the
density gradient does not seem to work in an empty box. A potential
gradient appears which should not. I don't see why.
The potential is higher on the side where the density is higher.

Ah, faddu is checking gtt_copy to see if we need to apply the
corrections, but I'm not setting it. Make gnt_copy and check it
too. That fixes it.  There is some bouncing aroung at the 10^{-2}
level but no obvious potential bias arises from a density gradient.
I think density gradients are working.

Renew geometry test files.

23 Nov 13

Remove iLs from bdyshare arguments and from being used.  Then iLs is
passed to bbdy and set there, but is not passed or used by bdyshare
routines. It is used internally by bdyset routines. It is also used in
sorrelaxgen and is the only way that it knows about the structure.
Therefore it is indeed needed in sormpi, but not now for bdyshare.

Now, therefore, bbdydecl.f which is included only in sormpi and bdyshare
works sensibly. In sormpi, ndimsdecl is a parameter and causes allocation
of the variables that are not passed as arguments. It checks to make sure
the ndims passed is not larger than the internal ndimsdecl parameter. 
In bdyshare ndimsdecl is an argument, and all the variables whose length
is defined by it are passed as arguments.

The result is that in principle, sormpi can be called with any dimension
le 3. But of course, the bdyshare routine must do the right things.
bdyset is also (possibly) called by sormpi. It's dependence on iLs is
illusory. It arises in the mditerate calls. No iLs is passed from 
sormpi to bdyset.

26 Nov 13

    Implement setdimens, a script to set allocated dimensions.
    
    Also modify some places where griddecl was not needed.
    It's not easy to removed griddecl completely because there
    are places in padvnc and the examine code where it is needed.
    Therefore setdimens can be run to set it to the defaults or to
    dimensions obtained from copticgeom.dat or the script command line.

30 Nov 13

Fixed problems with small grids. One was the usual problem of the special
particle. Fixed the test to ensure that is really in the region.
The other was the parity changing in sorrelaxgen, which previously 
assumed the grid was even. Fixed that. Now grids of length 3 which
are effectively two-dimensional, will run.


4 Dec 13

Running thin cases with large numbers of points in the other
directions overruns Lobjmax. This appears to be because many the
points are being given cij pointer entries. This is because all
boundary points are given entries. Researched back in the notes to see
where this was set. Dec 09.  Mesh Domain Boundary Pointers. The region
is set to -1 in order to prevent field interpolation from using the
edge points as a center from which to do field interpolation. A point
that is the center of interpolation reaches either side of itself, and
hence beyond the actual mesh, which is an error. If points are set as
outside the region, they are not used as a center of interpolation.

I think this need arises because of the choice to make the particle domain
edge equal to the last mesh nodes (except for periodic particles). Since
periodic particle only go to the half-way point between the last two 
mesh nodes, they ought never to choose by preference an edge node as 
a center of interpolation. Perhaps if I had made this particle domain
choice for all cases, the edge setting might be able to be avoided.
The mesh-edge-adjacent points are still in the particle region.
The question is whether their cij pointers are being set because of the
mesh-edge region or not. It does not seem that their pointers need to be
set. And in fact cijedge is iterated _after_ cijroutine has been interated.
Therefore the cijedge, which sets the special edge region, is not set when
the pointers are set for the adjacent points. So their cijpointers ought
to remain unset. Consequently there ought not to be additional cost for
doing field calculations for adjacent-node centered interpolations. 
If particles are periodic, therefore, there ought to be no significant
slow down arising from the fact that the edge nodes have pointers set.

Dec 5

Realized that the way geometry tests are done is too data intensive.
Every time the resulting phi files are changed, a lot of changes must
be put into the git repo. Therefore changed the testing to save a checksum
generated by sum, and compare whether that has changed.

Fixed bug in cylplot where ism1,2 were being initialized to 0 not 1.

Dec 9

Changed griddecl to include a parameter na_m2 that is the second largest
grid dimension. This allows us to specify workspace that is not excessive
in cases where one of the dimensions is much larger than all the others.

Dec 10

Merging of Chuteng's creation of vtkwriting for fluxexamine.
git branch fluxvtk
[tp400 coptic]$ git pull --no-commit hutch@loki:/home/ctzhou/COPTIC/.git/
remote: Counting objects: 93, done.
remote: Compressing objects: 100% (63/63), done.
remote: Total 72 (delta 47), reused 13 (delta 8)
Unpacking objects: 100% (72/72), done.
From loki:/home/ctzhou/COPTIC/
 * branch            HEAD       -> FETCH_HEAD
Automatic merge went well; stopped before committing as requested
[tp400 coptic]$ 
However, there's an error in the commit. Chuteng edits and commits.
I can't repeat the pull over the MERGE_HEAD.
Have to    git merge --abort  then can repeat
Remove some plotting commands from sphereplot to prevent additional
complexity. Then only faceplot has links to vtkwriting and vtkcom.f

========================================================================
Summary for Calendar Year 2013 of code developments

Fixes to Eneutral and Enfraction.
Improved file Arguments.
Velocity scaling of collision time.
Null-collisions colinit code.
Fixed bug from collisional increment of npart.
Fixed bug associated with arithmetic rather than xor of idiffreg.
Made the drift direction arbitrary.
Fixed bug to do with dtpos not being used to back up when tallying.
Updates to the makefile to work on NERSC.
Implement Te gradient.
Implement ne/i gradient.
Major tidy up of cartreinject associated with gradients.
Implement setdimens script.
Fixed small grid error in red-black iteration so it works for grids
as short as 3. (One point plus two boundaries).
========================================================================

11 Jan 14

Discovered a problem with the reorganization of meshcom.f to include griddecl
It is that in analysis (or any other directory) we can't include meshcom.f
unless griddecl.f is also in this directory, because when including meshcom
griddecl.f can't otherwise be found. meshcom.f includes griddecl only
after 2 Dec 13.

Adjust distread in partaccum.f not to require the individual nsubs to be
correct, only that there's enough total space allocated. Also make
more consistent so there are no assumed number of dimensions.

4 Mar 14

Replaced the random number integrator in volintegrate with a Sobol number
generator toms659. It is faster even for the same number of points used. 

24 Mar 14 implemented uniform v-binning when nsbins=nptdiag.

25 Mar 14
Implemented ndimsdecl.f for defining the # of dimensions.
Got rid of ndims_cij entirely replacing with ndimsdecl.
There was one gotya. That was with iterators that are assuming the
dimensions to be passed, but then include objcom which needs ndims.
Made those arguments into dummy.
Got rid of nc_ndims
Got rid of ndims_face
Got rid of ndims_grid

Working on ndims_mesh many many places where meshcom.f has to be 
preceded by ndimsdecl.f. Reached the point where 
      integer ndims_mesh
      parameter (ndims_mesh=ndims)
seems to work. Now we need to get rid of all ndims_mesh references.

Commit for safety.
Do universal change of ndims_mesh. Seems to work.
Got rid of ndims_mesh

Got rid of sc_ndims. Now I think only pp_ndims,ns_ndims, and nf_ndims remain.
These are in 3dcom.f
Included all the needed ndimsdecl.f before 3dcom.f. Compiles.
Now I think all the dependences on these _ndims can be changed to ndims.
Commit.

Do the universal replaces of these. OK. 
Got rid of pp_ndims, ns_ndims, nf_ndims

Now we have rationalized the ndims to the point where pretty much everything
depends on ndimsdecl.f. However, in quite a lot of routines there is an 
argument passed that has been renamed mdims and more or less ignored.
That redundancy ought to be removed or somehow rationalized/tested.

Actually bbdydecl.f and sormpi.f have ndimsdecl in it. Drat. Not yet complete.
Maybe complete but bad name clash. Changed name to ndimsbbdy in sormpi etc.

mdims rationalization not yet done. But that's enough for now.

26 Mar 14
There were some differences detected in cks. I don't know why.
Went back in and found that there were cks errors after the previous
update of cks. In fact the current cks are different from old ones
only for geom....dat files not for uniform, ... Therefore I am not going
to worry too much about it. I think we are probably ok, and the cks
files are updated as of 
commit 8af483247503a6880d8eb0ac799a72d9c5ca7b48

In mditerate.f there are no references to ndimsdecl.f the mdims is used
for local definitions of working arrays and is set to 3. 

Trying to get rid of ifull(3) references. Some of these might work if
ifull and iuds have at least 3 dimensions and the trailing dimension
length value is set to 1.
average.f replace the 3-D only routine with general.
checkcode.f implemented trap to skip. Needs to have array dims fixed.
cijplot.f   implemented trap to skip. But compile should fail.
cijroutine.f  warn in text3rgraph, but should work. 
partaccum.f   trap to skip pltsubdist, but might work.
partwriteread.f Ought to work for ndims<3 if ifull and iuds are 1.
phisoluplot.f  spherecheck can't work for iuds<3, but might not crash.
	       slicesolu calls slicing, might work.
	       gradradial calls getfield etc. Probably won't work.
	       fixedline may work.
slicesect.f    slice3web. Probably won't work.
volint.f       stored3geom. Probably will work.

By test inspection, these seem to be the only places left where the 
ifull is explicitly taken to be a 3-D array, and referenced that way.

There may well be reason to consider defining two versions of ndims.
One would always be 3 to give the maximum dimensions allowed. 
The other would be the actual number of active dimensions in this
compile. The above things that might work require there to be length
three vectors like ifull, iuds etc. They would use ndimsmax.

Actually the coptic main program already is assuming the length of
ifull because of its data statements.

Implemented ndimsmax in ndimsdecl.f so as to be able to test whether
things will compile with ndims set to 2. 

Try ndims=2. This triggers a number of compile errors.
Change the offending array definitions to ndimsmax.
Then finally we can compile, but there are various errors.
Progress till now is that we have errors in getlocal that seem to arise
from the way cij has been initialized. No time to go further at the moment.
To go back to 2d tests just set ndims=2 in ndimsdecl.f.

19 Apr 14
Remove ndims from the argument list of 
       chargetomesh
       psumperiod
       diagperiod
Remove meshcom also from these. It was there only for declarations
that are now in ndimsdecl (I think).
make geometry shows no differences.

Make the charge assignment for each particle a separate routine.
There might be slightly extra cost, but it is better structure.

Remove all references to npdim, the dimension parameter in partcom
from everything outside partcom.
It is tricky to convert partcom to using ndims because the reduce
routines have ndims as an argument and use partcom.
I see no reason why they need partcom. And in fact it makes no difference
if partcom inclusion is commented out.

There's a problem with psumtoq. It uses partcom for rhoinf. That's a
very bad use, but how does psumtoq get it otherwise? It is already using
four arguments. One fix would be to increase the number of arguments
in mditerarg to 5. That ought not to break anything else. 
Did that and had to add some extra dum5 arguments to silence warnings.
make geometry ok.

Added ndimsdecl.f a few places. Then we can compile with no dependence
on npdim from partcom.f. We've completely removed npdim in effect.

Make branch ionsonly

Need to fix direct dependence of 
partlocate postcollide tallyexit
on the integer i particle list position. Change the call to be to xi.
Seems to work.
I think we now have padvnc to the state where it only passes x_part(1,i)
to other called routines, not i itself. The point being that then the
called routines can be applied to any type of particle, presumably.

Get rid of if_part by incorporating it into x_part as x_part(iflag,i).
Needs modifications to pinit.f partwriteread.f, checkcode.f, padvnc.f,
partaccum.f.

Seems to complete ok. make geometry shows no differences when complete.

Remove also dtprec replacing it with x_part(idtp,i). Done.
Remove iregion_part, which has long been obsolete.
Add iic_part the start of the particle list. We are not using it yet
but the idea is that for multiple particle species it might become the
bottom of each stack in a long list. But we are a long way from being
able to implement that yet. There are many many places where it is
assumed that n_part, ioc_part, are scalars, not vectors. 

The idea I am working towards is this. Electrons need a time step far
shorter than the ions in order to resolve their orbits. This is of
course the big problem with full PIC. If one has to update the
potential and ions at each electron step, the work is far
larger. However, suppose we don't actually care about potential
fluctuations at the electron plasma timescale. We don't then have to
update the potential except when the ions are moved. Also, we don't
have to move the ions with timesteps nearly as short as the
electrons. So one could imagine moving the electrons fast enough to
resolve their orbits: a timestep far shorter than the ion timestep on
which one moves the ions and updates the potential. This won't
significantly help manage the cpu cost if there are as many electrons
as ions. Essentially all the effort will be in moving electrons,
because they move far more often. However, if one had fewer electrons,
by approximately the ratio of electron to ion timestep, then electron
cost would be reduced to comparable to ions. One would then worry that
the electron noise would become far larger than the ion noise. But
there's a way to prevent that. It is to deposit the electron charge at
every electron step. Then the number of electron depositions will be
as large as the number of ion depositions, and the electron noise will
be only comparable to the ion noise. The result will be an effective
smearing out of the electron charge over the orbit an electron makes
during one ion time-step. Physically, this is just what you want if
you wish to average over and suppress electron-generated potential
fluctuations. What I'm not quite sure about is whether this whole 
approach is stable. One might worry that failing to update the electric
potential gives a sort of over-explicit scheme that is unstable.
Insofar as the electron solution gives essentially a Boltzmann result,
the current iterative scheme is implemented in a linearly-implicit 
sense relative to Boltzmann electrons, and is very stable. I think there
is therefore reason to think that a stable scheme might be the outcome.

It seems likely that the most effective way to implement this scheme is
to combine the charge deposition with the particle step. At the moment,
COPTIC separates them completely doing chargetomesh in a different 
routine. I don't see why that is necessary. I hope it isn't.
However, in the first instance it may well be wise to combine only 
the new electron advancing and deposition.

21 Apr 14
Remove mdims from padvnc arguments.
Remove mdims from getadfield, setadfield argumenst
Also ucrhoset arguments, but that was a mistake corrected below.

A question arises in respect of documenting the density and other moments
of multiple species. If we directly add the electron density to psum, then
it is no longer the ion density. Instead it is the charge density. 

We have a mechanism for documenting various moments (including the 0th)
of a distribution as a function of mesh position. It is diagsum. 
Probably that's the right way to track the density (etc) of multiple
species. Currently diagsum is (x,y,z,ndiagmax) with ndiagmax=7.
It would be fairly straightforward to simply (extend ndiagmax and) add
on density and other diagnostics for other species at the end of it.
Then if one is using multiple species, one must use the diagsum instead
of psum for (even the ion) density. In effect, then psum becomes a
charge-sum rather than a particle sum. 

What then needs to be done is to call achargetomesh passing the address
of the start of the particular species' diagnostic storage.

Another question arises about ndiags. If it is the number of diagnostics
per species, we need to adjust diagperiod diagreduce commands etc. Looks
easy.

Add ndiags,psum,diagsum to padvnc argument list.

Implemented an alternative charge deposition that happens in padvnc.
It gives identical make results. 

No differences in make geometry till geomforce.dat. Then a floating exception.
Back up. It's not the alternative charge deposition that does it.
Must be something in the argument adjustments. Yes I made a mistake with
ucrhoset and mditerate arguments. Put that back to the way it was.
Ok. That seems to fix it.

Try timing make geometry. 
New version 
real    1m6.177s
user    1m3.044s
sys     0m2.796s
Old version
real    1m5.158s
user    1m2.072s
sys     0m2.720s
Hardly significant.

We now have an ion version of coptic running with charge deposition
being done (only) in the particle advance. Commit.


It seems as if the easiest way to implement multiple-stepping is to
install a secondary step loop just inside the particle loop, so the
particles that are being multi-stepped are just done that way and 
everything else is tracked as before. There needs to be a way to 
adjust the tracking/deposition weights for different types of particles.
Also there needs to be selection of particle iteration range limits etc.

Implemented array definitions of n_part,ioc_part,iic_part,ninjcomp
with equivalences to the first element to preserve prior programming.
Works without difference. Implement iic_part as the starting slot
of the particle padvnc iteration. Initialize iic_part in pinit. OK
but iicparta(2) is defaulted correctly only if nspeciesmax is 1. 

Replace ioc_part with iocparta(ispecies) in padvnc.
Replace n_part with nparta(ispecies)
Replace iic_part with iicparta(ispecies).
Replace ninjcomp with ninjcomp1(ispecies) in padvnc.
Replace n_partmax iteration with iicparta(nspeciesmax+1) and set
it to n_partmax in pinit by default. 
Now we have a general particle slot range iteration governed by 
ispecies. Although ispecies is always 1 at the moment.

Other effective outputs from padvnc:
fcollided -> colncom.f
ncollided -> colncom.f
nsubc -> colncom.f
phirein -> partcom.f
nrein -> partcom.f
iocparta(ispecies)
nparta(ispecies)
postcollide call. Increments colnforce -> 3dcom.f
caverein set right at start of padvnc from phirein.
nlost not output
tallyexit call. Calls objsect which adds intersection object's fluxes.
  There's no way currently to add different species fluxes.

For now it seems reasonable to decide to set n/fcollided, nsubc,
phirein, and postcollide only for the first species: ions.
Do this by adding ispecies.eq.1 to the collision tests.
phirein is prevented from doing anything by calling cavereinset
only when ispecies.eq.1. nrein is not adjusted because it is
used to tell whether we have reinjected enough for this species.
There's some danger with that in the diagnostics that print it.
tallyexit only when ispecies.eq.1. That should fix everything.

commit.

Now we need a way of activating new species. This requires
modifications to cmdline.f. Seems that we could have a switch -sp that
increments the nspecies, and defines eoverm of the nspecies
species. At the moment, the temperature of that species is simply 1,
electrons, and we can probably ignore the possibility of a different
drift for now. It seems reasonable that we would simply take the
density of the second species to be 1/sqrt(eoverm) of the
ions. However, it doesn't seem right to constrain both electron and
ion species to a fixed number because that would cause a fixed
possibly non-zero charge in the region. Probably therefore, electrons
should always be treated as having a fixed injection rate. Also the
number of reinjections per timestep should be scaled appropriately.
In summary, it is not obvious that anything about the electron
injection needs to be explicitly set in the cmdline.f code. Just
nspecies.

Later we might wish to set other species where Ts, vs, etc are input.
We also need to decide the whole reinjection process. It seems likely
that the best long-term approach is to generalize the existing
reinjection routines to accommodate a different mass. By the way we
already have rmtoz, the ratio of mass to charge of the ions, so we need
to be somewhat careful with eoverm. rmtoz is by default unity which 
refers to hydrogen. The only active place rmtoz is used currently is
in cijroutine for determining floating potential. Floating is another
challenge that I defer.

The difficulty is that creincom currently holds a lot of precalculated
data for the injection. It would have to be duplicated for multiple
species. The total volume of that data is ncrein*9 with ncrein=1000
so it is not too bulky. However, there's considerable coding that will
have to be redone. It is only in cartreinject.f.

Remove all nplasdims references and replace by reference to
ndimsdecl.f.

commit.

Remove references to mdims as a parameter from ptaccom.f definitions.
There remains the need to replace mdims as a variable reference in
various places in partaccum.f. Replaced with ndimsmax in the places it
is a reference to common. There are probably many places where the
reference ought to be to ndims. But haven't fixed those yet.  (Others are
arguments). Remove mdims entirely from ptaccom.f

Remove iasdims and replace with ndimsmax in mditcom.f 

Now the only *com.f file that has variables containing 'dim'
where it is not 'ndims' is 3dcom.f with nf_posdim and nf_dimlens.

Introduced array forms:
      real Ts(nspeciesmax),vds(nspeciesmax),rmtozs(nspeciesmax)
      real vpars(nspeciesmax)
      real vperps(ndims,nspeciesmax),vdrifts(ndims,nspeciesmax)

in plascom.f Equivalence cannot be used for dummy arguments.
Therefore just had to go through the entire routine and transform
the references into array with extra dimension. When completed
make geometry works. Add nspecies and nspeciesmax to the argument
list. That's both the actual species we are working on currently in 
cmdline and also the final number of species when cmdline finally returns.

Got things working up to a point with nspeciesmax=2 but not actually
initializing or moving the second species. Just setting its values.

commit

Changed ninjcalc to refer to nparta etc in cartreinject.f
One idea is to leave nrein the same for all species, and scale by
the number of substeps per step. But this only works for electrons.

There's a problem with Tneutral, a scalar. I don't think it is really
ever used as something different from Ti. It would be better, I think
if I just got rid of it. Replace with Ti in cartreinject.f, padvnc.f
pinit.f Everything else is just writing an reading. Leave and ignore.

vneutral is used in cartreinject to determine the extent to which the
distribution is maxwellian or not based on the different
vd-vneutral. One way to fix this without introducing multiple
vneutrals is to set vd-vneutral to zero for ispecies.ne.1 That says
only the first species can have velocity driven by electric field. The
others not. Implemented. Passed ispecies in local common within
cartreinject. We need to do the intialization cinjinit over all active
nspecies, and make the creincom contain multiple interpolation data.
Do the usual equivalencing of unarray to array. make geometry. Ok.
cinjinit seems fine.

Now cartreinject. Replace scalar rein references with vector (not
idrein). That may have got the reinjection to the point where it
is ready for more than one species. We still have rhoinfcalc, but 
it is not clear that it will matter. Leave it for now.

commit

We still have not got different masses implemented. It's a bit
annoying that we have the mass rmtoz rather than the inverse of the mass.
The places where mass makes a difference is in padvnc acceleration, in 
particle initialization, and in reinjection. (We haven't tackled flux
accumulation yet.)

Change rmtoz into eoverm everywhere in codebase.
Correct cartreinject for eoverms (except in rhoinfcalc).
Working on pinit. Part way done. Have to decide on nparta(ispecies)
if quasineutrality does not decide it. Presumably decides only the
topmost species nspecies, once the numbers of the others are decided.
We currently have ninjcomp set the same for each species if we are doing
-ri, but -ni is not set up to work properly, because for ispecies>1
we have to decide on the injection number independent of ispecies=1.
Also corrections to ninjcalc from caverein are of the wrong sign for
electrons. Basically ninjcalc is not correct for higher species.

24 Apr 14

Change eoverms to represent the sign of the charge as well as mass.
Make the reference abs(eoverms()) whereever it is governing a thermal
velocity width. 

The ripernode for higher ispecies, if governed by quasineutrality, when
-ni is set for ions, is equal to nion/volume. So do
            if(ispecies.gt.1)ripn=nparta(1)/volume
in ninjcalc. Now I think ninjcalc is correct for electrons.

Add ispecies to arguments of padvnc. make geometry OK.

commit

Work on padvnc to introduce the eoverm dependences. Moveparticle is handled
simply by passing it a Bfield that is eoverm times larger.

Seem to have got it going. Using padvc prior to the advance loop to do
the initial deposition of electrons. The electrons appear to add a negative
value of unity to the initial density, as expected. (Have not turned off
the Boltzmann electrons yet.) Thereafter the density is near unity because
we don't call the second species in the loop.

Implement diagsum for all the species, and adjust the references to 
accommodate that within coptic.f and padvnc.f. We need also to write out
diagnostic files for more species. Change that. Fix partexamine for
mdims->ndimsmax dependence. Made the writing of the species 2 diagnostics
work after a struggle with ndiagmax+1.

25 Apr 14

Implemented all the processes for working without faddu. Actually use
fnodensity, but probably it is not substantially different from 
just turning off faddu. 

Implement numratioa the ratio of species 1 to species ispecies number
of particles. In addition to defining the expected amount of space
needed for the particles (\propto 1/numratio) it defines the number of
minor steps taken per ion step. Consequently, the expected number
times the number of minor steps is equal to ni. When species 2 is
electrons, as it is by default, this means the set numbers of initialized
particles and reinjected particles for species 2 are such as to neutralize
the ions. The default numratioa(2)=sqrt(abs(eoverma(2)))=sqrt(1836).
It may be adjusted, after -sp, by the -nr argument. When -nr1 is used
e.g. ./coptic -dt.01 -sp -nr1, then the electrons are the same in number
as the ions and their step is the same as the ion step (no substeps). 
In that case we are running a full electrons/ions pic code. It runs
at approximately the same step rate, but one must choose the -dt to 
be small enough for stability and electron dynamics accuracy. 
That amounts to a requirement that dt < lambdadebye/sqrt(eoverm) which 
is (for electrons) ~.025*lambdadebye. The experimental boundary for lambda=1
for long wavelength (total particle number) instability on the standard
coptigeom is -dt.05. (For smaller lambdadebye it seems smaller than this 
scaling.)

It seems as if coptic is now running with particle electrons.

commit

mpiexec -n 2 is not obviously broken. It works with multiple processes and
might not be incorrect.

Did some RefManual updating.

Did some checking using gfortran corrected a bug and some warnings.
Do particle slot check in pinit to prevent overflow of slots.

2 May 14

Make the default numratio -nr equal to 1, not 1836. Fix RefManual.
Fix segfault caused by incorrect slicegweb workspace zp.
Run some other tests

Scalar, with electrons: 0m34.998s without 0m13.637s. Worse than a
factor of 2 for same timestep. Probably because of noise and poisson time.

Add geometry/uniformelec.cks and .dat for testing electron case.

3 May 14
Chris reports errors with Bt turned on and electrons. I think we should
first try to get drift electrons going. This is bound up with Btinf.
However, at the moment, when Bt exceeds Btinf, the perpendicular 
motion is taken to be only the set perpendicular drift, not including
the ExB that arises from the local E. That's in a sense consistent, but
I think not what we really want.

Another issue is that the mass is not properly accounted for in Bt effects.
Actually it is handled by passing Bt*eoverms(ispecies). Fixed that in all
calls.

It would also be rather useful to be able to examine electron orbits as
we can for ion orbits. Added array index nspeciesmax to xorbit,yorbit...
Added do over species into orbit3plot. Generalized the padvnc code.
Seems to be working. Multiple species orbits are plotted.

Implement automatic fallback to drift orbits. Test is whether the
angle theta in the moveparticle is .gt.1 If so, drift setting the
perpendicular velocity equal to the drift velocity only. This will
almost always be tripped for electrons at a moderate field or above.

Implemented vperp plus ExB drift in both the strong and infinite case of
moving. We ought therefore to have reliable orbit integration. 

Added magnetic field to uniformelec.dat

6 May 2014 Restart

./check is ok
./check50 is ok.
./checksp gives differences. This is a new test with electrons.

To restart electrons requires us to write them all, and also to write
nspecies, the arrays such as iocparta, and so on. This is going to be
a substantial change to the particle file organization.

Reorganized the particle data reading and writing. After debugging, 
./check works correctly, which says the restart of just ions is working.
However, ./checksp is not working. Had to insert electrons into the
chargetomesh call and remove the initial move. This means they are not
smoothed at the start. Fixed! Now electron restart is working.

Implement thetamax test in cinjinit removing Btinf. There is substantial
additional testing to be done. Magnetized electrons don't seem to be 
correct yet. The plasma goes very negative and too many electron deposits
occur. This seems to be because the reinjection number for electrons is
calculated without consideration of the magnetic field. At least, the 
number is the same for zero field as for finite (and hence for electrons
large) field.

7 May 2014

Tidy up parts of the main iteration loop to make their work into subroutines
Implement lengthening of the step number format when the nsteps exceeds 9999.

commit.

8 May 2014

Multispecies data not yet implemented: 

Flux to objects. Probably can be accommodated within the existing data
structures which allow for up to 5 quantities to be accumulated on 
objects. We could increase the number of quantities.

Electron force contributions. Effectively these are additional momentum
flux contributions. 

Particle distribution diagnostics. 

Start working on fluxes and forces.
Add ispecies to arguments of tallyexit, objsect, ... binadding.
Add nf_species to 3dcom.f. It is set to nspecies before calling 
fluxdatainit. Now we are at liberty to set mf_quant according to 
whatever number of species we wish to track.
However, mf_quant might then be overloaded if it is also used to 
tell how many quantities to track. So we need to be careful.
It is used in reduce.f and objplot.f.
Create a species-arrayed quantity count. kf_quant(object,species).
Make mf_quant the sum of this over species.
We need a cumulative quant address too if_quant(object,species) which
is the start of the data for that species. Add that to 3dcom.

Got working to write flx file. Testing. Seems to work. To examine with
fluxexamine, you just have to specify the appropriate -piii switch with
quantity number decided in terms of the stacked up stuff. So really
at the moment you need to know how many quantities per species so that
you can look at the right thing. Fluxexamine ought to be modified to
make this easier and transparent. But that's not urgent.

commit

Thinking about particle distributions. I realize there's a big bug
in the multispecies code. It is that the particle accumulation 
routines are assuming that x_part is 3*mdims+1 but it is now
3*mdims+2. I really ought to get rid of the mdims usage completely.
Start by changing mdims+1 to mdims+2. This seems to fix the clearly
erroneous partexamine output. Actually that's not quite right the
flags are in 3*mdims+1 but the length is 3*mdims+2.

Purge all mdims from partaccum.f 
Convert particle passing to reference to partcom.f. Now we are safe
but still not doing any accumulation past the first species.
We might perhaps easily implement accumulation of one of the species
by adjusting the range of accumulation iic ioc.
Implement that, passing argument ispecies.
Implement control of ispecies in partexamine.
coptic always assumes species 1.

commit.

Fixed bug in periodicwrite diagsum dimension declaration. This fixes
the diagnostics of electrons. And I notice there are electrons with
substantial perpendicular velocity but only at the axis-3 ends of the
box. Perhaps these are reinjected with non-zero velocity?

Found bug in velocity cosine normalization for multiple species. Fixed.

Setting the tangential velocity to zero does change the weird perpendicular
velocity electrons. But there's also a systematic offset. Needs to be 
more thoroughly investigated. Basically, how does the reinjection 
and initialization detect that it should treat a species as drifting?
And when it does, what should happen to the random choice?

Drift species should be injected with zero perpendicular velocity,
and with appropriately random parallel velocity. This means perpendicular
and parallel relative to B. At the moment the selection is done with
the cumulative probability distributions in the coordinate directions.
That doesn't seem to be correct because it will give rise to large
tranverse velocities. We do need to get the fluxes on each face from
a face-directed flux probability. However, having decided to inject,
we ought to select velocities representing a true drift case.
We also ought probably to avoid caverein.

In fact, I realize that the same problem exists when the drift velocity
is not in the direction of a coordinate axis. The probability distribution
is then not separable in the coordinate directions and is probably wrong.
In effect the expected tangential velocity depends upon the normal velocity.
I suspect the natural way to solve this problem is to use the collisional
reinjection technique. However, it needs to be initialized differently
in pinit.

Fix the test for overrun of nf_datasize. It did not work correctly for
the current code status. (And gave overruns on big data.)
Things are a bit tricky with restarts. Fluxdatainit is done before we
do restarting. Therefore, if we are going to restart we don't know the
total number of steps that is going to be taken at the time of
fluxdatainit because it is the number taken already plus the new
number. I think that means if we are going to restart, then we must
do the test based on nf_maxsteps not the nsteps asked for. However, if
we know we are not restarting we can do the test based on nsteps.

Created new common nf_nsteps which is the number of steps that nf_step
is supposed to go to. It is not written or read in fluxexamine, because
when flux is written the nf_step determines how many steps the data is.
Thus nf_nsteps is used only for initialization to tell how much data
will be needed in this run. Normally nf_nsteps=nsteps. However, if we
are restarting flux, then nf_nsteps=nf_maxsteps because we don't know
how much total data we'll need till the restart is done and that isn't
done till after the fluxdatainit. Seems to work.

commit

Why does fluxdatainit have to be done before cijroutine? 
Earlier note:
Had to move the fluxdatainit before cijroutine otherwise the ijbin
is not calculated correctly.
This apparently arises because of 
                  idob_cij(ioad+1,oi_cij+ichain)=ijbin
It is used subsequently in cijdirect to access flux densities for
floating potential calculations.

13 May 2014
Perpendicular Temperature implementation.

14 May 2014

Implementation of maxwellstats which creates a bank of particles 
distributed as an anisotropic maxwellian shifted by the component
of drift velocity in the parallel direction. Parallel is either 
B (if non-zero) or v direction. Tperps is the temperature perpendicular
to it (gyrotropic at present). 
This routine could be called multiple times to generate a bank that
has particles corresponding to multiple maxwellians.

Changed baresheath.dat to include drift velocity and collisions to
generalize its test for collisions.

We need to decide when the maxwellstats approach gets called. It is
needed if Tperp!=Ti, or when there is drift velocity in a direction
not along coordinate axis. Implemented those tests in pinit and
set notseparable variable. 

Implemented calling of maxwellstats and colreinit. 
Seems to work. At least does not crash with anisotropic temperature.
Can't be compared exactly with standard case because statistics are
different.

Now we need to set Tperp small for species that are effectively 
infinite B. Implemented and fixed several problems for electrons
that required the ispecies reference for cdistfluxs etc.

There's another problem with ncdist for multiple species. Needed to
reset ncdist to zero before starting the next species' maxwellstats.
Probably ncdist should become an array to make the species independent.
It did. I just need to reference it.

Made -rx0. the default now. Added -rx.5 to several standard geometry
files to retain their checksum outputs.


Commit both branches. 

Investigating more portable options for file writing. Fortran 2003 has
access='stream' which gets rid of the record length indicators that are
causes of differences between compilers. However g77 does not recognize
it. It is an alternative to the access='sequential' that is used by 
default. (Pathscale does not support it!)

Fixed creintest in testing/ make testing now does not give errors.

Implemented make Setup streamset and streamunset that cause the 
file writing to be access='stream' or undo it. This ought to be 
used in situations where a FORTRAN 2003 compatible compiler is used
and portability of the binary files is required. If g77 is desired
the stream access cannot be used.

20 May

Trying to track down the remaining velocity diagnostic nonzeroes when 
using drift electrons. I realize the cdist is not quite right. 
I've implemented maxwellian with Ts and Tperp but drift only in the
parallel direction. If I have perp drift, that's not correct.
It is correct if there's no B-field. So it's only the Bt case that 
is incorrect. I need to add on the vperp which is the drift perpendicular
to the Bfield.

There's a confusing problem in that I am using non-zero vdrift(1) to 
indicate non-z drift, and setting it equal to a tiny value. It would 
be better to test if vdrift(3) is exactly 1. Did that.

We get a hang in the electron reinjection when they are colreinject
if the ions are not. (Which they now aren't). This is some problem
I don't understand with the colreinject multispecies handling. 
Make everything in cdistcom into multi species, in colreinject, colreinit.
Still hangs in invtfunc. Found the ncdist dependence problem and fixed.
Things look better but we still have strange edge v-diagnostics.

Working on tracking this down. It seems to come out of moveparticle.
Furthermore it arises only when dtpos is smaller than standard. 
This seems to make sense because it happens when dtpos*Bt*eoverms
is smaller than the threshold. Perhaps it is not possible to put the
drift case detection into moveparticle because of the possibility of
a much reduced dtpos. We ought to make the decision once and for all
for each species and treat all particles the same.

Move the drift versus move decision into padvnc. 

Found the problem with the zeroes in diagexamine and changed to require
the divisor to have only gt 0 not gt 5.

Looks as if the drift electrons are working.

Commit.

Discovered a major corruption of the notes. The commit 877b36131 on
May 16 appeared to lose major portions of the May notes. I take the
latest notes and laminate it onto the notes from the prior commit
defd4fd. That way I seem to have recovered most of the lost information.
However, I am puzzled as to how this loss occurred.

Checksum on uniformelec is different. Reset.

25 Jun 2014

My tests of ran1 for numerical methods shows that it has only a very 
limited number of independent samples. Although this probably doesn't
matter much, it would still be a good practice to replace it with a
better code. I have a copy of RANLUX which is self-contained 
restartable fortran. My tests show it is much better than ran1 even
at the lowest level of luxury; (luxury means throwing away some of the 
random numbers to improve randomness.) 

First tried make testing. Found a bug in sortest. This led to a hunt for
a major hidden segfault bug. Eventually found in a remnant argument to
meshconstruct. Really bad. Don't know how long that's been there. Fixed.
[Mistake. See below]

Found checksum differences in make geometry. Not all: baresheath[2],
geomsmall, geomthin, uniform4. Turns out those are all cases that do
not have partperiod set.

Back-track versions to establish where the difference happened.

e2a35b87301a8574983aed59df535919f4971611 OK

Just prior to now:
1c738f8cbde093915c72141a003e39965db16d63 OK except for
geometry/uniformelec.cks:
1c1
< 19669    34
---
> 31687    34
Wed Jun 25 15:20:40 EDT 2014
There's a diff in uniformelec.cks, but no others.
Putting back the meshconstruct bug, the cks differences go away (except
the uniformelec). Goodness knows what it did, but fixing that bug made
the checksums change. I see that the coptic call has the extra argument
therefore removing it from the meshconstruct was wrong. I should have
added it to the sortest call.

Backtrack the meshconstruct fix, and correct it to a sortest fix.

Now think about ran1 replacement. There is an issue with restart.
Gasdev has an internal state that needs to be saved along with the 
ranlux restart information. Also the code needs to call the transfer
operations to get the ranlux state in and out. That probably can be done
in partwrite/read. That's where it is currently done in effect.

There are maybe 80 calls to ran1 in the code base. Do I want to change
all those calls, or do I want a wrapper. I think I probably want to change.
First thing to do is to add extra restart capability that stores and 
resets ranlux. Did that and git added ranlux.f. Checks ok. 

Insert rluxgo calls as appropriate in coptic.f.
Replace all ran1 calls outside randf.f with ranlux calls. 
Replace gasdev random with ranlux. These changes change the make
test. They should. Turning off the ran1 initialization in coptic.
I find we can remove ran1 from the code base and still compile.
./check gives no errors. Same with /check50. Basically seems to be working.

Remake the geometry cks. Then those check out with subsequent makes.
(Of course.)

Finished replacing ran1 with ranlux. But haven't ripped out all the old
code.

Commit.

Now remove ran1 procedure. Removed references to ran1 from everything
except partwriteread, ran1com and randf.f.
Now move the storage of gasdev state into the end of ranluxstate.
Check still works.
Remove ran1 equivalence to ranstate so it is dummy.

In the interests of thoroughness try to compile with reinject.o as the
REINJECT choice. It is broken. Thats because plascom.f now needs
ndimsdecl.f That's easy to fix. However, now there are undefined
references in pinit.f and padvnc.f. Copied the relevant routines from
cartreinject.  Probably the reinject version isn't working correctly,
but at least it now runs.

Go through the same procedure for orbitinject. It stops with an error message
0001-212 Error in external integration initialization. rb=  0.

These routines do not break when every reincom has a rancom. Thus we are 
going to rationalize the rancom information into reincom.f
Did that. Everything compiles etc. Remove rancom.f reference everywhere.
Now we can use that as our common file for ranlux.

Remove ranstate writing (and reading for V>2) from partwriteread.
Remove ranstate from ran1com. Ok. 
Rename what's left to rancom.f and change reference in gasdev and 
partwriteread.

git rm ran1com.f All that's now left is ran1.f which is the routine
itself. Not under git. Move it to numrec. Remove ran1com.f ref from makefile.
Now seem to have purged ran1 from everything.

Commit.

We have a segfault with -gp with slicegweb from main and -gt.
All slicegweb. This proves to be caused by ifix uninitialized
Setting to zero seems to fix it. Also put a safer test into slicegweb.

commit

Remove dependence on randc.c and randc.c from objects and git.

commit

Remove dependence on randf.f by moving gasdev to ranlux and other stuff
to average.f. git rm randf.f

commit

Fix bug in makefile associated with preset G77.

1 July 2014

Subtraction of objects controlling potental.
--------------------------------------------

We want to be able to construct more complicated objects, especially ones
with reentrant holes, using subtraction. This has not yet been done.
The boolean for the particle region doesn't control boundary conditions.

The logic of object Boundary Condition specification appears to be this.

An object can be additive, subtractive, or null. 
We have currently implemented additive and null objects.

Null objects A-B=C=0 do nothing to potential boundary conditions.
They are just surfaces which can measure fluxes.

Additive objects set boundary conditions, normally Robin. A,B,C.
There is at present some ambiguity about what happens if the surfaces
of more than one additive object lie between two mesh nodes. Eventually
that ought to be systematized.

Subtractive objects do not possess their own boundary conditions. Instead,
they serve to reduce the effective volume of an additive object. The 
regions of the additive object that are inside the subtractive object are
removed. As a result:

1) Surfaces of the additive object that lie within the subtractive object
are removed and exert no boundary influence. Call such surfaces subtractED.

2) Surfaces of the subtractive object that lie within the additive object
become surfaces of the additive object and acquire its boundary condition.
Call such surfaces subtractIVE.

These appear sufficient for objects with uniform boundary conditions. More
complications will arise if non-uniform conditions are required.

It appears that this logic can be implemented in the routine potlsect.

Currently for any mesh leg, it cycles through each object in sequence.
If it finds an intersection, fraction is set non-unity. Then the 
conditions(3) are calculated and the routine returns. Consequently
only the first object that crosses the leg is accounted for. In effect
that's the systematization of multiple object crossings: the earliest
object is the only one that counts.

Clearly we've got to implement more logic here, to remove subtractED
surfaces that lie within subtracted regions and to add subtractIVE
surfaces inside additive regions. 

I suppose it will be helpful if every additive object has a list of
associated subtractive objects. Then when an intersection is found, we
can test if it's to be subtracted. It may also be helpful to be able
to specify whether the inside or the outside of a subtractive object
is subtracted. That's one bit for each. I think that information ought
to be associated with the additive object from which it is
subtracted. If this approach is followed, a subtractive object need
actually be no different from any other object. Sounds as if what is
needed for each (additive) object is
1) The number of subtractive objects associated: normv
2) For i=1,normv the object number and sign of subtracted objects.

Implemented the mechanisms to set the association.

There's a problem with boxedge. It finds new fractions not found by 
the original potlsect calls when subtracting is used. This is a bit 
surprising because boxedge uses potlsect. Tracked down the bug to 
an error in the new code position calculation. Seems gone now.

We now have subtractED surfaces working not yet subtractIVE.

2 Jul 2014

One way to implement subtractive surfaces is simply to assign the additive
surface's condition to the subtractive object and declare that the 
subtractive object is additive with the complement of the additive object
subtracted from it. There are then four options I could imagine:

1) The subtracted object does not add subtractive surfaces; instead it
just hollows out the original object leaving a shell with holes.

2) The subtracted object adds all its usual surfaces and removes the
contained additive object's surfaces.

3) The subtracted object acquires the conditions of the additive and
generates subtractive surfaces with those conditions.

4) The subtracted object has its own conditions and applies them to the
subtractive surfaces. 

We have to be careful of overloading the parameters. Can a subtractive
object be also an ordinary object? Not if it has the complement of the 
additive object subtracted. How do we indicate not to add subtractive
surfaces? Perhaps use a different type number. If so then the options above
correspond to 

1) Type number 89. Subtractive boundary conditions input as null. Left.
2) Type number 89. Subtractive BCs input non-null. Left.
3) Type number 88. Null BC input. BCs set to additive object's. 
   Subtraction set to complement of additive object.
4) Type number 88. Non-Null BC input. BCs left alone.
   Subtraction set to complement of additive object.

These require that the subtraction input line occur AFTER all object lines.
Implemented. Seems to work. 

There is a difference in the region textplot between 3 and 4. It is to
do with the setting of the ifield_mask. With 3 the subtractive object
is masked out. With 4 it is not. I'm not sure which is best. ifield_mask
is relevant to padvnc and getfield interpolation. Need to think.
When doing gradlocalregion, the iregion passed has been masked by ifield_mask
Generally null objects are masked out of consideration to prevent doing
spurious extrapolation across null-object boundaries. That probably ought
to be the case for all subtractive objects since generally they will
have surfaces that are irrelevant but in the particle region where the
field is needed. If, however, subtractive objects are universally
field-masked, then near the subtractive surfaces they introduce, there
will not be extrapolation when perhaps there should. The test in 
gradlocal region is between iregion and idob_cij(iregion_cij,.). Therefore
presumably the most important thing is that these be consistent.
It may therefore be that it makes little difference. I observe that

I get a different result from runs with subtraction of null and subtraction
of a BC identical to the additive object, 3,4. So there is something different
happening. 

However, phi files are the same with or without the extra command:
               ifield_mask=IBSET(ifield_mask,isoc-1)

But somehow subtracting an explicit BC is different from subtracting a
null BC even if BC values end up the same.  Somewhat puzzling. I can't
reproduce this difference. I find identity between Explicit and Implied.

Go carefully. There are two different reference files given.phi zeromasked.phi
   	      	      diff given.phi	      diff zeromasked.phi

No IBSET Implicit	      differ		      don't
No IBSET Explicit	      don't		      differ
IBSET Implic	      	      don't		      differ
IBSET Explic		      don't		      differ

Thus the final answer is that there is a difference between implicit and
explicit if the extra command is absent, but not if it is present.
Leave it present for now. Implicit subtractive objects are not masked,
they are included in the iregion_masked calculation.

Some tests with different cylinders and spheres show that there are
sometimes No good vertices errors when the subtracting object is nearly
tangent to the additive object. That might be worth trying to fix.
It might be affected by the above IBSET command.

Cube test tickles small bug in zero volume cube test. Fixed.

Update the RefManual. I realize there's a puzzle with particle absorption
for a hollow subtract 89 object. Need a new absorption condition, since
particles travel everywhere, but are only absorbed by the shell.

3 July 2014

More general objects, surfaces of revolution. 
---------------------------------------------

In general these are not too hard to implement without flux collection.
The hard part is flux collection tracking and storage. 

If we consider the revolution of a piece-wise linear polyline about an
axis, then there is a natural division of the flux surface by line-element.
Then presumably equal spacing of revolution angle would be the obvious
orthogonal division. It might more generally be possible to divide each
line element into a number of equal-length sections.

A convex object of revolution can be considered composed of a monotonically
increasing set of values of the axial coordinate, z, at which the radial 
coordinate, r, is specified. It could be specified by two positions
x_0 and x_1, which are the ends of the axis, and some number of point pairs
(z_f,r) denoting the axial position as a fraction of x_1-x_0 and the radius.

Determination of whether a point is inside would proceed by 
(1) Finding z. If 0<z<1 (2) Finding the corresponding r. (3) Is rp<r?
Probably if the number of segments is small, bisection interpolation 
would be fast enough to find r.

Cones have two segments (one node). Truncated cone 3 segments.

Currently the largest object is a pp, which has a total of 7x3=21 reals in
its obj_geom structure, this is after the ABC specification.
The revolution data structure for polylines etc would most likely be 
Base. Base point="center" x_0. 3 reals.
Apex. Other end of axis. x_1. Needed to construct r. 3 reals
Dir. Normalized vector direction such that z=Dir.(x-x_0). 3 reals
N (number of line segments, zero for unused?) 1 integer.
Then we need to put the z and r into separate arrays so they can
be solved for inside_geom. Currently I've allocated new arrays for them
since I was running out of space.

There is a bigger task for storing flux data, which is to discover the
intersection of a line between two postions with the surface of revolution.

Added data structure references to 3dcom.f

commit

Implement corrected structures. Implement initialization and inside_geom
detection for surface of revolution. Have not yet done the fsect for it.
So potlsect ought not yet to work. 

fsect needs to determine the point of intersection of a line between two
points and the object surface. It could use some of the inside_geom
code to find the segment of each point, and then test just the
line segments between those for each point. I wonder if we have a
generic test of intersection of two finite line segments? Probably not,
because that's a 2-D construct, not a 3-D construct.
Wrote a subroutine to return the fractional distances for two line segs.

This is insufficient however. Intersection of a 3-D segment with a surface
of revolution is non-linear because of possible angle differences at the
segment ends. There might be more than one intersection. 

Implemented a srvfsect function based only on inside_geom and bisection.
It seems to work pretty well, and fast enough not to be problematic.
Although I developed a sophisticated root searcher for cases where
the root is not bracketed by the original points, that does not seem to be
necessary. There may be some intersections that I miss, but presumably
only in regions where there are dimensions smaller than the mesh spacing,
where resolution is impossible anyway. Intelligent choice of mesh alignment
can minimize any inaccuracy (e.g. choose odd number of points if axis is
at center of grid).

I think I have surfaces of revolution working.

commit

4 Jul 2014

More general surfaces of revolution
-----------------------------------

So far what is implemented is convex surfaces of revolution in which
the (z,r) pairs are monotonic in z. However, since the intersection 
code is based purely upon inside_geom, it seems relatively easy to 
implement more general surfaces of revolution which might be concave.
The difference is that we can't determine insideness based upon the
comparison of rp with r(z). Instead, we have to calculate (rp,zp) and
then determine the number of intersections of the line joining p to an
outside point, po, with the 2-D contour to be revolved. If that number
is odd then it's inside. Code for determining this sort of question is
in ES2D. The wall (piecewise linear contour) must either be closed
or else have both ends at r=0. (In the latter case an implied closing
segment along r=0 need not be tested for intersections.) It does not
seem implausible that doing the general wall intersection test would
be as quick as the current search for z and interpolation.

Implemented type 7 general surface of revolution. This isn't quite
right yet because I add in the ends of the axial vector to the wall.
That's not right. Still for speed comparisons it is fine. I find
that type 6 is about 15% faster than type 7 in the calculation of
cijroutine which is dominated by potlsect and inside_geom. The difference
is hardly significant.

Made type 7 require the entire wall to be specified. To perform the
same as type 6 it requires two additional pairs to be specified:
0.,0. at start and 1.,0. at end. 

Update RefManual

commit

Failed various geometry tests (with very slow performance). 

If I delete the cks files then git checkout does not actually check them
out. Instead it tells me they are missing and I need to 
git checkout --files geometry/*.cks to get them.
Now got all July 4th files.
make, make geometry. Errors only in geommany. Wild!
git log geometry/geommany.cks shows that it was last logged on
commit fc53702be6d595386a25aae76f92eaffebd4b391
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Thu Jun 26 10:57:33 2014 -0400

Conclusion: geommany is behaving differently. But its the only one really.
Checkout cbd173c99a14a4d47b6175edcaa54ecc64bd24be Failed.
476db44fa8ef4690f1f0ab63b1fbbb8ea812a25b	  No differences.
1e036dd12f37836f8c1681270903feef6217c352	  No differences.
cbd173c99a14a4d47b6175edcaa54ecc64bd24be	  Failed.
The last two are adjacent
commit cbd173c99a14a4d47b6175edcaa54ecc64bd24be
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Thu Jul 3 09:35:00 2014 -0400

    Completion of object subtraction code and start of surface of revolution
    
    Subtractive objects appear to be working as expected. RefManual updated.

commit 1e036dd12f37836f8c1681270903feef6217c352
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Sat Jun 28 18:18:37 2014 -0400

    Remove randf.f, moving gasdev to ranlux.f and others to average.f

Evidently the object subtraction code was the point where geommany lost
its cks.
Look at git diff between these commits. There are a number in the object
areas. There is also a major change in accis involving large numbers
of files.
Checkout in copticdebug the 1e036 version.
Copy the 3dobjects.f file to coptic. make. Then no differences.
Conclusion. It's a change in the 3dobjects.f that caused the change
in geommany.
Checkout cbd17 in debug. Failed.
cp prior 3dobjects.f make current. OK.

Found the change to be in the section near
Sphere outward normal. It used to be in error. Therefore the Failed ought
to be ignored and .cks fixed.

Redo make geometry with current build. Everything is really ok. 

commit.

5 Jul 2014

Hollow cylinders are simply a general surface of revolution. There is no
need to implement them separately. 

What we have not yet done is implement flux accounting for surfaces
of revolution. That is probably the next thing to do. For the object
file input, one needs perhaps to specify npair+-1 integers representing
the number of equal fractions into which each line-segment of the 
contour of revolution is split, and the number of angle facets.

9 Jul 2014

Working on general surface of revolution flux identification. 
Developed an addressing scheme, and its initialization, using 
a fractional index along the contour.

Need a reference theta direction. Ought to be calculated automatically,
or we could specify a reference vector the way that the non-aligned
cylinder does. That would elaborate the input line rather. Not sure.
Checking into general cylinder case. It does a tranform to
contravariant components. I want nearly that too but probably not
explicitly in the way that does it. [Done explicitly. See later.]

-----------
Profiling comparison of cylinder with equivalent surface of revolution
./coptic.prof -ni1000000 -s50 geomSoR.dat
SoR

  %   cumulative   self              self     total    
 time   seconds   seconds    calls   s/call   s/call  name    
 22.95     17.95    17.95 630981495     0.00     0.00  gradlocalregion_
 18.72     32.59    14.64 157635282     0.00     0.00  getfield_
 12.12     42.07     9.48 160644291     0.00     0.00  interp_
  9.62     49.60     7.53 215361636     0.00     0.00  inside_geom__
  9.05     56.68     7.08 51000000     0.00     0.00  achargetomesh_
  5.47     60.96     4.28       50     0.09     1.43  padvnc_
  2.76     63.12     2.16 157635282     0.00     0.00  box2interpnew_
  2.61     65.16     2.04 76561781     0.00     0.00  w2sect_

[3]     91.5    4.28   67.26      50         padvnc_ [3]
               14.29   21.17 153817641/157635282     getfield_ [4]
                1.35   12.00 52545094/53545094     partlocate_ [6]
                6.94    2.56 50000000/51000000     achargetomesh_ [8]
                0.66    2.36 51415435/52424638     linregion_ [13]
                0.27    2.29 50000000/154545094     insideall_ [10]
                0.37    1.32 1272547/1272547     getpotential_ [17]
                0.81    0.00 51272547/51272547     moveparticle_ [24]
                0.25    0.24 1272547/1272547     cartreinject_ [28]
                0.12    0.24 1272547/1272547     diaginject_ [30]
                0.00    0.03 1272547/11935435     ranlux_ [33]
                0.00    0.00 1272547/1272547     reinject_ [104]
                0.00    0.00  142888/142888      tallyexit_ [106]
                0.00    0.00      50/50          cavereinset_ [120]

Clearly this is totally dominated by the electric field evaluation and
the direct calls to insideall_ (which is where extra costs are incurred
for the SoR) are totally negligible: Roughly 1% of total. Partlocate
costs are dominated by interp (and interp by partlocate). inside_geom
costs mostly come from insideall calls, 15% from linregion calls.

Cylinder
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 25.50     18.58    18.58 630981503     0.00     0.00  gradlocalregion_
 18.09     31.75    13.18 157635282     0.00     0.00  getfield_
 12.54     40.89     9.14 160644291     0.00     0.00  interp_
  9.40     47.74     6.85 51000000     0.00     0.00  achargetomesh_
  7.70     53.34     5.61 206970420     0.00     0.00  inside_geom__
  6.05     57.75     4.41       50     0.09     1.34  padvnc_
  3.24     60.11     2.36 157635282     0.00     0.00  box2interpnew_
  2.30     61.79     1.68 53545094     0.00     0.00  partlocate_
  2.23     63.41     1.63 37938341     0.00     0.00  gradinterp_
  2.18     65.00     1.59 40062302     0.00     0.00  __ieee754_exp_sse2
  1.96     66.43     1.43     3094     0.00     0.00  sorrelaxgen_
  1.52     67.54     1.11 154545094     0.00     0.00  insideall_
  0.95     68.23     0.69 51272547     0.00     0.00  moveparticle_
  0.72     68.75     0.53 52424638     0.00     0.00  linregion_
  0.55     69.15     0.40  1272547     0.00     0.00  fillinlin_
  0.47     69.49     0.34  1272547     0.00     0.00  getpotential_
Total  72.84

Re-do SoR
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 23.92     18.94    18.94 630981495     0.00     0.00  gradlocalregion_
 17.51     32.81    13.87 157635282     0.00     0.00  getfield_
 12.09     42.38     9.58 160644291     0.00     0.00  interp_
  9.90     50.22     7.84 222204652     0.00     0.00  inside_geom__
  8.32     56.81     6.59 51000000     0.00     0.00  achargetomesh_
  5.91     61.49     4.68       50     0.09     1.44  padvnc_
  2.74     63.66     2.17 80892917     0.00     0.00  w2sect_
  2.68     65.78     2.12 157635282     0.00     0.00  box2interpnew_
  2.35     67.64     1.86     3094     0.00     0.00  sorrelaxgen_
  2.07     69.28     1.64 37938334     0.00     0.00  gradinterp_
  1.81     70.71     1.43 40075802     0.00     0.00  __ieee754_exp_sse2
  1.55     71.93     1.23 53545094     0.00     0.00  partlocate_
  1.29     72.95     1.02 154545366     0.00     0.00  insideall_
  0.95     73.71     0.76 51272547     0.00     0.00  moveparticle_
  0.83     74.37     0.66 59267382     0.00     0.00  linregion_
  0.52     74.78     0.41  1272547     0.00     0.00  getpotential_
Total  79.17 

There is a 2 second (~2% of total, 30% of routine) increase in inside_geom
cpu cost. That is amazingly cheap for general SoR. w2sect is also in effect
part of the additional cost. So perhaps there is about the same again 
increase. But all of this is so small as to change the total by only
about 10%.

Conclusion. It would be possible to use the general SoR routine for all
(circular) general cylindrical shapes without out significant computing
additional cost.

Adjusted the input format of SoR to include reference vector, and 
a radius magnitude. Started to reconfigure to use cylinit to 
calculate all the contravariant vectors. Had to move sr_npair.
However, not yet completed the move to cylinit because it writes
back normalized contravariant vectors over the ovec positions, and
that's contrary to current expectations. 

11 Jul 14

I think the reason it writes the vectors back is simply to have a full
covariant set of vectors as well as contravariant. The covariant vectors
do not seem to be used. And if I switch off the writing back, then type
5 cylinder gives identical particle results. However the .flx files seem
to differ. Yes the covariant vectors appear to be needed for fluxexamine
to reconstruct the object. Perhaps we ought to fix the SoR code not to
break rather than changing the cylg code. 

It seems inside_geom is the problem. It thinks ovec still contains
the apex-base vector, and subtracts z times it to get radial vector.
I should probably use the contravariant vectors instead to get rb, rg
and hence r. That might require more multiplications. The current approach 
takes three squares and three multiplies. Dotting into two contra vectors
would require 6 multiplies and 6 squares. It has the advantage that I 
could have different radii in different directions.
 
I think the only thing actually wrong at the moment is that I use the
wrong direction, since the directions have been exchanged to (r,th,z)
by the cylinit code. Change to the correct direction. Then it finally
works, with exactly the cylinit code. I have not changed the way r is
used or gone to using 2 separate perpendicular vectors. (But I could.)

Ok now we have a rationalized SoR code consistent with cyl. Not yet
flux tracking on SoR though.

make geometry. OK.

Added a non-aligned cylinder to geomcyl.dat and updated the cks.
Added a new geomSoR.dat file to geometry.

commit.

Trying to decide the best way to track flux for general surface of 
revolution. It may be easiest to think in terms of the plotting.

Think about cylgplot (in objplot.f). The way it works
is by constructing a set of facets possibly smaller than the actual
collection areas. For this purpose it uses ofn1-3. The number of 
divisions in r,th,z. It then draws the invisible end, circular face,
and visible end, separately. This is done as follows
For each angle i to nangle, chosen to start from nearest to eye
  for each r j to nr,
    for each corner k to 5
      get the three contravariant components xcontra
      call contra3world to make them world.
      set rface(k,:) to this position.
    call facecolor to color the face (and do vtk output)

   call facecolor(iosw,2+ie,j,itc+1,iobj,iav,rface,fmin
     $           ,fmax,1,lfw,isign) 
   subroutine facecolor(iosw,imin,k2,k3,iobj,iav,rface,fmin,fmax
     $     ,i2,lfw,isign)
c imin is the face index (1,2 or 3 for a cylinder) 2+ie
c i2 is the dimension-index of the first facet index 1
c k2,k3 are the indexes of the facet within the face. j,itc+1
c iav is the address of the start of the face within ff_data.
c Coloring by flux does this:
         ijbin=(k2-1)+nf_dimlens(nf_flux,ifobj,i2)*(k3-1)
     $        +nf_faceind(nf_flux,ifobj,imin)
         iadd=ijbin+iav
Thus each facet has just two indices 
and each face needs the extra information
 nf_dimlens(nf_flux,ifobj,i2), nf_faceind(nf_flux,ifobj,imin)
 how many facet divisions    , start of this face

The generalization appears to be that each line segment defines a face.
Theta (instead of r) could be chosen as the first facet index. This
would avoid having to know the dimlens different for all line segments.
These nf_values would have to be entered by fluxinit. However, there
are then not enough nf_faceind() slots to give the start of every face.
Basically this quantity is the cumulative number of facets. Its third
allocated dimension is 2*ndims. That would limit it to 6 faces, which 
is the number for a cube. It could be increased, I suppose. It is far
smaller than nf_address, whose last dimension is nstepsmax, 
so it won't have any impact on code size. Might as well bite the bullet.
Set to sr_vlen-1. Now we have enough. So the above call for SoR
should have imin the face index 1-19, i2=1 (the theta dimension), 
k2,k3 the theta and r2 indexes, iav start of face in ff_data. Looks
straightforward.

Start to write the monotonic SoR version of fluxplot. 
We also need to write the srv parameters into the flux file in order
that they are available. (Not yet done).
Decided to incorporate the srv parameters into the obj_geom structure.
Done. Removed the srvnr,z,div data. Now incorporated into obj_geom.
Adjusted flux write and read to accommodate new odata length and to
grandfather versions earlier than 5. Now all parameters are available.

Implemented coloring of Surface of Revolution just by facet, and debugged.
I think we therefore have the positional information correctly stored,
recovered and interpreted now for the monotonic SoR. The general SOR
needs a reordering of the face order in which drawing occurs --- at 
least for the drawing version; it would not matter for vtk writing.

Order of drawing for general SoR. 
Based on thinking about a torus:
 Start at the furthest theta (from eye).
   Start at the furthest rz position and draw all faces in sequence 
 Iterate theta.
Started with srvgplot which is simply srvplot with the theta and rz
iterations reversed in order. That just works. 
Now I need a torus to see how badly it works.

21 Jul 2014

Completed development of a sorting algorithm based on tests for closeness
see note book. 

c Seems that the standard insertion sort does not work for faces that
c are disjoint. It assumes that if face x is greater than face y, then
c all the faces ordered earlier than y are less than x, but that's not
c true if there's a face z that is disjoint from y and happens to have
c been ordered earlier as a random choice.

c I think that selection sort would solve this problem, at the cost of 
c some extra effort. 

c Discovered another problem with the ambiguity of theta modulo 2\pi.

Installed into objplot srvgplot for general surface of revolution.
After correcting some of the calling parameters, it works! Both for
general and for convex surfaces.

commit

Now we need to implement the ijbin calculation for surfaces of revolution.
And then the full flux tracking. This is how facecolor accesses the ijbin
c Coloring by flux. For cyl, i2=2 but we need srv i2=1, imin=irz.

         ijbin=(k2-1)+nf_dimlens(nf_flux,ifobj,i2)*(k3-1)
     $        +nf_faceind(nf_flux,ifobj,imin)
         iadd=ijbin+iav
         ff=ff_data(iadd)
We pass  k2=itc+1 where
         t1=2.*pi*(i-1)/nangle-pi
         itc=int(objg(ofn1)*(t1/(2.*pi)+0.500001))
i.e.             No of thetaflux * t1/(2*pi) 
          =NoTFlux*(i-1)/nangle= NoTFlux*Theta/2pi.

srvfsect uses bisection to find the intersection. Once having done that,
we can calculate the angle and wall position from the final position.

22 Jul 14
Remove the pp_ and sr_ structures and make everying the ostructure.
Run make geometry. Ok.
Commit.

Work on ijbin calculation. 

23 July 14

Got an ijbin calculation in srvgfsect working. It counts ijbin in the 
order theta,ifct,irz, which is what I designed. Using the -gw257 switch
one can see the intersections in sensible places. However, the face
labelling from objplot does not agree with it. Therefore ijbin is not
being correctly calculated in it. Changed the inputs to facecolor
to correctly offset irz and to put i2=1. Now I can see the intercepts 
appearing in correctly labelled facets.

Fix the objplot so that it puts back the gradlegend. Now all is working
except perhaps the areas. Need to think how to test whether the flux
density is working sensibly.

commit.

24 July 14

Constructed a spherelike object type 6 SOR with equal area faces.
Actually equally spaced in cos(theta) (z).
It shows approximately equal fluxes. But not equal flux densities.
There's clearly something wrong with the area calculations.
That was found to be caused by not having completed the area scaling
by the zscale and rscale parameters. Completing fixes it.

commit

Discovered an error in world3contra that meant it would not do round
trip calculations (and was not doing calculations properly). Fixing
that stops the problems with no intersection found. It might also 
remove the need for the rather elaborate search strategy for finding
the psect. I don't know what errors this introduced in prior 
calculations. 

commit.

25 July 14

Need to sort out the rscale question. 
The radius comes in at ocylrad. It denotes the size in world units of
a radius of one in contravariant coordinates. Positioninit currently
gets it from the contravariant vector length. What happens in 3dobjects
is that cylinit uses the radius to scale the two perpendicular contra
vectors. This then does propagate to positioninit. 

However, if we change the radius to be anything except 1 we get No
intersection errors. So we don't have it accounted for in all the right
places. I suspect this is because the inside_geom calculation does not
use the contravariant vectors, but does its own subtraction. That doesn't
correct for this factor. Even though the radius (rscale) is implicitly
in the contra vectors, it is problematic to have to calculate them
from it. Too slow. It must be stored. 

Rearranged storage to accommodate this. Implemented storage and scaling
in inside_geom. This fixes the No intersection errors. Things look good.

commit.

Add geomCyl and geomSoR to geometry. Remove a couple of duplicative cases.

commit.

1 Aug 2014

Running some ISP geometry cases with electrons. I see unaccountable
leakage of electrons into regions where there ought to be essentially
zero density.  It seems likely that this is caused by the electrons
crossing the boundary in one step. Shortening the steps does seem to
reduce the leakage. Increasing dt increases it. I thus think that
something more robust than just detection of a change in the
inside_all is necessary for telling when an orbit crosses a surface.

I think this never showed up so much before because now that I have general
surface of revolution, such as a hollow cylinder I'm using here, it is much
more likely that a particle can step through the object and out the other
side. (Also of course electrons take bigger steps.)

I have a notion that the criterion ought to be changed from merely changing
the inside-object code, to examining the step and seeing if it encounters
an object. In principle this would require a routine to tell chord 
intersections with objects.

Start to construct icross_geom that will find the number of crossings.
Still needs work. Haven't finished cylinder implementation.
Haven't started SoR implementation. Haven't tested any.

Scaling r with z approach to SoR.

Split 3dobjects into two with the second half becoming intersects.f

commit though we haven't tested any of the new icross-geom code.

Plan for testing. 
We need to use the coptic infrastructure for specifying and setting 
up the objects. Then we might be able to use actual orbits for testing
or else we might want to use individual chords and icross_geom manually.

4 Aug 2014

Finished off icross_geom code.
There are some particles that cross incorrectly.

Compare speed with and without icrossall test on standard cubic (sphere)
case with -ni1000000 -s30
without: user    0m41.131s
with     user    0m43.555s

Cost negligible!. There are perhaps 10 particles per step showing uncaught
crossings. They all show two radii just above 1. That's correct.

Compare geomSphSoR.dat -s20. Type 6 SOR with 7 npairs representing sphere.
without  user    0m29.878s
with     user    0m32.742s
Small. Not quite so negligible but nothing to worry about.

Swap the role of insideall and icrossall.
Had then to implement a new routine leaveregion. It detects the exit
from the particle region accounting for booleans, based on examining
crossings between xprior and xnow, rather than on the now region being
different than prior. Thus it detects passage through an object and back
out the other side. It all seems consistent with the previous critieria
for simple object blocks (no compound particle region). It also does not
seem to cost significantly more, because of course it is not called unless
we detect a crossing with icrossall. Need to test with compound objects.
(Mind you, the old code was hardly so tested.)

Now testing on geometry files.
baresheath.cks ok
geomcube.cks fails
geomcubic.cks fails
geomcyl crashes with Undetected region transition.

There's still a lot of work to do! Start with Undetected region.
This seems to arise from the special orbit starting just on the 
surface of an object. Yes, moving it so it doesn't exactly coincide
solves the problem.

5 Aug
Fixed various bugs till we seem to have all geometry tests running.
However, many of them now have differences in the cks.

commit

time ./coptic geomSphSoR.dat -ni1000000 -s30

New user    0m48.611s
Old user    0m43.587s

There is a 10% cost addition in moving to the new intersection-based code.
Using a fairly expensive object 
#     ABC,    Base,   Apex,   Ref, Radius,Npair, Pairs
6  1 0 2, 0. 0. -1, .0 0. 1, 1 0 0, 1.,7, 0.661 0.125,  0.866 0.250,  0.968 0.375,  1.000 0.500,  0.968 0.625,  0.866 0.750,  0.661 0.875, 1, 4, 1,1,1,1,1,1,1,1

Rationalized passthrough simply to count and then report.
geometry/geomcube.dat is a test that has many, because of sharp edges.

Removed all geometry/*.cks files and regenerated them.

commit

ISP running on sceptic machine shows we are now excluding the electrons
that were previously leaking in. There are other problems with the nearness
of the boundaries and strong field.

There are several early undetected region transition errors I should
probably look into. They appear to be the special particle for each node.
These arose from the disc case of srvsect which had incompatible z tests
that excluded particle exactly on the disc. Fixed.

ISP really needs a way to start up faster. However, one can't just enhance
the step size in an electron simulation, because it makes it unstable. 
Perhaps one could run a Boltzmann case initially, with much bigger steps
of course, and then introduce the electrons when that has settled down.
Is there are sensible way to turn fluid (Boltzmann) electrons into particles?

One might be able to track electrons with large (but perhaps ramping down)
steps, and because one was not using them for the potential calculation
still avoid instability. Or perhaps there's a sum of Boltzmann and particle
electrons with appropriate balance. Then when we reach the steady state we
transition to using the electrons for charge balance. 

15 Aug 2014

Discovered some pointers to the time differences with gfortran and g77
on X1. Back on tp400 
mpif77 	       	     real    0m35.406s
mpif77 -f77=g77      real    0m17.262s
gfortran -O3	     real    0m16.280s
gfortran 	     real    0m35.377s
g77		     real    0m34.851s
g77 -O3		     real    0m17.211s
g77 -O1		     real    0m19.991s
gfortran -O1	     real    0m18.274s

Clearly the difference is somehow that mpif77 -f77=g77 is optimizing,
but mpif77 is not. However, there is no sign from mpif77 -show that this
is the case:
[tp400 coptic]$ mpif77 -f77=g77 -show
g77 -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lmpich -lopa -lpthread -lpthread -lrt

This is different from X1 where the -O2 flag is included. mpif77 has changed.
Seems that there is some code in the makefile that might be responsible.
mpif77 -f77=g77 -O3 -funroll-loops -finline-functions	 real    0m16.110s
gfortran -O3 -funroll-loops -finline-functions		 real    0m16.229s
indistinguishable.

Adjusting the compiler. If mpif77 is allowed to choose gfortran  
clean compile takes 
mpif77 	      	    real    0m19.266s
mpif77 -f77=g77     real    0m12.350s
There's some penalty when developing.

10 Oct 14

Added a switch to allow a fractional extra Boltzmann species. It is only
implemented when there is more than one species. We specify a fractional
extra species which behaves with linearized Boltzmann-like response.
That is, the extra function is faddu = boltzamp*u. 

This has the effect of damping out long wavelength modes in runs where
the Debyelength is small compared with the domain sized. I think it might
also serve the purpose of enabling faster equilibration initially. Then
ramp down the Boltzmann amplitude. I think there is still a limitation
on the timestep size that will handicap acceleration at the start. 

When lambda_{de} is 1, the timestep must be shorter than approximately 
0.01 for stability.

My guess about stability is that there is some minimum fraction of electron
response that is Boltzmann that will stabilize large steps. Effectively
Boltzmann electrons are implicit and particle electrons are explicit.
However, it appears the fraction of Boltzmann for stabilization may be
approximately 1.

10 Nov 14
Discovered that the unformatted binary file format of gfortran is the same
as pathscale. Therefore my previous problems were just g77. A big benefit
of moving to gfortran is therefore compatibility of output files and 
analysis routines.

24 Nov 14
Fix bug in objplot commented out using rs. 

git push.

Unfortunately I discover that g77 won't accept the include file for the
arguments of the cmdline call. So put back the explicit arguments.
Left the include as part of sortest. Perhaps that will fix my problems.

Find that there are many cks differences. They seem to go back quite a way.

Checkout abd9cc1fc424453e251163792203691c65bd8ef9 11 July 14
make mproper; make; make geometry Failed some: 
geometry/baresheath2.cks:
geometry/baresheath.cks:
geometry/uniform128.cks:
geometry/uniform4.cks:
geometry/uniform.cks:
geometry/uniformelec.cks:
geometry/uniformonly.cks:

checkout f2879ac9c1c077c1c4c7d89c206005607f0626ad  5 Aug when cks were 
remade. We still have fails:
geometry/baresheath.cks:
geometry/uniform128.cks:
geometry/uniform4.cks:
geometry/uniform.cks:
geometry/uniformelec.cks:

I don't understand these unreliable results. I don't seem to have the make
file set up right. But in any case I can't verify a time at which all the
geometry tests were right.

Resetting all the .cks files one does
rm geometry/*.cks
for file in geometry/*.dat; do make ${file%dat}cks; done
Then make geometry is up to date.
Do make mproper; make; make geometry and we get no differences.

Tried to implement an automatic GeometryTests target in the makefile
that is invoked if compiler or makefile is newer. This might help me
avoid the problem of these unknown cks differences. 

Fix object plotting nonappearance in fluxexamine by adding an accisflush
call before eye3d. Improve the control possibilities of the -gc wiremesh
plotting to allow parts of the mesh to be cut off.

26 Nov 14

Clarified some descriptions of position data in fluxexamine.

We do not seem to have correct accounting for the flux through the
cubes in geomrecess.dat. It ought to add up to zero. It
doesn't. Therefore I'm looking into it some more. The tallyexit is now
triggered by icross_geom. Which looks for all crossings.  It uses
cubenormsect which gets all the crossings.  However, tallyexit instead
calls objsect which calls cubefsect.  cubefsect has a short cut in it
that ignores cases that are both outside the cube, based on
inside_geom tests. Actually this shortcut seems necessary since the
call of cubeusect says it must have one end within the cube.  This
logic seems to be erroneous in the general application of
objsect. cubefsect is also called by potlsect, which is used for
setting cij. In that context the shortcut probably makes
sense. objsect is called nowhere except by tallyexit. The fsect routines
appear to be useful only for potlsect where the chords are along the
legs of the mesh.

Moved potlsect to fsects.f where it logically belongs. Now there's
nothing in intersects.f that calls the fsects.f routines. There remain
the objsect calls of fsect routines in fluxdata.f. But those appear to
be logically inadequate and ought to be replaced. The crucial thing
that cubefsect does is to return the ijbin. That's what is needed for
the tallying. Nothing in intersects.f calculates ijbin now. What I
need is a replacement that calculates ijbin in general based on
possibly multiple intersections and does the appropriate tallying.

cubeusect is where the restriction to inside/outside occurs and the 
calculation of ijbin. What we need is a more general version of cubeusect
that removes the restriction and returns more general ijbin information
about possible multiple intersections. cubeusect works by finding the 
crossings of six planes and choosing the one with minimum fraction from
xp1. That seems to be where the need for it to be inside arises.
The generalization I need is that we should find the intersection with
each bounding plane that lies within the others (as cubenormsect does).
Then for each find the ijbin for that. For a cube there are at most 2
since it is not reentrant. The determination of ijbin works by finding
the fractional position relative to the planes in the other dimensions
and multiplying by the number of facets in those other dimensions.
That should be encapsulated. Did that. Quite easy, but only for the
cube like cases.

Now implemented alternative test for intersection like cube normsect.
This does not quite give exactly the same result from diagnostics.
I don't quite understand why. Removed the minimization test. There are
then lots more evaluations of ijbin. This transition is not quite completed
we now need to properly account for multiple intersections. Probably 
we need tests to insure there are no more than 2. 

Since potlsect uses the fact that the minimum fraction is returned by
cylusect, there is a bit of of a problem with the rework to make it
consistent with the prior version. cylnormsect does the necessary sorting
too. Perhaps I ought to use that.

I notice there is a bug in cylnormsect in the sorting. Seems it is not
done correctly. Fixed but not specifically tested. Encapsulate into
a little routine insertsorted. We could then use that for cubefsect.
Now I see there is another logic error in cubenormsect that it can only
return at most one intersection. Actually spherenormsect was already
fixed to deal with multiple intersections, in objsect. But not the other
shapes.

cubenormsect is called only in intersects.f by icross-geom.
cubeusect is called only in fsects.d by cubefsect and pllelofsect.
cubefsect is called only in fluxdata.f by objsect and in fsects.f by potlsect.

29 Nov 14

Using geometry/geomcube now shows gaps in the edge mesh, which are fixed
by swapping back to cubefsect0. Fixed by ensuring fmin is set.
Now gives identical phi.

Ensure that in the files cijroutine and fluxdata that call potlsect
and objsect, the ijbins are allocated arrays so storage is available for
multiple intersections. This is fine. However there are issues to do 
with rank mismatches if we make potlsects's declaration ijbin(*).
My hope is that we can make a gradual transition making some sects
arrays and leaving others as is. Rank mismatches prevents this within
the file fsects. So made all declarations of ijbin that need it into
arrays in file fsects.

Separate cubeusect and ijbincube into two calls so one does not call
the other. No. That doesn't work because of the need for the face
information. So add the imin as a return from cubeusect. Has to be an
array.

Start to modify objsect to call cubeusect and ijbincube directly.
Comparing with cubefsect, I see that the intersection points are all
the same within rounding as the cubefsect, but there are more found by
cubeusect (as expected).

Running isecttest geometry/geomcube.dat
I find that cubenormsect is in error finding the intersections. 
Fix that after some effort. This same error was present in the new
cubeusect and probably explains the incorrect results.

Tested and debugged insertsorted[2].

Now for cubes:
objsect uses cubeusect, but does not yet account for all intersections.
potlsect uses cubefsect which calls cubeusect (and ijbincube). 
icross-geom uses cubenormsect (twice) which is independent.
I could presumably get rid of cubenormsect at this stage if I wished,
provided cubeusect really agrees. However, cubeusect requires extra
array for imin.

cubeusect does not use iobj or ijbin and they could be removed from 
the argument list. Did that. Now cubeusect and cubenormsect are 
functionally equivalent except for the extra imin argument and sort
that cubeusect has. 
 
Found that there are some funky xn1,xn2 orders in fsects that ought to
be checked in their usage.

cubefsect is now used only for potlsect, not by objsect in fluxdata.f.
ijbin is not used in potlsect, and need not be calculated by
cubefsect.

I am now evolving toward a situation where (at least for cubes) the 
fsects.f code like cubefsect becomes highly simplified since it is 
only needed for potlsect. But there is a separate routine ijbincube
that calculates the facet number of an intersection. It requires the
face (imin) to be returned by prior cubeusect. It would be nice to 
do away with cubefsect entirely. Make the potlsect cube section
an explicit of the simplified cubefsect. Now we've removed cubefsect.

I do not think ijbin needs to be an array. So I ought to reverse that
change. Not yet done.

commit  a2249ac36e4edb299da7922e2531f92700e7fec1

30 Nov 14

Add restriction to only 2 intesections in cube.
Now we need to code the handling of multiple intersections in the
objsect code in fluxdata. Seem to have done that.

Now we need to sort out the logic of tallyexit and leaveregion. 
Tallyexit needs to tally only those crossings that occur prior to
leaving the region.

leaveregion appears to have a shortcoming in that it examines the objects
of the boolean block in the order of their index. It chooses as the 
relevant crossing the first that it finds. That might not correspond to
the lowest fractional value. It depends on the object order. 

Modified it to sort in order. 

2 Dec 14 

Observed in ./coptic geometry/geomcube.dat -gf1 -gw256 a step to face
23 that crosses twice. In the diagnostics that is explicable. Adjusted
fraction in objsect so that it ends at the first crossing and can see
that happen. However, then the face number is incorrect. 
This reveals a problem in objsect. It is that if the region excludes 
the object then we should tally only the first crossing, whereas if
not, then we should tally all crossings. How do we tell?

First clean up tallyexit by passing xprior rather than recalculating it.
Then remove dtpos from tallyexit and objsect. No make changes.
Remove separate x1,x2 from objsect by using x1,xi passed.

One possibility for fixing the tallying is that leaveregion _changes_
the final particle position if it leaves the region to end at a place
just outside the place where it leaves the region. That means resetting
xi to correspond to a fraction a tiny bit larger. Unfortunately that 
does not work because ijbin is then found incorrectly for a radius that
ends up being exactly 1. This is because the number of intersections
drops to zero. So instead just pass fmax to tallyexit and objsect,
and discount fractions which exceed it. Seems to work.

Now working on spherefsect. Encapsulate the ijbin calculation into its
own routine. Done. Replace spherefsect in fluxdata.f objsect by the
equivalent calls. Done.

Commit. We need to track some of these changes.
b2155a858e8474c355640874db76e5bcdd39cf61

Now replace spherefsect in potlsect. That makes it now unused. Remove.
Remove now unused cubefsect.
Perform the same replacement on pllelofsect. Multiple crossings make
the new code fluxes different. Crosschecked by switching them off
and getting the old result. So correct. pllelofsect removed.

commit

Ran ./coptic -s200 geometry/geomppiped.dat
and found that the flux tended to zero on average as it should for this
non-absorbing object. There are of course substantial fluctuations.
Looking at the floating potential is a reasonable way to see the small
total flux.

I think this now ought to be able to tell if the cubes on the edge of
a cylinder are sensible.

make geometry fails geomcube, geomcubic, geommany, geompiped, geomSoR,
geomtest. Most of those are expected. (Maybe not geomcubic,SoR)
Remove .cks files. Recreate.

4 Dec 14

Replace the explicit contravariant normalization in cylgfsect by 
xp2contra. Gives the same result. Since this code returns ins1 and ins2
zero if we are outside the _unit cube_ then they zeroes indicate outside
the unit cylinder too. But 1s do not necessarily indicate inside the 
unit cylinder.

Rework fsects.f and fluxdata.f to remove cylfsect cylgfsect.

Now everything is done by calling cylusect direction. We must change 
cylusect to produce a sorted list of multiple crossings, and the faces
associated with them, like cubeusect. 

Remove nsdim from the argument list of cylusect.
Separated the ijbincyl part of the code into its own routine.
Removed cylgfsect, cylfsect.
Removed ijbincyl from cylusect so it is only called explicitly and only
by objsect in fluxdata.f
Not quite finished inside cylusect setting the ordered faces, etc.
Also not decided sd direction in objsect. Fixed that.
Removed unused sd(min) from cylusect argument list.
Got new cylusect version with sorted fmin going.
Removed bits of old one. 

Found a bit of a problem with objsect and whether sd has been set correctly
or not. Cyl code needs a value of sd set. But if so, then so should the
cube code, but it's not broken, apparently. That was a test in objsect
that no longer makes sense and is removed.

I think we now have the cylinder code migrated over to use the new approach.
However, I have not examined the intersects for rationality.

commit.

Ran isecttest. Looks ok.
Looked at some intersection plots and could not see any problems.

srvsect has not yet been migrated.

geomrecess.dat gives nsect multiple messages as it should.
However, the flux cancellation does not look very good. I'm not convinced
it is working. Ideas: does fmax prevent proper accounting? We need some 
more diagnosis. Found that sd was not being set correctly 
Rationalized the sd setting process. Now we are really getting zero
for separated cubes (no subtraction) even with large step size.

Eventually getting back to geomrecess we now see something that is not
so obviously wrong.

Discovered that the cij setting is wrong. Appears not to be transforming
the cylinder to normalized coordinates correctly. Yes. Found the error
and corrected. 

Slimming down the cube thickness I start to get substantial imbalance
again. Taking away the cylinder and boolean restores reasonable balance.
Especially with substantial drift.
With cylinder but no boolean. x-cubes (2 and 3)
   10.44    1.06    4.06  -13.81   -0.31   -1.31
    3.44    0.38    2.44   -3.56   -0.44   -1.25
With boolean we get some fmin>max indicators and 
   -8.69   -1.38    0.62   17.56   -1.69   -1.38
   19.75   -1.44    0.12  -14.12   -1.00   -1.31
Running longer cases it's completely clear there's an imbalance. And it's
there for all the cubes. 
The imbalance is there for bigger cubes but maybe not quite as bad.

Trying some cubeincube tests. Can't see a big inconsistency. Perhaps
it is the complicated boolean that causes problems, or perhaps it's
the cylinder. 

Introduced a boolean difference between two cubes, plus a third very thin
measuring cube that I can move around. This seems to show all is well
with a cube subtracting a rectangular hole out of another cube. The net
flux through the subtracting cube is close to zero. The plane measures
what it should when near the other surfaces.

Swapping the main object for a cylinder instead of a cube gives a substantial
error in the flux across the bottom of the subtractive cube. All objects
are non-boundary potential setting. There appear to be fluxes to the
measuring plane well inside the particle inclusion region. I don't
understand what denexamine shows. I need to understand whether/why there
are particles in the non-boolean region.

6 Dec 14

The way to diagnose just the particle density is to use diagexamine which
doesn't have all the compensating Boltzmann electron compensations.
The density is zero inside but the cellcount is not. The reason is that
the density is normalized by volumes and for zero volumes is put to zero.
So look at the cellcount.

Cellcount is zero inside cube. Not inside the cylinder. Clearly there is
a leakage problem. 
It does not seem to be present for a simple non-subtracted geomcyl.

Confirm with 64^3 mesh. Definite leakage with cylinder and
subtraction. Not present for cube. Not present without subtraction.

Can confirm the particle leakage with a diagnostic in padvnc.
Looks as if the leakage is a failure of leaveregion, since it appears
that icross is nonzero for the particles entering the non-region and
becoming leaked. Found a logical error in cylnormsect.
Replaced cylnormset with cylusect. This makes the leakage go away. 

I believe it is fixed, but I'm leaving in padvnc the leakage test.

The geomrecess results now look pretty plausible. No sign of leakage.

commit. push.

Replace cubenormsect with cubeusect in icross_geom. So now cubenormsect
is not used.

Work on srvsect to try to use it for everything: make it return ids,
use insertsorted2 rather than incrementing icross_geom. Seems still to 
work. Encapsulate ijbinsrv. Beware that from fsects, the chord ends
are in world-units, non-normalized.

Started to implement in fluxdata objsect routine. There are a few differences
between srvsect and srvfsect. Some rounding errors are to be expected, 
but there are a few cases that srvfsect does not find and sets fraction 
to be 1. These all seem to have two fmins. sd is set differently of course.

It would be good to understand these differences because they might not
be benign. 
If sd is allowed set by srvfsect. Then we get the same floating potential
as before. That is presumably because the binadding adds nothing if sd
is zero from srvfsect.
Restricting fmin to le fmax does not make any change of floating, and does
not prevent the sd recalculation from changing things.
Removing srvfsect entirely gives the same result as calculating sd 
in objsect. Thus it is only the sd difference from srvfsect that is
changing the floating potential. 
Built a diagnostic plot for the srvsect intersections. 
Seems to show that crossings are being correctly detected.
The crossings that srvfsect misses appear to be real and ought to be 
counted.

Basically I think that fluxdata.f has completed its transition to
operation without srvfsect.

10 Dec 14

Replacing srvfsect with srvsect in potlsect causes various errors. 
Getpotential and Getfield errors and very slow volume calculation from type 6, 
and Getfield no good vertices from type 7. 

The getpotential errors are type 2 which means fillinlin was used. 4 or 6
is the value of imissing. 

All these seem like problems to do with finding no or few vertices. I
assume that this problem arises because srvsect is so much better at
finding intersections than srvfsect.

However, the Used No of pointers:       32768  of       32768  points.
makes things look worse. There's something very bad with the setup.
Confirmed using -gt.

Things are not good with srvsect in potlsect. Theres a bug.
Yes. f(1) needs to be 1. for potlsect, when there's no intersect.

Fixing that I get
Used No of pointers:        6595  of       32768  points.
which is more sensible.
No more getpotential errors. However, still some Getfield No good errors.
Put in zero field fallback for this so calculation completes. Then:
srvsect:
 Flux density*r^2, normalized to rhoinf  0.358004034    
 Floating potential=  -3.86594462    
srvfsect:
 Flux density*r^2, normalized to rhoinf  0.381870955    
 Floating potential=  -3.80140615    
Looks as if the results could be compatible.
At the very least we now see that using srvsect we have a useful test case
for developing better No good vertices code.

It turns out that the no good vertices reports are arising from not 
excluding the particles from region 2. If the boolean excludes 2 as 
well, they go away. Things are therefore not terribly bad.

The fallback of putting the No good getfield value to zero is actually
physically reasonable since No good occurs for highly reentrant regions
surrounded by genuine boundary conditions. If the BC is a fixed potential
then it is reasonable to approximate the field as zero. 

That leads me to think that perhaps one ought to consider setting the 
field from the difference in the boundary conditions. That would be one
option. Another might be to use the potential on the other side of the
boundary even though it isn't in the region. There are several options.
One might even imagine detecting what the BC type on the boundary is
and making a decision based on that. But getfield doesn't have much
information.

Summary. 

We now have srvfsect removed from usage. 
The geometry test gives some No good vertices when particles are not excluded
from inside object 2.
The fallback for No good vertices has been changed to set the field to zero.
This is a reasonable choice for cases where the point under consideration
is inside a narrow reentrant area with fixed potential. 

18 Dec 2014

In a big run with 32 processors we see a handful of particle leakage
events. A nice way to diagnose them is using the velocity in diagexamine.
At least one of them is through the side of the cylinder away from all
other objects. Therefore it implies there is a way for a cylinder to
leak. It seems likely that this issue arises in spheresect (as applied
to a cylinder). Ideally for a point that happens to be exactly on the
cylinder, one would want the routine to yield an intersection once for
either the step to it or the step from it. I'm not sure the code does
that. B and C become zero with corresponding points on the sphere.
I might have to construct a testing routine that examines the results
of adjacent steps that happen to be right on the cylinder. I think in 
order to leak there needs to be a situation where neither step indicates
intersection.

Cylusect examines sds and if non-zero the return value of f1 and f2,
which must be >=0. and <1. to count. The order of returns is irrelevant. 

I wonder whether the radial shortcut is the reason. Perhaps there's a
way that the first point is to spheresect's view exactly on the cylinder
but to the radius calculation is a tiny bit inside. Remove that possibility
by making the radial test just outside the cylinder.

Add control of minimum cell count -m switch to diagexamine.
I think this ought to be a float not an integer. It's not quite right.
Add a small amount to it to avoid divide by zero.

Commit.

Using the new diagexamine code on the loki 90 and 45 runs. I see there
are leaked particles on both. So the tweak on the spheresect has not
prevented leakage. It's only a couple of particles, but it would be 
really nice to stop it entirely. The radial shortcut was not the (only)
problem.

Try the following change:
c         if(C.gt.0. .and. B.lt.0) then
c Discrim<|B|. Can take minus sign and still get positive. 
         if(C.ge.0. .and. B.lt.0) then
c Discrim<=|B|. Can take minus sign and still get non-negative. 
It ought to yield an intersection when the particle starts on the sphere
and moves inward. Previously that was excluded.

Built testing/spheresecttest
There appear to be various problems with systematic steps around the
sphere. Although the above is probably a step in the right direction,
there are still peculiarities near the sphere that this testing 
reveals. These are not easy to sort out as long as the code is 
asymmetric between the two ends. That's built in, since f=1 indicates
no intersection, but there are other inconsistencies between the C
value and the f values. Some fs are zero without C being exactly zero.
On outward going steps, this leads to wrong ordering with zero being
f2.

The problem appears to be if C is extremely small, then the
discriminent becomes =B and the f becomes zero, even when C is
negative. That can be prevented by an explict test for fdiff=0
perhaps. But maybe there's a way that f can be obtained more
accurately when C or D is small by an expansion.

Rewrote spheresect in a totally symmetric way solving for the
parametric position wrt the centroid, then adding 0.5. Also using
only the f values to decide the sorting. It then seems to perform
much better on the spheresecttest provided halfmin is taken as
exactly 0.5. Also the sumtol can be taken to essentially zero, 
since we seem now to have exactly complementary results from swapping
xp1 and xp2. I think it is fixed.

21 Dec 14
Sorting out fluxexamine.
There are several different types of plot.

1. Spatially summed flux number for time evolution.
2. Time averaged flux.
Those by default happen for all the objects. They are emitted by the
code in fluxdata.f, governed by positive passed flux quantity. 
Inside fluxexamine, iplot=this flux quantity.

3. No of particles vs time
This is turned off only by -q iquiet. Unfortunately, -q also turns off
flux averaging by setting iplot=0. That's what I want to stop.
I do so by changing the switch to set -i-9 and -p-1 as well as iquiet.

4. Force-3. 
This is turned off by iplot zero. But then iquant=0 turns off
the flux averaging. So that's incompatible. It is also skipped if
this object is masked. So that's the way to mask out those plots.
However, masking avoids printing the forces. So we don't do this under
-q. In fact the plots are plotted in that case, but no pltend is called
so the plot just flashes past and you hardly notice it. 

5. 3-D flux plot of objects colored.
This is performed by objplot. It can be turned off by -i-9. Which sets
iosw outside the plot range. 

If we want the fluxexamine code to be able to emit no printing except
for some values that it chooses, there's a problem. It is that fluxaverage
always prints the averages. However, probably we should not sweat that.
Just print out what we want later. (And look inside fluxaverage for how
to access the data.)

22 Dec 14

Implement ijbinindex function that can be passed object, iface, i1,i2,i3
and returns the ijbin. Implement -g switch which uses this to print out
the value of a facet. Multiple such switches can be given.

24 Dec 14

Phiexamine tickles a new bug in accis. It is that if color setting is
called in the form of (e.g.) accisgradset. That calls accisinit if
the display has not already been inited. Then vmode is not passed back
to the fortran side and pltend() thinks it is called improperly.
Fix by making all calls to svga set vmode.

commit

Had a particle leakage on X1 with 4 processes. That's not fixed in the
latest build.

29 Dec 2014
Found that in accis there were several files that were not properly 
added to the git repo. pltinit.f Makefile configure. Don't know why
but added them. Then git pull on sceptic machine gives a working system.

Discovered that numprocs is equal to 1 inside psumreduce. That explains
the problems I had with stability of -rx runs. As the number of processes
increases, the effective value of -rx does too. Eventually it becomes
crazy. 

The reason is that numprocs is passed to cmdline and a default set there
AFTER nprocs has been determined and used to set numprocs in coptic.
I shudder to think what has been happening to the rhoinf calculations.
This confusion is partly caused by the fact that numprocs in mpibddy is
not referred to through the common partcom.f where the rest of the code
gets at it. I wish it wasn't there. It would be more consistent to use
myidcom.f as the only place: nprocs. Let's sort this out.
First rename to nprcsses in mpibbdy and nonmpibbdy. 
Next remove numprocs from the cmdline code. It was never even documented.
We still have two things numprocs in partcom and nprocs in myidcom.
It's not easy to get rid of one of them.

Actually numprocs is used only in benign initialization code except 
for the psumreduce place. That's why nothing went wrong before.

It's still going to be the case that -rx should be set no larger than
Ti for stability.

=======================================================================
Summary for Calendar 2014 of code developments.

Replaced the random numbers with Sobol numbers in volint.
Implemented ndimsdecl.f
Replace most explicit 3-Dimensional uses of ifull( ).
2-D version work begun but not completed.
Made rhoinf an argument to psumtoq to remove partcom dependence.

April 
Implemented multiple species (electrons). 
Move charge deposition into the particle advance rather than having
a separate chargetomesh routine. (To enable multiple species.)

May
Multiple-species flux accounting.
Implemented Tperp different from Tpara, maxwellstats.
Implemented streamset script. But it's not terribly useful.
Got drift electrons working.

June
Replace RAN1 with RANLUX. Much more reliable random number generator.
Update reinject.f orbitinject.f.

July
Implement object subtraction with potential boundary control.
Implement Surfaces of Revolution.

Aug
Change to icross-geom tests from inside_geom tests for object intersections.
Standardize to mpif77/gfortran compiler with optimization.
Unformatted fortran write/read of gfortran is the same as pathscale.

Oct
Added the ability to have a fractional Boltzmann species to help stabilize
electron pic. It damps out the long-wavelength noise.

Nov
Implement automated cks checking.
Rationalized the object intersection code to remove many duplicates.
Corrected/simplified the flux tallying and fsect.f as part of this process.
Rewrote spheresect in a symmetric way.
Most particle leakage is now fixed but there are still occasional cases.

Dec
Fixed numprocs=1 bug in -rx usage.

==========================================================================

1 Jan 2015

Implemented f2vx 2-D distribution diagnostics accumulation for cases
where nptdiag==nsbins. The 2-D distribution must be substantially 
coarser. Therefore it only accumulates in the coarse case, where,
incidentally, uniform bins are used for nsbins. Normally nptdiag is 
much larger than nsbins, and would become unwieldy. 

pex writing now includes the extra data which can therefore be got back.
I think the reduce code ought also to be working appropriately but it
is not significantly tested.

The partaccum.f plotting has some rudimentary display of the data, but
needs more work to make it sensible and to control the display.

2 Jan 2015
Added -pu switch to coptic and partexamine to set the otherwise large
nptdiag to be equal to nsbins and hence do uniform bins. At present
nsbins is not itself setable. 

Improved the 3-D plots of the 2-D distribution functions in partaccum.

Adjusted the accis makefile in ~/accis to do the coptic/accis sync by
rsync instead of git pull. This prevents creating any secondary repo
in the coptic/accis directory and keeps things clean by using src/accis
as a staging area.

Method to make sure that you have all the files of a particular build
present:

git checkout -- .

Note that simply   git checkout master  does not do the same. It does not
renew files that have been deleted or are otherwise absent.

Implemented individual save statements in mpibbdy to enable removal of the
general save statement and hence avoid the warning arising from the mpif.h
statements.

Also in mpibbdy, change the behavior to do a reapportionment of the 
processes only if the number asked for is greater than the actual available
processes. Then ask for 100x100x100 by default (ridiculously large). 
That enables one to specify fewer blocks than processes if one wishes,
using the -id flag. It appears to work.

Now make gives NO warnings. And the cks checks are all ok.
If compiler is changed to g77 then also no warnings, but cks fail.
Explicitly set SHELL in makefile to bash. It was previously sh by default.
Either one works fine. 
Fix a few small bugs in testing/.

commit.

The long startup times for large meshes are caused predominantly by 
the boxedge calls in cijroutine. There are 2**ndims calls to boxedge
for each point. And boxedge itself calls potlsect about 12 times (once 
for each cube edge). Consequently there are about 100 potlsect calls
for each mesh point. That is confirmed by gprof. 

It certainly does not seem necessary to have 2**ndims calls per point.
As far as I know the only thing that the boxedge stuff is now usefully
doing is to set the pointer for any point which is part of a box that
has an intersection. This is necessary to enable the presumption that
any box corner whose pointer is unset can be assumed to be in the same
region as any position in the box. However, this additional box setting
needs to be done only once per box, not repetitively. Therefore a routine
that calls boxedge and if it returns intersections sets all the pointers
of corners of that box, would greatly reduce the work in cijroutine.

There's a question as to whether iflag_cij must be set. It looks as if
the only thing we must have is idob_cij(iregion_cij,int(cij(1)) in
gradlocalregion. iregion is not set by objstart. It it initialized in
iregioninit called by coptic, so no problem. 

----------------
Discovered a bug that appeared on Dec 2nd in the standard geomcubic.dat
test with extra pointer regions being set. It occurred between commit
b2155 and 102... The complete removal of spheresect...
commit 102d49374dda2fe7b0654f842e6a909bbb0873aa
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue Dec 2 15:31:54 2014 -0500

    Completion of removal of spherefsect, cubefsect, pllelofsect

commit b2155a858e8474c355640874db76e5bcdd39cf61
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue Dec 2 13:45:22 2014 -0500

    Continuing rework of the intersection and flux tallying code.

That's when spherefsect was replaced in potlsect. Seems the likely culprit.
Yes. It is fixed by 
               if(fraction.gt.1.or.fraction.lt.0.)fraction=1.
immediately after the spheresect call. In potlsect fraction must be 1
unless there's a real intersection. That's different from spheresect's
convention. Seems fixed.
----------------

Implement newbox code in cijroutine. It sets the pointer in trailing
boxes that have intersections. After debugging, it gives the same results
as the old treatment. There was some trickiness with oi_cij. Unfortunately
it then fails several geometry tests. 

Some of these failures are to do with the spheresect bug I just fixed.
So I need to go carefully. Recreated the .cks files with the fixed
spheresect but prior cijroutine. Then we get no differences. 

Then switch to the edited cijroutine but with newbox turned off.
Ok that stays correct. No differences. 

Now switch on newbox. There are differences in these only:
geometry/geomcyl.cks:
geometry/geommany.cks:
geometry/geomSoR.cks:

geomcyl.dat give box recuts that are not present with newbox=.false.
Same with the other 2. That's the cause of the differences. We are not
doing the mesh shifting etc with the newbox code.

This is then the time to ditch the recut code. Removed the above 3 cks
files and remade them with the newbox code. Make the newbox the default.

commit.

gprof shows the potlsect calls drop from 100M to 17M. A factor of about 6.
The time for startup improves by a similar factor. Yeah!

7 Jan 2015 
With new cijroutine got a lot of No Good Vertices errors. However, it's
not clear whether the issue is reall the new code. It can be avoided by
shifting the mesh. I now don't have the test for mesh shifting etc.
Perhaps that's part of my problem.

These are with the geomrecessed configuration. They occur at positions 43 and
85 in the radial direction, on a 128 mesh from -3 to 3. 
x(43)=-1.0157, x(44)=-0.9685, x(85)=+0.9685. So this problem is at the outer
boundary of the cubes near the surface of the cylinder. The mesh spacing
is 0.0472. There is no nodal plane between the outer boundary of the cube
and of the cylinder where they are being subtracted. The problem exists
for 
2, 0,0,0, .88,0,0, .13,.15,.3,  1, 1,1,1 but not for
2, 0,0,0, .9,0,0, .13,.15,.3,  1, 1,1,1
For these two cases the cube face is at
1.01 and 1.03 respectively. This seems to confirm the thesis above.
These two x-direction locations are associated with y-values that are 
clustered near 62 and near 67, whose r-values are +-0.11811. That seems 
to be the first and last y-position inside the cube. 

The No good vertices arises from tests like
idob_cij(iregion_cij,icp0).ne.iregion
if(icp1.ne.0.and.idob_cij(iregion_cij,icp1).ne.iregion)then
It appears that this test will not permit one to calculate derivative
in a situation where a subtraction puts two surfaces close to each other.

It seems these tests are not logically correct for a subtraction
situation. We ought to use the boolean to decide whether one has
crossed a real surface. However, the pointer probably ought not to have
been set in the first place. And the -gt flag shows pointers not set
for the relevant positions. Therefore a test based upon cij pointer might
be able to filter this problem. No, I think the pointer is being set
by proximity to the y-face. If so, then this problem arises for positions
that are in thin region of the cube that extends outside the cylinder, and
also lies within a box that is intercepted by the y-faces of the cube.
The latter makes its pointer set. The former prevents its vertices from
being allowed.

Perhaps we really do have to look at the boolean, or possibly at the
fractions not just the pointer.

The way subtraction works is that inside potlsect, the fractions are set
to 1 if the lattice leg is across a subtractive boundary. And if fraction
=1, then conditions are not set. In fact arguably potlsect ought to be
(re-)used for an on-the-fly test to see if vertices are really not good.
Alternatively we could use the fractions already calculated.

More directly, if we draw a line from the field point to a vertex, and
find where it crosses an object, if that crossing is inside the
object(s) subtracted from that object, then it is not a crossing.
Also if the object is a subtractive object, then if the crossing is outside
the object from which it is subtracted, then it is not a crossing. 
If there are no valid crossings between the field point and the vertex,
then it is good. 


16 Jan 2015

Debugging infinite B case without collisions but with cross-field drift.
Found error in cinjinit xw not set correctly.
But there seems to be a more general problem that if the width is zero,
as it is when infinite B happens with a field tangential to a face,
then I don't see why cumprob would work properly.

Tracking this problem down. cumprob and grein look ok. 
Proved that this problem has been present since at least May 2014.
It's not new and has been present in the driftparticle treatment 
since it was implemented. Multiplying the x-face grein arbitrarily by
an enhancement factor causes the density to rise. However, it does
not remove the density _ramp_ as the x drift direction is crossed. 
It moves the whole thing up. So it does not seem to be just the 
reinjection at the x-face. It is present regardless of whether dimension
2 is periodic.

The sign of the ramp slope changes when the x-extent is increased beyond
-6. (-3 was the prior). This ought to tell me something. It appears to
be the x-z aspect ratio, because one can make things go positive by 
reducing z-extent too. With equal x and z length, there is no problem.

I discover that the infinite-B case is setting Tperp small and the
test in pinit then sets notseparable, which then calls colnreinject 
rather than cartreinject. Turning that off gives apparently correct results.
Evidently the collisionless aparatus is not working properly.
Prevent it getting called by mods to pinit for now.

It appears that the cdistcums are set incorrectly without paying attention
to the area of the faces. That's simply an error. 
Found how to correct it very easily in the colreinit, since fcarea
is already calculated. Then the problem is gone even if we use 
the collisional aparatus for infinite B. So revert to that.

20 Jan 2015

Tracking down cause of the slightly peaked z-density-profile.
The peaking can be changed by adding additional gasdev calls into the pinit
loop. This implicates the random number generator. 
Actually there was a slight problem in that gd_iset was never initialized
in gasdev before. Added that initialization. It changes the occurence of
peaking, but does not prevent it.
Increased the number of particles. It does not go away. 

Initialize as       call rluxgo(4,myid,0,0) instead of call rluxgo(0,myid,0,0)
changes again. Try level 1. Looks fine. 
Perhaps the luxury level zero is too optimistic. Here are the purported costs.
C                               default
C  Luxury Level   0     1     2   *3*    4
      DATA NDSKIP/0,   24,   73,  199,  365 /
Corresponds to p=24    48    97   223   389
C     time factor 1     2     3     6    10   on slow workstation
C                 1    1.5    2     3     5   on fast mainframe
Use 1 for now.

Also fix Boltzmann electron compensation off when using electrons.

It's important to note that the ranlux changes make all results now 
different. What used to be
 Flux density*r^2, normalized to rhoinf  0.844536364    
 Floating potential=  -3.00770116    
is now
 Flux density*r^2, normalized to rhoinf  0.909752071    
 Floating potential=  -2.93331695    
Also all the cks files need updating. I accidentally included 
the strange griddecl.f too, which needs to be regressed.

So I'm doing that in my sandbox and commit/push 

22 Jan

Fix bug in cmdline usage message with format of -ni.

23 Jan

Need to implement a more systematic compensation of the Boltzmann electrons.
At the moment, they are compensated in psumtoq by putting additional charge
in places that are outside the particle region. It might be better instead
to adjust faddu so that it simply does not put Boltzmann electron response
in such places. That would be perhaps a clearer solution.

Since faddu is called at each Poisson-solve-iteration, it needs to be 
rather efficient in deciding whether to include Boltzmann response or
not. At the moment, although the index is passed to it, the decision 
whether to use that index is made on the basis of ptchcom commons.

Chargetomesh (psumtoq) decides whether we are outside the region based
on the value of volumes(index). Unfortunately, this is not in a common
block; so it's tough to access for faddu. faddu needs to be kept clean
so sormpi does not need to know anything about it. So the challenge is
this:
faddu needs access to mesh quantities via common.
mesh quantities like volumes aren't in commons. 

The way this is addressed for point charges etc, is that in ptchcom.f there
are some mesh quantities: uc, rhoc, Tec, (and their single index equivalences
uci, rhoci, Teci). This common is available to faddu. This is the place to
put the needed information. At the moment the Boltzmann compensation is NOT
done in this way. It is done instead by adding charge in psumtoq. I think
this needs to change. We could, for example, have a mesh of values of 
boltzamp that is zero outside the particle region. This would have to be
set in initializations, like uci, rhoci etc, probably in setadfield.

Change boltzamp default to 1, and -sp switch to set it to zero.
./coptic -sp output is (still using old implementation)
 Flux density*r^2, normalized to rhoinf  0.867844701    
 Floating potential=  -2.98047614    

Change to new code fadcomp. Get the same flux/float for standard and -sp
cases. Make setadfield called always. Things stay the same. Ok. 
So far in the tests ./coptic uses the old faddu. ./coptic sp uses the new
fadcom, but with boltzamp=0, which just switches off electron
compensation everywhere.

Now there is difference between old and new implementations for -spb0.5
(say). In neither case is the psumtoq compensation being applied. 
The new code appears to give flat phi inside the object as it should.

There's a puzzle about point charges with influence beyond the particle
region. I haven't quite sorted it out. There is a compensation of electron
charge in what the rho is set to. That seems to be to null out the faddu
contribution. This can't easily be tested because it needs a point 
charge crazy close to the edge of particle region. 

This version gives
 Flux density*r^2, normalized to rhoinf  0.909752905    
 Floating potential=  -2.93331599    
when it calls with faddu. 
But
 Flux density*r^2, normalized to rhoinf  0.909752071    
 Floating potential=  -2.93331695    
when with fadcomp, which is the same as previously. So the transition is
good. 

Now the principle is psumtoq sets the rho equal to scaled psum or to zero
depending whether we are inside or outside particle region. (test volumes).
ucrhoset puts boltzwt to zero outside particle region. (test linregion).
to do that setadfield must now always be called.
fadcomp is called in all cases except when boltzamp is zero.
In that case, the faddu use is disabled by a switch to save time.

The way that behaviour ought to have changed:
1) With -spb we should now be zeroing the external charge correctly.
2) We are now using an non-linear fadcomp for Boltzamp cases, instead of
a linear approx. This might cause problems.

I realize there's an issue that is not new. It is that the relaxation in
sormpi is adjusted by the amount of additional function. However, it looks
as if this is going to be correct. So I do nothing to the oaddu code.

Regenerate .cks files.

20 Mar 2015

Reinstituted the sorgen omega fix that is better for convergence.
This causes all the cks files to be changed (naturally).
There were some other tests associated with large cases that were done
but the hacks were removed prior to testing. 

Regenerate the .cks files.
Commit.
Fix electron and outside-region initialization bugs. 
Commit.

This changes the geometry tests on quite a few files. 
Regenerate the .cks files.

Now there's the question of the integer number of injections.
Implement ninjadd as the optional additional injection in padvnc.
Then tests use ninjcompa(ispecies)+ninjadd. Set to zero initially.

22 Mar 2015

Implement partial injection parameter pinjcompa.
Use it to decide randomly at the start of padvnc whether to use
zero or one for ninjadd. Seems to work. 
In effect this is now a corrected quiet injection to the extent that
the average injection rate is constant within one of the same
(rather than fluctuating with a Poisson distribution) 
but it now has an accurate mean value for low injection numbers.

This should break all the tests again. And it does.

Regenerate the .cks files.

Commit.

2 Apr 2015

Fix bug in moveparticle that the theta is negative for electrons but 
the test was theta.le.thetatoosmall. Make abs(theta).le.thetatoosmall.

regenerate .cks files. Commit. Push.

3 Apr 2015

Quiet initialization. Can be accomplished by dividing the volume into
blocks, injecting a fixed (plus 1 fraction) number of particles into each
block, then subtracting their average velocity-offset to avoid flux 
noise. 

One needs some thoughtful algorithm for block choice, since, for example
Haakonsen is doing quasi 2-D calculations. Perhaps the number of blocks
in a particular direction should be no larger than the number of cells.
Other than that, it would make sense perhaps to make the block aspect
ratio near 1. In other words, the number of blocks in a particular direction 
should be proportional to the length of the side in that direction.
How many blocks should there be?

8 Apr
Bugs Status

----------------
On 27 Mar Christian reported this as being the cause of the insulating 
surface problems:
diff --git a/fsects.f b/fsects.f
index 24b1ce4..6a0160f 100644
--- a/fsects.f
+++ b/fsects.f
@@ -112,6 +112,7 @@ c Convert to normalized.
      $                 /obj_geom(oradius+j-1,i)
                enddo
                call cylusect(xn1,xn2,i,nsect,fmin,ids)
+               call ijbincyl(i,ids,fmin,xn1,xn2,ijbin)
                fraction=fmin(1)
             elseif(itype.eq.4)then
 c Parallelopiped.
@@ -122,6 +123,7 @@ c Parallelopiped.
 c Non-aligned cylinder.
                call xp2contra(i,xp1,xp2,xn1,xn2,ins1,ins2)        
                call cylusect(xn1,xn2,i,nsect,fmin,ids)
+               call ijbincyl(i,ids,fmin,xn1,xn2,ijbin)
                fraction=fmin(1)
             elseif(itype.eq.6)then
                call xp2contra(i,xp1,xp2,xn1,xn2,ins1,ins2)


But soon after
 segfaults at
cijroutine.f line ~1100: "a=obj_geom(oabc,iobj)", where oabc=2 and
iobj=1065353216 (presumably incorrect).

And on 31 Mar he reported:

The segfault seems to be caused by the difference between oihere and oi_cij when
idob_cij is being populated; that difference is ignored when reading the data. I
patched it on the reading side (see below), but I don't understand this part of
the code well enough to say whether that is the right way to fix it/how robust
it is (for example, the local value of ichain is now ignored).

--- a/cijroutine.f
+++ b/cijroutine.f
@@ -1044,7 +1044,7 @@ c Object information
 c-----------------------------------------------------
       real tiny
       parameter (tiny=1.e-15)
-      integer oisor
+      integer oisor,oip
 c The total areas and flux counts of all flux-tracked objects
       real totarea(nf_obj),totflux(nf_obj)
 c Probably this ought to be set up in a common. But for now:
@@ -1099,8 +1099,9 @@ c For each direction in this dimension,
      $              .and.dob_cij(ioad,oisor).ge.0.)then
 c We intersected an object in this direction. Adjust Cij and B_y
 c Get back coef
-                  iobj=idob_cij(ioad,oisor+ichain)
-                  coefoa=dob_cij(ioad+2,oisor+ichain)
+                  oip=idob_cij(iextra_cij,oisor)
+                  iobj=idob_cij(ioad,oip)
+                  coefoa=dob_cij(ioad+2,oip)
 c These must get the information from somewhere.
 c Assume that a and b are unchanged by the variability:
                   a=obj_geom(oabc,iobj)
@@ -1111,7 +1112,7 @@ c-------------
                   if(i2type.eq.4)then
 c                     write(*,*)'Insulating',ifobj
 c Address the flux data
-                     ijbin(1)=idob_cij(ioad+1,oisor+ichain)
+                     ijbin(1)=idob_cij(ioad+1,oip)
 c Pull the area of this facet into here
                      iaddress=ijbin(1)+nf_address(nf_flux,ifobj,nf_pa)
                      area=ff_data(iaddress)

8 Apr 
Implemented the ijbin calculation calls for all the geometry types in
potlsect. 

Not yet the oihere problem fix.


10 Apr 2015
Working on cijroutine.f

First let's tidy up cijroutine. Remove the newbox code that disables some
long obsolete code that did not work well. 

Understanding the cijroutine code. Because of the new boxedge treatment,
the chained extra data for flux-dependent intersections is not now 
necessarily immediately above the data for the intersection it was chained
from. However, the code in cijdirect appears to assume that it is.
That seems to be the problem. 

The difficulty now is that since cijdirect just iterates through all
the objectcrossing data, how does it know which of these data slots is
chained and which is an actual intersection. It ought to skip the
chained.  But how does it know? Well it does nothing if
idob_cij(iextra_cij,oisor) is unset: it just skips to the next.
So, since the data of a chained slot should have this unset, we should be ok. 
Therefore his patch is basically correct EXCEPT the skip has to be removed.

15 Apr 2015
Working on reported floating segfault I find that there are bounds errors
when potlsect is called in cijroutine. This appears to be because the 
nf_map(iobj) is zero. The object is not yet mapped when ijbincube is being
called. It really ought not to be called in the first instance when the
cijroutine is setting things up. So add a test to evaluate infobj=nf_map(iobj)

16 Apr 15

Further pursuing this problem with cij. First, I note that since fluxdata
is deliberately initialized before cijroutine or cijedge is called, the
immediately prior report can't be quite right. 

More significantly there appears to be a problem in cijroutine in that
it is implicitly presuming cij to have been intialized to zero. The
first thing that cij does is for any box that has intersections do an
objstart.  The first thing that objstart does is to test if cij(icij2)
is zero.  If it isn't then it is not set up properly because the
rubbish prevents it.  In earlier versions, this problem did not exist
because there there was special treatment setting cij(icij)=0 at the
start of cijroutine.  It looks as if the modifications I made to
rationalize and speed it up have broken it. 

Looking back I see that the new code is supposed to set only trailing boxes
that have already been examined. Hence it ought to be possible simply to
zero the cij we are working on initially. First I put a write statement
to tell if it's non-zero. Then I zero it if not. 

I think this is simply an error that has been present since Jan 2015,
and just needs cij(icij)=0 at the start.

Reset geometry/uniformelec.cks (broken by electron fixes earlier):

commit.

Doesn't solve the pathscale problem with segfaults. It appears that 
cijroutine initializes only nodes that don't include the boundary.
Then when cijedge initialzes the boundary, it breaks. Yes the mditerate
call for cijroutine uses ium2 and starts at an incremented pointer.
Therefore, the fix is that cijedge should always set cij to zero and then
do its job. Seems to prevent the segfault.

Implement. commit. push.

20 Apr 15 Problem with the theta.lt.thetatoosmall code. It appears not
to apply the electric field corresponding to -vdriftxB that is
needed. Now that electrons are allowed. We might be in a situation
with small drift of ions becoming non-negligible. It needs to be
implemented. moveparticle seems the sensible place to put the extra
acceleration, since it is not required for zero field or for strong
field treated via the drift mechanism.  But then it needs to be added
to the moveparticle arguments. Also we need to know which species'
drift field to use, but ispecies is not known in
moveparticle. Currently an integer is passed but not used.
Replace that with driftfields(1,ispecies). 

Tested with blankgeom.dat which has no objects but a 1.e-6 variation of
potential round the boundary to force the convergence to be quick.

23 Apr 2015
Correcting the cyclotronic mover to avoid electron instabilities.
Simple test case moverunstable.dat
# Mesh geometry
91,1,32,0, -0.025,0.025,
92,1,3,0, -0.5,0.5,
93,1,64,0, -0.05,0.05

# Cylinder
#1027,       1.,0.,0.,  -1.0,0.0,0.0,  1.,0.5,1., 2,  1, 1,512,1

# Particle region
#99,1,-1,0

# Potential boundary conditions
111
112
113

# Periodic particles in direction 2 (faking pot. with zero grad.)
98,4,4,4

# s steps length dt
Arguments: -s1280 -dt0.00004
Arguments: -ri1600000 -rx0.
Arguments: -l0.0065625 -Bz6. -v4. -vx-1. -vy0. -vz0.
# Turn on electrons
Arguments: -sp -zm-1024
# Use uniform distribution bins
Arguments: -pu100
# Save md diagnostics every a steps.
Arguments: -md4 -a100

This is unstable with the current mover.
Deleting the erroneous E kick in padvnc DOES NOT stabilize it.
However, deleting the ExB contribution to EB velocity shift DOES STABILIZE.
Conclusion: One must not add the grad\phi x B term to the vdrift.

Reworked moveparticle so that all of the move is in it and there are
three options. B=0, thetatoosmall=Boris, cyclotronic.
Drift (infinite B) is in the driftparticle routine, unchanged.

Both movers are now stable with the above input file. They can be favored
by adjusting thetatoosmall to either a giant or tiny value.

We are now back to a situation where driftfields are not used.

27 Apr 15 

Thinking about reported electron excess (ion deficit) at drift input.
Ignoring dtremain, particles are reinjected at the boundary and then
given a standard move with a dtpos that is a random fraction of
current dtpos, with dtp equal to zero. That means dtaccel is equal to
half dtpos. It mocks up a situation where the previous move was of 
negligible length, in which case the distribution ought to be correct.
The only ambiguity I see is that the theta is being calculated with 
dtpos in cyclotron mover and dtaccel in the Boris.

I think this is now a mistake. The cyclotronic mover should rotate the
velocity by dtaccel and the gyroradius by dtpos. So implemented that.

Constructed a bdrifttest.dat file and found a leading edge transient.
However, that transient is present even with zero B-field, so the 
case doesn't really test the bug reported. Instead it is something to do
with reinjection regardless of B. Lowering dt rapidly decreases it.
It is not affected by Boris versus cyclotron.

Notebook of 17 Aug 11 discuss the suppression of gyrating particle loss.
This is quite possibly the problem, and nothing to do with the mover
questions. However, this effect ought to be present with the Boris mover.

Created a test case and found that anything other than moving both 
v and g by theta in the cyclotronic mover produces a velocity layer
at the edges from reinjection. So went back to that. Basically I have
not made much progress on this problem. 

11 May 15
Fixed mditerset not to use itermults because that does not override NaNs.
Also explicitly initialized the uave and qave because of the same issue.
Included the updated driver vecglx that correctly does non-convex polygons.

12 May 15
Read the fluxfile earlier when restarting with flux, so as to enable the
fluxdata init to be with the actual needed number of total steps.

14 May 15

Change the boltzamp and boltzwt handling so that fadcomp uses their product
to set the compensation. That enables boltzamp to be ramped down with
time if desired. (Boltzwt then does not need to change). 

Now think about boltzamp. Does it make sense to have electrons that are
full charge weight PLUS a boltzamp response? At the moment that is what the
code seems to have. The Refmanual is not up to date on this point since it
says the boltzamp is linearized, but it isn't since we are now using fadcomp.
It looks as if the code was not properly completed for the transition to
fadcomp.

Changed achargetomesh so that echarge only applies to the psum, not
to the diagsum. Then we can allow non-unity echarge without silly things
happening. Changed the calls to achargetomesh to include a factor 
(1-boltzamp) in echarge for species 2. That means the number of electrons
ought to remain equal to the number of ions when boltzamp is non-zero.
But the extra charge is compensated by reducing the charge value on
the electrons accordingly. 

26 Aug 2015
Put step number print into reportprogress.

27 Aug 2015

How to run coptic with just electrons. The basic idea is to make species 1
electrons and turn off faddu. Boltzamp=0 implies faddu off. So simply 
using the switch -spb0. accomplishes that. However, we are not then
subtracting a uniform background. We need to be able to do so. 
Implemented automatic (separate) subtraction if boltzamp=0 and nspecies=0.
However that has the problem that it subtracts the background even inside
objects. That's not what I intended. Still it would work for an empty box.
So create a new psumtoqminus and call the alternate. This works.
Actually we now seem to have two parallel approaches to fixing inside
objects. This and boltzwt. Maybe ought to rationalize. Not yet.

Change -spb switch to -sb. 
Implement a test case with -zm-1 and positive object potential. This is 
then essentially a standard case for electrons and uniform immobile ions.

We have a working electrons in immobile background code.

Constructed phasespace.f in analysis. It plots phasespace points and
constructs and plots a phasespace histogram. 

29 Aug 2015
Fix bug in accis/slicing code tickled by 1d cases.
Implement boltzsign tracking and correct handling. This permits us to 
use non-zero boltzamp with negatively charged ions (simulating electrons).
At the moment we set the sign equal to the sign of species 1. This then
works for one or two species I hope. But not obviously for more. 

1-d settings with negative ions only now seems to work. Ran a 256 mesh
looks fine. Diagnostics are a bit strange since 3-D, but workable.

3 Sep 2015

Created phasespace plotting routine. Points. Contours. And energy histogram.

It's not obvious that the energy histogram is really useful. It is hard to
convert it into a true distribution function since the area by which to
divide is uncertain when potential varies. It gives f only when there is
essentially a uniform potential in the region sampled. 

Electron hole initialization ideas. 

We want to start with a self-consistent electron distribution. The
untrapped part of it should connect to the Maxwellian at infinity. 
The trapped part of it is a free choice. But the combined distribution
should be consistent with Poisson's equation and the potential that binds
the hole.

The most promising-seeming approach appears to be to start from a specified
potential. Its spatial variation can reasonably be taken as the sech^4 form.
Its peak value determines the width of the hole. If we choose a particular
f(E) shape in the trapped region, then we can calculate everywhere from 
the given potential shape what f(v) is. Possion's equation tells us what
the total trapped density must be as a function of x. But we don't really
wish to satisfy exactly either Poisson or the distribution being constant on
energy contours, because sech^4 arises only from the linearized approx of
a certain (parabolic) velocity distribution. Still, we could. 

What satisfying Poisson tells us is the total trapped density as a function
of position (potential). We might simply take a specified shape of the f
in the hole as being the way the trapped particles are distributed. 
Then the algorithm would be something like:

1. Choose the x-position on the basis of the (total) density distribution

2. Decide if particle is trapped or untrapped, knowing untrapped density.  

These two steps can be combined, eliminating any adjustment of the
position selection by choosing position from a uniform variate, and
then selecting one variate extra p such that there are three ranges
$0<p<untrappedden$, $untrappedden <p<total$, $total<p<1$. The third is
simply rejected, so we've done the spatial selection by rejection.

2a. If untrapped, distribute with external (Maxwellian) energy distribution.
2b. If trapped, distribute with chosen trapped v-profile. 

These two stages can be done using the same random number u (restricted
appropriately to the ranges). The external distribution is
f(u)={1\over\sqrt{\pi}}\exp(-[\sqrt{u^2-\phi}-\pm u_m]^2)
where u_m is the velocity shift. I could generate the cumulative
probability distribution as I do the untrapped density calculation.

The trapped v-profile could presumably be taken to be triangular if one
wished. Or parabolic. Or rectangular. One could also adopt a different
hole potential shape if desired.

Because the bulk of the density deficit in the trapped region is required
to reduce the density perturbation so that total charge is non-negative, 
it seems likely that the bit extra needed for the trapping potential 
might not cause a problem when it is not quite self-consistent. As the
trapped particles circulate in the hole after initialization, they will
produce potential perturbations because they are not quite self-consistent.
This will likely produce mixing within the trapped region (and maybe at 
the boundary). With luck, it won't be too bad.

I have a fortran routine in untrapden for calculating the untrapped density
from a shifted Maxwellian background. There does not seem an analytic form
for the untrapped density when there's a shift. Improved that to return
the cumulative distributions.

7 Sep 15

Wrote routine to construct a cumulative probability distribution for 
holes based upon the parabolic hole. It will return a range greater than
one for a flat distribution, which is presumably the largest one of interest.
Therefore if we scale the range so that the flattop at the peak potential
is unity, the rejection will be reasonably efficient. However, we ought
to reject particles based upon the total density so we don't call this 
relatively expensive routine unless we need to. We can still use the same
random number to interpolate the v for accepted choices.

I now seem to have hole initialization working hacked into coptic.
A 1-d hole in a 3-D box persists for 500 steps of 0.1 in a +-10 box.
(Uniform background -sb0). Single species. The species is positively charged.
The only thing that is changed is the particle initialization.
There is no quieting, purely random 800k particles.

10 Sep 2015
Implement switches for holeum and holeeta setting into ih.
holeum is the drift of the Maxwellian in the rest frame of the hole.
holespeed is the speed of the hole relative to the mesh. These are 
both in sqrt(T/m) units like vd etc. Only holeum needs to be set.
holespeed is calculated using vd. Updated RefManual.

15 Sep 2015
Discovered a bug/problem associated with rhoinf calculation when going
to smaller dt. The charge cancellation is clearly broken at the first 
solve. This appears to be because rhoinfcalc gives a more inaccurate 
value 4.9861 versus 4.9990 (-ri5). The number of reinjections per
timestep is 308 versus 772. So there's some rounding error arising from 
the small number of reinjections. There needs to be a more consistent
calculation. It's nothing to do with trapinit per se. 

Remove rhoinf initialization from pinit. It was being set incorrectly 
as if for a spherical domain, when rhoinf=0. Also initialize rhoinf to
zero in coptic. Changes no geometry tests.

The call of rhoinfcalc and determination of the rhoinf from the number
of injections, nrein, is problematic for small numbers of reinjections.
It also seems to be inappropriate when using a fixed injection rate
ninjcomp. In that case we ought simply to leave rhoinf at its fixed
set value in most cases, certainly not vary it according to the actual
number of injections, unless there's some other reason. rhoinfcalc does
account for the number of processes, because nrein increases with 
that number. So probably it needs to do something at least once.
Introduced a new calculation in rhoinfcalc when ninjcomp is nonzero. 
It requires adjustment of the acceleration code, which was anyway not
quite correct for partial injections.

Unfortunately this correction changes all the geometry files. So we 
need to regen them. Implement make target regeom to remake them.
And run it. 
Flux density*r^2, normalized to rhoinf  0.905194461

Now we have a problem with the hole initialization. It's been broken by
what I did. Neglected to remove the spurious initialization from trapinit.

6 Oct 2015
Implemented mpibarrier because there was a problem on loki with MPI where
if storedgeom.dat did not pre-exist the different nodes tried to read and
write it at the same time and became confused.

Discovered that the pathscale compiler does something really stupid with
block data.       block data com3dset includes       include '3dcom.f'
and inside that there is a giant ff_data which is 10M floats. 
It isn't initialized in that block data, because in gfortran that 
creates an object file that is very big. But in pathscale, even though it is
not initialized, it is included in the object file, which is then 44M big.
Fixed by replacing with explicit initialization.

26 Oct 2015
Working on reorganizing the directories. Introduce COPTIC_ROOT.
Found GeometryTests broken. Happened at
commit cbcbdda1d20bbf2ce2e9d8ccc226c51fd7a8876f
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Fri Oct 2 10:12:17 2015 -0400
    Improved parsing of array3read leading string for grid consistency.
Not surprising since the array3write string was changed.
make regeom.

Rework references to libcoptic and other source files.
Correct sortest.f to do call blockdatainit.

28 Oct 15
Rework early parts of coptic.f to encapsulate the default and parameter
setting and some restarting.

I ought to move ibool_part to partcom instead of 3dcom. That would enable
me to get rid of a lot of 3dcom dependencies. 

29 Oct 15
Moved ibool_part. But it needed several additional adjustments where ibool_part
appeared. So this was not a big step forward. (More like backward).
Move iptch_mask into ptchcom.f replacing iptch_copy everywhere. Still doesn't
much help. Disentangling this seems too difficult.

31 Oct 15

Exploring periodic particles without cijedge. A standard run has differences.
A one-particle run does not. I conclude that it is only when there are 
particles near the edge that the cijedge not being called makes a difference.

I find that the difference arises from interpolations.f gradlocalregion
and can be prevented by adding to this test:
      if(icp0.eq.0.or.idob_cij(iregion_cij,icp0).eq.-1)then
I believe this test is triggered when in the edge cell but only for the
electric field in the tangential direction.

Looking back in notes, I see 4 Dec 13 that I concluded that there was no
significant cost from edge nodes having their pointers set. I now believe
that conclusion was false because it neglected the tangential field 
multilinear interpolation which will try to access excluded fields. 
Indeed the multilinear interpolation of tangential fields appears to be
incorrect for periodic cases! (Probably no big deal; certainly no error
for n=3 dimension compaction.) 

It is only periodic particles that causes the domain shortening. Also
diagperiod exchanges particle and diagnostic depositions are based on
testing particle periodicity ipartperiod.

There's something strange in bdyshare with periodicity with idone(2).

We need to generalize this flag definition.
c Flags for which dimensions are periodic or absorbing for particles.
c 0 open, 1 lower absorbs, 2 upper absorbs, 3 both absorb, 4 periodic

A reasonable approach would be to designate which directions have
between-node boundaries by the second byte via values
c 0 neither, 1 lower between-node, 2 upper between-node, 3 both between
             bit 8                bit 9                bits 8+9
	     64	 		  128 		       192
However, periodic requires both already. Also periodic requires a different
charge transfer from non-periodic. 

Implemented setting. Implemented xmeshstart end adjustment.
Implemented partlocate adjustment. Implemented ghost diagnostic transfer.
Implemented cijroutine selective edge setting. Not set for periodic
or for half-node boundaries.

I believe I am now in a position to test the effects of omitting or not
omitting the cijedge setting on (e.g.) periodic boundaries. There are
some changes to phi for periodic conditions. But that's what I showed
already. I think they are the tangential components.

A 3-grid gives corrupt gradlocalregion. Actually the problems seem to 
be present even without periodic particles. Seems the sphere is
a problem. Remove it.

Test of 32x32x3 with periodic in small direction. A small (~10%) speed
up from removing the cijedge setting from the periodic direction but
not much. There's an improvement in correctness for periodic dimensions.

New
real    0m22.678s
user    0m22.630s
Old
real    0m24.596s
user    0m24.516s

The new version fails geometry test on baresheath and unform4.
Skipping the cijedge adjustment gives agreement on all cases. 
So the differences are just arising from the correction of periodic
particle treatment by the new code.

make regeom

Need to reintroduce TAGS:
etags * ../analysis/*
Then M-.

3 Nov 2015

New shortcut for getfield, bypassing getlocalgrad.
-s100 -ri1000
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 30.81     23.32    23.32 306886434     0.00     0.00  getfield_
 18.59     37.39    14.07 310015443     0.00     0.00  interp_
  7.20     42.83     5.45 407015274     0.00     0.00  inside_geom_
  7.02     48.15     5.32 308345599     0.00     0.00  gradlocalregion_
  5.89     52.61     4.46      100    44.60   718.20  padvnc_
  5.87     57.05     4.45 100616156     0.00     0.00  spheresect_

Total 75.67

Old version for same test:
 24.68     20.83    20.83 1228525933     0.00     0.00  gradlocalregion_
 19.72     37.47    16.64 306886434     0.00     0.00  getfield_
 16.03     50.99    13.53 310015443     0.00     0.00  interp_
  6.47     56.45     5.46 407015274     0.00     0.00  inside_geom_
  5.76     61.31     4.86      100    48.60   806.02  padvnc_
  5.27     65.76     4.45 98397734     0.00     0.00  achargetomesh_
  5.07     70.04     4.28 100616156     0.00     0.00  spheresect_
  3.83     73.27     3.24 103215478     0.00     0.00  partlocate_

Total 84.37
The shortcut does save over 10% of total. (On a uniform grid).
It also saves a lot more on the getfield total 23.32+5.32 vs 37.47
About 9 seconds of 37. => 0.76. It is worth it.

Also fiddled with the iinc calculation but found no improvement.

Unfortunately, make geometry is broken by the shortcut. It was rounding.
Rewrote in same form as original. Unbreaks it. Ok now.

The routine interp uses 18% of the time. Almost all of it is from partlocate.
This could be substantially reduced if a more efficient location were used.
One possibility is to have an inverse lookup table based upon the fractional
value of position as index returning the corresponding grid index. 
It would be natural to put this in meshcom. If the lookup table is fine
enough, then looking up the p-index below and above will return the same
value and no bisection will be needed. Then interp could just be a fall-back
hardly ever called. 

Implemented a reverse lookup scheme:

  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 36.36     24.68    24.68 306886434     0.00     0.00  getfield_
  9.17     30.91     6.23 103215478     0.00     0.00  partlocate_
  7.69     36.13     5.22 407015274     0.00     0.00  inside_geom_
  7.60     41.29     5.16 308345599     0.00     0.00  gradlocalregion_
  7.07     46.08     4.80 98397734     0.00     0.00  achargetomesh_
  6.73     50.65     4.57 100616156     0.00     0.00  spheresect_
  6.34     54.95     4.30      100    43.00   640.15  padvnc_
  3.44     57.28     2.34 46927877     0.00     0.00  interp_
  2.64     59.07     1.79    11307     0.16     0.18  sorrelaxgen_
  1.68     60.21     1.14 67356357     0.00     0.00  gradinterp_
  1.62     61.31     1.10 99821835     0.00     0.00  moveparticle_

Total 67.87

Interp costs are way down. Partlocate self costs are up a bit, since
it now does the work previously in interp. But it's a significant win.
make geometry is fine.

Things are about as tight now as it is reasonable to aim for. getfield
is the main cost. The only other big win at hand is to drop the getfield
for periodic dimensions of length <=3. This would save approximately
1/3 of the getfield cost. That's about 8 seconds in 67. About 12%.
That's not nearly as good as a true 2-D code because we are still
doing multilinear interpolation in the uniform direction (unnecessarily).
If the multilinear interpolation were corrected for reduced dimension
that would take a factor of 2 off getfield in the other directions. 
We'd then get 2/3 less effort in getfield and gradlocalregion which
together are 30s of 67. So we'd save 20s off 67, but only for 2-D runs.

9 Nov 2015 
Implement a test for 3-size and periodic potential that is
supposed to save time.  It saves about 20% of the time for one
periodic dimension. And another 20% for the second. Significant.
The remaining getfield costs only about 25% of the remaining total. 
So no major gains would be obtained from improvements to it. 

10 Nov 15

Testing density gradient operation. Found that there's a small discrepancy
in the compensation that gives rise to a systematically low charge in
an open box when density gradient is present. It depresses the potential
in the middle. It is substantially affected by drift velocity and B-field

One problem is with ninjcalc which is incorrect for density gradients
and gives an nparta value that is wrong. Then the number of particles
initialized is incorrect. The problem is that it assumes the nparta
= ripn*volume with volume calculated naively. But for gradient cases
we are using rhoinfinity as the value at the reference (center) position
so this calculation is incorrect. Corrected using the exponential forms.
Looks much better with zero drift velocity. 

However, turning drift velocity on, we get evolution to a substantial 
error again. So probably there's another error. Certainly the particle
number drops a lot. We aren't injecting enough particles. Biggest
deficit is at positive values of the drift dimension. On reflection, I think
this is an incoherent test since the drift velocity has a component in
the direction of the density gradient. That does not make sense in the 
physical model I've been thinking of. Therefore implement a test to 
prevent it.

Seems to be working.
Can't use -l0. with -gn because the direct potential calculation has no
compensation. Correct that by subtracting uc(ind) in the quasineutral
routine. Gives a flat potential. Seems right.

12 Nov 15 

fluxexamine gives segfaults when called with -q on loki
(unfortunately not on X1). The GL color setting are referenced by gdb,
and with vecx the XSetForeground is referenced. I think what's happening 
is that e.g. color setting routines are being called without pltinit
having initialized the display etc. That's causing incorrect calls
to X and GL which are the segfaults.

13 Nov 15

Make partwrite use the nprocs to decide the length of the extension that
the file it writes to uses.

10 Feb 16
Running large 65^3 cases on NERSC, with more than 1000 processes.
First 2048 exceeds maxprocs. So it stops. I increase maxprocs to 4096.
Then I get segfaults which appear to be happening in sormpi. 

I can avoid the segfaults by limiting the number of processes devoted to
sormpi, using a switch -id8,8,8 which is 8^3=512 processes in the 
cartesian communicator. I think I still used 2048 processes for the rest.
But I've never used different numbers before so there might be issues
not resolved properly. Actually 512 is enough for decent 3d calculation
of hole stability.

My best guess is that it is caused by norigmax not being large enough 
(1500). Tried setting to maxprocs in the bbdy routine. It then gives
a warning message unless the setting at the other place used is the
same as maxprocs. This seems like a good compromise.

Do run with 2048 processors with this increased. Fixed!

28 Apr 16

Implement anisotropic temperature with hole initialization. 
It is interpreted as setting the transverse temperature
at the center of the 1-d phase-space, while the temperature of passing
particles remains isotropic, and the trapped transverse temperature varies 
linearly in energy from the external value to the center value. 

Seems I've made the diagnostic files write to the scratchpath now so I
can't use them as a sign that the loki run is proceeding.  I need
something, I think. Yes that was 7 Feb
dacf4974ef029ff0368d01790047d7e7a5ba8092 I think that was because of 
issues on Cori.

Use monitor `masternodes` instead on loki.

29 Apr 16
Implement extra hole parameter governing the power of the negative energy that
scales the Tperp interpolation.

13 May 16
Remove lentrim from stringsnames.f because it is now part of the accis
distribution. Accis bug fix in vecglx. Also fixed benign bug in Makefile. 

17 May 16

Attempting to initialize a disk-hole with finite perpendicular extent.
Simply scaling the psi of the trapinit hole with transverse position
gives a potential that has a big positive value for all parallel
positions in the transverse regions of the hole. This seems to show
that the rejection scheme produces a density deficit when it is
called, that actually does not depend on phi. I think this is true. 
In a region where phi is zero, dentot=1, and denmax > 1 if psi is nonzero.
So we reject (random*denmax>=dentot) a fraction 1-1/denmax.

It used not to matter so much since it applied everywhere, and since we
continued to try to inject until we had a total number of particles that
provides average density equal to what we set. There was no resultant
transverse density gradient. Adjusted the internal iteration of the 
assignment so that a new transverse position is taken only if the 
iflag is not 1. I.e. not on rejection. Thus rejected particles are
retried at the same transverse position so there is never a tendency 
to develop density gradients in the transverse direction. This does 
fix the potential hump with holerad set non-zero. So this demonstrates
that I had diagnosed the problem more or less correctly. Now we seem
to get a transversely localized hole. But on X1 the noise is too
great to do serious runs.


24 May 2016
Working on holes with multiple species. 
I find that there's a problem with drift velocities and reinjection 
initialization with multiple species. The possibly different drifts of
the different species are not recognized because the functional code
passed by cinjinit just refers to vd, vpar, ... instead of vds(ispec)
vpars(ispec) .... Replaced with vds etc.

25 May 2016
Went through other places (egrep '[^a-z]vd[^a-z]' *.f) where we are
still referring to vd not vds. Fixed many. 
array3write/read and writefluxfile still refer to vd because really
they ought to be recoded to do multiple species writing of vd. However,
vd is not integral to the the actual output in those files, it is just
part of the parameter documentation, so it is very low priority.

particle data for all species is written out and read back. partexamine
has switches to look at different species. I think that's enough 
rationalization for now.

21 Jun 2016

Implement passing of xprior (including velocity) to achargetomesh in
order to fix the bias in velocity diagnostics. chargetomesh is called
once on restart. It does not know the xprior. So I just pass the
current x as xprior in this case. Then in achargetomesh I effectively
advance the velocity by half of its previous step: a forward
extrapolation. Since the state of x/v is at the end of a kick then
drift cycle, a perfect diagnostic would require knowledge of the next
kick, but we can't afford that, so we assume the next kick is the same
as the prior kick. It hasn't obviously broken anything, but it is hard
to verify real improvement except on the extreme cases like holes.

23 Nov 2016
With a second species the facet areas are being written incorrectly to the
flux file. It appears their values are clobbered by the second species
somehow. The values for the first object are already corrupted in the
fluxave call. Other objects are not obviously corrupted.

The difference was because only the first object had multiple fluxes (3)
specified. Going back to 1 (flux only no momentum) the corruption goes away.
Making just object 2 have multiple fluxes causes object 1 areas to be
corrupted. Five values cf the 6 faces. Giving object 4 multiples corrupts
areas at addresses 4-15.
Corruption does not happen if we use -s0 no steps.

Found an error in fluxdata. Some higher flux quantities used mf_quant
rather than kf_quant to determine whether to write higher flux
quantities. Correcting that seems to fix the corruption. Declare fixed.

28 Nov 16
Pursuing other sor problems. srvsect appears to have bugs or inaccuracy
that causes puzzling values. It also seems to have an incomplete section for
cones that degenerate to cylinders [no. misread.]. Needs to be sorted out.
srvsect appears to have logical errors. Found a bug with not testing the
r range. Fixed that. It stops the puzzling errors, but not the leakage.

The leakage is because icross is zero for xprior and x_part clearly such
that the srv object is crossed. icross is set by icross_all which calls
icross_geom for each object. icross_geom uses srvsect. So we ought to be
able to verify the problem.

I seem to have tracked it to a rounding problem in the crossing
determination in srvsect. For near-disc cones, the fractional axial
position can be sufficiently inaccurately determined as to be outside
the range of z for the segment. It seems we need a more appropriate
way to combine the finding of cone intersection with the determination
of whether the intersection is within the segment limits.

Implemented oblique angle test. It gets rid of some of the leakage but
not all. There's still a problem for some reason. It seems to me that
conesect can't really work for disparities between x and z greater than
about 10^4, because the A coefficient is x^2+y^2-z^2. So if x/z<10^{-4},
then its contribution is in the rounding range. The z values are in the 
ball park of 10^4, so we are in this domain.

29 Nov 16

Finally sorted this problem out. It arose because there is an extreme
rounding error in conesect that comes from evaluating B^2-AC
directly. Rewriting the discriminant in a form that explicitly
accounts for the cancellation of the large terms fixes that problem
and now we get no particle leakage.

On the way implemented dbgcon.f which provides a common block for
indicating the debug level. Left some of the tests in conesect and
srvsect.

Fix bug in writing of srv angle. Fix labels for fluxexamine srv cases.
There seems to be a problem with color surface plotting of srvs. 
Fix negative radius bug in srvgplot.

10 Jan 16
Fix bugs in objplot associated with facecolor and large numbers of facets.
Now we need to figure out how to display the electron fluxes.

24 Jan 16
Modifications of fluxexamine and related routines to allow access to
data from other species. There's a problem that arises in that objects
can have different numbers of tracked fluxes. Therefore if_quant is
different for different objects, and the offset for different objects
is therefore different. Objplot does not know what species
it is dealing with but cycles through the objects, requiring in principle
possibly different if_quant offsets for each object. Fix this problem
by adding if_species integer to the 3dcom.f which is used for the
working species in fluxexamine. By default it is set to 1. In coptic
it will currently never be anything different so only the first
species will be plotted by objplot. But when called from fluxexamine,
if_species might be 2. vtkwrite routine has the same problem.

In objplot.f changed all the iav=nf... statements to add
+if_quant(ifobj,if_species) to iq. This might be enough.
Since vtkwrite uses the low level plotting sphereplot etc.
It also ought to be fixed by this expedient. Then the only thing
really changed in fluxexamine is the setting of if_species and
passing adjusted iq to fluxave routine (only).

Got different plots and different fluxave evolutions,
with -s1 and -s2 switch, but maybe not correct.
My first thought was that the electron flux ought to be much higher.
That is true only for the first step or two. However, since the
potential is -3, electrons are mostly repelled and so their flux
ought indeed to drop to a low level. Therefore it is not obviously
impossible that the electron and ion flux levels are about the same. 

Fixed the cumbersome plotting out of averaged data in fluxdata and
fluxexamine so that they only do it if there are no more than 200
values. This is a temporary fix until I can decide whether there's a
better approach.

25 Jan 16

I find that the solver works differently with different MPI cartesian
communicator layout, using the -id switch. It should not! I think
there's a bug or rounding problem. Viewing the solution display, it is
clear that the boundary conditions on the first step are wrong. Only
one of the cartesian blocks has them right. After the first step,
things seem to correct. The bug seems to be that the bdyfaceinit is
not called because no line of type 100 in the copticgeom.dat file
exists. and iCFcount is therefore zero. (This does not seem to be
the problem for the sphere benchmark). Remove the iCFcount test from
the bdyfaceinit calls. Although we now seem to get consistent initialization,
there are still big differences between -id1,1,1 and -id2,1,1. There should
be none.

The flux printed at the first step is different! Plainly everything is
different. Why? Ah! It is because the process(es) not included in the
cartesian communicator never get the potential updated! So there's no
obvious way to allow the cartesian communicator to be smaller than the
entire mpi. I ought not to allow it until that's fixed.

Implemented meshbroadcast that broadcasts the potential mesh to all nodes
from 0 if the cartesian communicator is not full rank. So now it should
work, at the cost of one broadcast per step. I doubt if that's a big
hit in the context of the various reductions.

Observed on Engaging that reducing the cartesian size to -id4,4,2 is
about optimal for a 65^3 mesh. Greater than that, one sees substantial
degradation.

It occurs to me that a way to avoid the broadcast and minimize the
communications cost, might be to implement multiple cartesian communicators.
Then each cartcomm can do its own sormpi calculation, including the
reduce at the end. In which case all nodes (assuming all nodes are filled)
end up with the potential data. This is substantially more complicated
though, and I don't know how to exclude nodes from creating a cartesian
communicator. Doesn't sound worth it.

6 Mar 17

I showed how to generate f90-compatible source.
Gave serious consideration to restructuring the code to use modules
rather than the commons in include files. It emerges that the rewrite
would be very difficult because of several incompatibilities. In
general the problem surrounds dependencies of include files modules on
one another, such as the dependence upon ndimsdecl and griddecl. Modules
cannot tolerate multiple definitions of the same things within a
program unit. If something is defined in a module that is USEd, then
it can't be defined anywhere else in the same unit. However, a module
is modular, and MUST have the definitions of what it operates on declared
within itself. This is contrary to some of coptic where declarations are
required by some includes, but their requirements are satisfied by prior
inclusions, rather than by nested inclusions.

Based on this realization and some reading concerning other people's
experience with code conversion, it seems not worth converting to
modules.  The benefits are limited, the complexity will actually be
increased, not reduced (because of .mod files and nesting), and quite
likely compilation will be substantially slowed.

31 Mar 17
Implemented dividing diagsum by iavesteps, to make the diagnostic density
and average not sum over iavesteps.

18 Jul 17
Implementation of rising density. Not obviously what I wanted. If rhoinf
is allowed to increase in accordance with the increase in ninjcomp, then
the potential develops a hill that attracts electrons and causes them to
have a hollow top of f(v) (in a 1-D configuration). Alternatively, if
rhoinf is not updated, (but the ninjcomp is ramped up) then a negative
potential trough develops, and the electron distribution is hardly 
changed. I presume that is because the trough repells just enough electrons
to keep the density constant. So the trough defeats the effective density
rise. What I really want is for no overall trough or hill to develop. 
That seems to require rhoinf to rise more slowly than the density. But 
it is hard to guess how much. Reduced the rhoinf update rate to half
that of ninjcomp. This gave a shallower trough and the dimple began to
show up. 0.7 is nearer a zero potental trough at least for -da-.001

I need a good way to see phase space and holes, in 1-dimension. 
The xes1 scatter plot is a good display. All it does is XDrawPoints
instead of XDrawLines. But it would need some accis programming.
Tested with pointtest.f and found that the mechanism I have put in place
is probably fast enough. Something like 2M points per second (with PS writing
enabled) and 10M/s with PS writing off.

19 Jul 17
Implemented scatterxy plot in accis and it sort of works in coptic.
Signs of hole formation only towards the end of 1000 steps with the
above parameters, and a couple of clear holes for -da-.003.
What's needed for this density rising case is a way to make the 
background (effective ion density) and the injection rate change
consistently in such a way as not to induce big potential gradients
across the domain.

Implemented a different approach to bckgd adjustment. The ninjcomp
is ramped up at a constant rate specified by -da-.03 being the fraction
of initial bdtnow=1.+abs(bdt)*j*dt by which the injection is incremented 
in time unit 1. Then the bckgd charge subtraction is adjusted at each
step via
            bckgd=bckgd-crelax*phirein*(1.-boltzamp)*eoverms(1)
(from its initial value (1.-boltzamp)*eoverms(1)). This is essentially 
an integral controller. It goes unstable unless crelax is quite small
-rx.03 was small enough for my tests. If we detect a large phirein, then
a trap declares this is going to be unstable and stops. The boundary 
conditions at the injecting ends must not be fixed. 1 5 0 (mostly gradient)
worked fine. 

This produces a potential that adjusts its end values to be approximately
zero on a longish timescale. It then shows clear hole formation as a
result of density ramp -da-.003 (which is +1.5 in 1000 steps). The holes
are very deep: \psi~0.5, in a domain 128\lambda_D wide. Diagnostics
work ok, but I would really like to have a compact movie production
facility. 

Another approach would be to change bckgd in accordance with the total
number of particles in the domain n_part. The bckgd value should be
such that V.bckgd.rhoinf = n_part*eoverms, so the scaling factor is
bckgd -> [n_part/(V.rhoinf)]*(1-boltzamp)*eoverms. This appears nearly
correct, and nicely stable. However, we need to subtract about 1000 from
n_part to give an approximately zero potential. So there's something not
quite right. Found that n_part is exactly ndeposits. So that's not the
problem. I think probably it is at the non-periodic faces where the 
potential boundary condition is applied we are not using all the particle
weight in the poisson equation. No, that is not confirmed by reducing
the number of cells. It's more subtle. Found the problem. Had to have
an extra factor (1+bdt*dt) because rhoinfcalc uses nrein which was 
evaluated at the previous advancing step. I'm not sure I fully understand
this, but it seems to work. 

Unfortunately it does not work with multiple processes. Need an
additional factor numprocs, since rhoinf is for the total density, but
n_part is only on this processor.

21 July 17
Got a decent plotting of phase-space going. Implemented meanings of 
switches to control it. 

-gn which is to do with collisional distributions but is now overloaded,
now means plot out the phasespace at every step, if this is effectively
a 1-d calculation. (Nx3x3). 

Without that switch, a phasespace plot is output every time a .pex file
is output (every -a steps) (i.e. if ldistshow is set.) The phasespace
plot is a snap-shot, not a running average. Which means the accumulation
is only done on diag-output steps.

Might be as good just to write out the data. Implemented routine
adjustments to do so and include u (phi). Size is tiny:
24k. Essentially the size of the pixel buffer plus a bit.  This would
allow us to use more pixels without getting too big.

Implemented main routine phasereadplot to create the ps files from the
pps files, via phasereadplot *.pps* Works, and is very quick. 

Reorganized as follows. 
-gp[M] (when 1-D) gives phasespace plots only every -a unless:
-gn asks phase-space outputs regardless of plotting, at every [Mth] step.
And if -gn and -gp are used, then to turn off displaying the plots we need:
-gx[NN] sets the accis pfswitch (to -3) if no NN.

Examples
-gn outputs just a pps file every step, no display.
-gp outputs a pps file and displays every -a steps.
-gn -gp outputs a pps file and displays every step.
-gn -gp5 outputs a pps file and displays every 5 steps.
-gn -gp -gx outputs a pps file and a ps file each step but does not display.
-gn -gp2 -gx3 outputs pps and ps files and displays every 2 steps.

27 Jul 17 

Fixed the restart code for density rising and related problems to do
restart correctly. The issue was getting rhoinfinity and bckgd correct
whether or not we use the same number of processes for restart.

16 Oct 17
Made diagexamine able to accept multiple files on the command line and
put out .ps files for each one. This is for generating animations etc.

3 Nov 17
Investigating the trapinit code for hole formation and reduced transverse
temperature, because of interest in hole stability. Found something in 
the normalization of the parallel velocity selection that I did not understand
It appeared that the unnormalized cump was used incorrectly. However, it
turned out that the factor that was the ratio of cump-range to dentot, was
1.0002, i.e. it is correct. I still don't quite understand why, but it means
prior runs were not significantly affected. 

Implemented local diagnostics inside trapinit, that are turned on in the 
code by making idiag non zero. Various selections of particles whose 
distributions are to be histogrammed are possible. Experiments show that
if one selects for deeply trapped particles, then the transverse temperature
is indeed narrowed for tp0.1. and widened for tp10. So that code does
seem to work ~ correctly. However, I notice that the effects apply to 
a rather small fraction of the particles. Maybe that's why you can't
stabilize by reducing tp but you can _de_stabilize by tp10. 

6 Dec 17
Trying to make very deep holes with large \psi. There are various problems
associated with the calculation of the untrapped density, and warning
messages associated with eta adjustments. Looked at that code and 
somewhat rationalized it. Basically, the quantity 'ratio' must lie 
between 1/(1+eta) and 1. If it doesn't, I now simply set it to be 0.01
and set eta accordingly. That will generally make the trapped density
at that point very small, with steep sides. The resulting potential 
won't so well be given by the sech^4 profile. However, it looks as if
the result is actually pretty good up to psi=4. Higher values of -ih
do not make the psi larger. Instead they produce non-monotonic potential
shapes. psi of 4 may be just enough to do what I wanted which is to 
explore holes deeper than psi=3 for transverse stability.

Unfortunately it looks as if the potential quickly relaxes to a lower
peak of order 2-3 (or lower after 20 steps), so I think it is not
going to work for what I want. 
Also, I notice that there are 5M tries for 0.8M particles. A lot of 
rejections!

It is not obvious what is happening to the hole after initialization,
but it appears to broaden and suffer a perturbation that is symmetric
in x. All this investigation can be done on the laptop, because noise
is not a problem with such deep holes. Maybe not. Even a potential of
1 has problems perhaps noise is not so benign as I thought.

I think there really needs to be a major upgrade if I am going to make
holes deeper than psi=1, because the current holes are based on small
psi approximations that don't give self-consistent deep holes.

16 Dec 17
Running trapcumtest for higher phi values shows that bad things happen for 
phi>~2.5. The cumulative distribution becomes non-monotonic, which shows
that nothing is really working correctly in this region of psi. It looks 
as if the trapped density is becoming negligibly small. Actually it is
becoming negative.

I have this idea that possibly I should be using the numerical hole code
developed for the integral approach to give deeper holes.

Playing around with BGKint, I find that negative-f is required for 
sech^4 holes deeper than roughly 2, which seems to be the problem. 
However, if we use tl=0, coshlen=4.+psi.2., then I find that we can
make holes up to great depths without negative f. The formula is
then phi=psi*2/(1+cosh^4(x/coshlen)). But I don't have a formula for
what f_trapped is.

29 Dec 17

Adjust hole initialization so that the Tperp refers to trapped
particles and not to untrapped, ONLY if holepow is not equal to zero.
This is logical since holepow constrols the connection of trapped
perp temperature to external. Also make the default value of holepow
equal to zero, so the default is the same Tperp for trapped and 
untrapped.

3 Jan 18

Implement diagexamine analysis of kink amplitude measurement and 
fitting an exponential to the growth of the kink. 


21 May 18

Implementing the ability to run with all boundaries periodic. 
If all domain boundaries are periodic, AND there are no objects in the 
domain with inhomogenous BCs then potential is arbitrary with respect to
an additive constant. It tends then to integrate up to infinity. That is
physically expected if there is some net charge in the domain, because
then we are dealing with a net charge over an effectively infinite 
domain. Thus having truly zero net charge is essential. But also we 
need to anchor the potential to some value which will usually be 
"zero at infinity". 

Running COPTIC for holes has a particular problem associated with the
fact that if the particles are periodic, there is no way to get rid of
a net charge even with non-periodic potential. That is a related but
somewhat different problem. 

First fix the testing in makefile to use the new file names.
Simplify the boltzamp ramping code.

Now set up test copticgeom.dat and run with -gp on 65x65 mesh.
Runs fine with 98,0,4,4 but crashes after two steps with 98,4,4,4.
Plainly the potential is breaking. But that seems to be because the
density drops drastically, to nearly -1. It doesn't with 98,0,4,4.
In both cases a Fixed injection count is reported. Obviously that's
wrong for a fully periodic particle case. However it does not seem
to explain the big density drop.

Found that the problem was that .not.lnotallp sets the ninjcomp to zero
so as to prevent reinjection, but then that caused rhoinfinity to be
recalculated in rhoinfcalc which was erroneous. Therefore changed the
condition for resetting rhoinf to:
      if(ninjcomp.ne.0.or..not.lnotallp)then
Then we get reasonable result for particles all periodic. 

There are still long-lived oscillations lasting at least 500 steps, 
that are waves with k approximately in the x direction and wavelength
approximately the domain size. They are substantially worse when all
periodic than when x is not periodic. That sort of makes sense in that
there is no way for the waves to be damped with all periodic. Quiet
start might fix it.

Then running with allperiodic potential boundaries sort of works 
provided the particles are also all periodic. However early on there
is a clear instability in the iteration scheme at high k. Tried to 
avoid by removing -sb.01 switch, but then crashed with corrupt gralocalregion
nans. I think that was a mistake. Setting -sb.001 stabilizes the hf.
(But not the long wavelength.) -sb0 runs fine. I think that is the better
way to run.

Quiet start most naturally would use the SOBL quasi-random vector
generator. Using for position setting just needs a new version of 
ranlenposition. But that leaves the velocities noisy and might not
be enough. One might also be able to do the same for velocity but
I think that would require cycling through the particles twice if we
are to use the quasi-random (toms) code for both.

Implemented a clean up and ranlen3position that gets the vector position
using either SOBOL or ranlux. However, I recall that the problem with 
using SOBOL is that there is no obvious way to produce independent 
sequences of sobol numbers from the toms659 code. Therefore it is hard
to benefit from multiple core calculations. I don't have a good solution 
for this problem despite a net search. One alternative might be to use
some sort of slightly randomized lattice, instead. Basically it is the
long wavelength perturbations that we want to suppress. A lattice would
do that. 

30 May 2018

Obtained the strange hole attenuation effect after a restart.
It shows only in the diag file not in the live step by step display.
For -s50 and -a40 potential is suppressed to about 0.6.
The diag file is output at 90 and 140, with similar suppression.
Changing to -a10, we get significant depression at diag160. ~0.7.
which decays away by about dia190, i.e. it takes much of the 50 step
run to disappear.
Changing to -a1 the depression is much smaller, maybe absent
Changing to -a20 a big depression at 270. Substantially reduced at 290.
I'd say we have a normalization problem in the diagnostic averaging.

It might just be the potential, which is drawn directly from uave.

There is also a problem with the cellcount for 201, but not for later.
Also slight problem for 160, even slighter for 270. Seems that the
cell count is approximately one step out of the -a number too large 
after restart. This is because chargetomesh duplicates the deposition
of padvnc and I put it before entering the loop to set psum. So moved
the diagsum initialization to after the chargetomesh. That fixes it.

Temporarily changed the -gp plot to plot uave instead of u.
It shows that uave is zero after a restart and gradually ramps up as
time goes on. This is the cause of the diagnostic problem for
potential. (But there might also be issues with normalization of other
diagnostics.) A complete restart would presumably require reading in
of pha file into uave. Implemented that. Seems to work.

Now back to periodicity tests. Even with Bx1. we find that initial
non-uniformity of density in the y-direction remains for hundreds of
steps, indicating there is no diffusion of particles in that
direction.  This might be fixed by spatial quiet start alone since one
could then have a permanent smoothness of average particles at
position y regardless of velocity noisy startup. By the way this
explains why the eigenmode of the omegapcostheta must be antisymmetric
in (e.g.) Wu runs since they have periodic particles and thus have the
same permanence of average density.

Anyway both particles and potential fully periodic now seems to work,
provided -sb0.0

Found an error dnum q-error using -ni particle setting. (With periodic
particles). Might be because rhoinf is being calculated incorrectly.
Reported as 0.496031 with 100000 in 32*31.5*50*8 volume= 0.24802.
In other words, it is a factor of 2 too high. But no. I was running 
two processes, so it was correct. Seems particle periodicity is 
incompatble with -ni switch. That is perhaps sensible since the -ni
leads to immediate reinjection when a particle leaves the domain, 
which contradicts particle periodicity when crossing a periodic 
boundary. 

31 May 2018

Found that the permanent y-ripple that happens with all particle periodicity
for a hole can mostly be avoided by using quasi-random spacing in the
y-direction initialization (quiet start). There is not advantage to using
it in the other directions. And in fact doing so is worse. The remaining
randomness in velocity has some effect of smoothing to overcome the fact
that the quasirandom initialization is not different between different
processes. But we might wish to worry about whether this really scales to
large numbers of processes. Made the qr start automatic when all particles
are periodic and Bfield is large. But the qr is only in dimension 2, so 
it will be wrong if B points in a different direction.

1 June 2018
Thinking about improving the hole initialization.
Currently the initialization uses rejection as follows:

1. Choose a random x position.
2. Decide on the basis of a new random variate to make it one of three things
   a. Trapped
   b. Untrapped
   c. Rejected (in accordance with the density x-profile)

This works but is (A) inefficient since for a deep hole a lot of rejections
take place because the peak density is greater than the background density,
and (B) unable to be replaced by a quiet start algorithm. 

If instead the x-position were chosen from a quiet qrn and a cumulative 
position probability without rejection, both these problems would be solved.
We could do quiet start with greater efficiency. The density profile is
indeed known if we specify the potential profile \rho = d^2\phi/dx^2.
So constructing (once-off) the cumulative potential probability would not
be difficult. 

Another current big inefficiency is that untrapcum is called for every
particle initialized at a different potential. It actually synthesizes
and entire velocity cumulative distribution by adding into the middle
the trapped cumulative dist. This is probably where most of the cpu time of 
initialization is spent. I could continue this, but it might be worth
some time savings to store and interpolate, instead of recalculating each
time. The natural thing would be to store at an array of \phi values.
If one wished to solve the BGK problem for arbitrary phi-shape. It might
make sense to solve also for the cump(phi) at the same time. 
Since the solution of the hole structure gives f(W) where W=q\phi+mv^2/2,
it seems one only really needs f(W) and the potential to draw 
a random velocity at some potential. There seems to be a bit of trickiness
in the non-symmetric possibility of passing particle distributions. And
I don't see how exactly to draw an energy W without having integrated 
the distribution at the specific phi (or interpolating it).

17 Jul 18 
Attempting to understand and rationalize Xiang's BGKint version cumdis.
      dv=sqphi/(nt+1.)
! Trapped part (which is antisymmetric)
      i0=int(phi(k)*(nphi-1)/psi)
      remi0=sqrt(abs(u0(i0)**2-psi+phi(k)))
      cumv(0,k)=0.
      cump(0,k)=0.
      do i=1,nt
         cumv(i,k)=i*dv
         cumv(-i,k)=-i*dv
         j0=int((phi(k)-(cumv(i,k))**2)*(nphi-1)/psi)
         remj0=cumv(i,k)-sqrt(abs(u0(j0+1)**2-psi+phi(k)))
         if(i0.eq.500)then
           cump(i,k)=0
         else
         cump(i,k)=remi0*(f0(i0)+(f0(i0+1)-f0(i0))/2*
     $           (sqrt(psi-phi(k))-u0(i0))/(u0(i0+1)-u0(i0)))
         endif
         do j=j0+2,i0
           cump(i,k)=cump(i,k)+(f0(i0+j0+1-j)+f0(i0+j0+2-j))/2*
     $             (sqrt(abs(u0(i0+j0+1-j)**2-psi+phi(k)))
     $             -sqrt(abs(u0(i0+j0+2-j)**2-psi+phi(k)))) 
         enddo         
         cump(i,k)=cump(i,k)+remj0*(f0(j0+1)+(f0(j0)-f0(j0+1))/2*
     $           (sqrt(abs(cumv(i,k)**2-phi(k)+psi))-u0(j0+1))/
     $           (u0(j0)-u0(j0+1)))
         cump(-i,k)=-cump(i,k)
      enddo

Arriving here, f0 is the velocity distribution at phi=psi, u0 is the
velocity array on which it is defined, u0(i)=sqrt(psi-i*phistep)
from BGKint.  
i0 is the index (from 0, rounded down) at which a uniform phi array
from 0 to psi i0*phistep is equal to phi(k). So it is the highest index
that gives a positive velocity (exists) at phi(k).

Given that, abs(u0(i0)**2-psi+phi(k)=psi-i0*phistep - psi+phi(k)=
phi(k)-i0*phistep, which is the potential remainder. And remi0 is the
sqrt of that.

cumv(i) is the trapped velocity array which goes from -+ the sepx at -+nt.
j0 is the index at which j0*phistep=minus the total energy cumv(i)^2-phi(k)
So it is the index to which one would need to consider when calculating
cump(i). 

i0*phistep~=phi(k);             |i0*phistep-phi(k)|=remi0^2
j0*phistep~=phi-cumv(i)^2.; 	sqrt|j0*phistep-phi(k)|+cumv(i)=remj0

cump somehow needs to be the integral of f dv at position k. I am 
struggling to understand how this is supposed to work. It certainly
ought not to need two nested do loops. 
Instead, one ought to do one integral consisting of summing the f(v)
over the cumv arrays with f(v) obtained by translating the cumv into
the phistep index. Probably one ought to do it inward from the separatix,
since the phisteps are closer together in v there. Maybe the inner do loop
applies only when there is more than one phistep per cumvstep. But still
it seems inappropriate to redo the prior integral. 

A simpler way to calculate cump is just to do trapezoid integration using
a consistent interpolation to get f(cumv), and not to worry if there are
multiple f(u) nodes within the cumv cell.

BGKint returns f0 on a grid u0=sqrt(psi-i*phistep) uniform in u0^2.
BGKint calls untrapden for each phi of the potential array
untrapden uses f on a uniform ua grid to get the density and returns
it in fma fpa if nin>0 on entry. But there's an inconsistency that
those fma and fpa are not arguments but they are declared with a 
dimension that is an argument. I don't know how that works. 
Checking with my original free-standing version, I find this to be an
addition relative to BGKint code. The version of untrappeden in the BGKint
code has far fewer arguments. I think Xiang introduced this problem.
Untrapcumtest has nin as a parameter, which might be why the compiler
does not choke. So does trapinit. I think that explains it. 
There are lots of confusions associated with the refinement process
that means the used dimension of ua, fpa etc are variable. It might be
better to fix them. However, untrappedden was used in the old trapinit
untrapcum to get the distributions fpa fma ua.

19 July 18
Probably it is best not to suppose there is any known algorithm for 
the spacing of f0(u0), rather to write the routine generally for getting
f(u) (and integrating it) when f0(u0) is known and given in the form of
arrays f0 and u0. 

20 July 18
Did not complete the inclusion into qinit. There is something strange
with the new qinit that make the potential ramp up with time. 
The actual initialization might be more or less right now, but
I'm not confident.

23 July 18
Particles are disappearing in large numbers during iterations with qinit.
Fixed several errors in coding. Now seems to work correctly. 
A 1-d hole has greatly quieted initialization than before. 
Also, the qinit is at least twice the speed of trapinit.
Figured out the drifting distributions and corrected sqrt(2) and sign.

Now runs with periodic particles and potential with approximately a
constant offset of potential. Not unreasonably since \phi is now not
defined modulo a constant. It looks as if the offset makes the mean
potential approximately zero. Level does not significantly drift after
500 timesteps. Trying -sb0.01 shows no improvement on the offset but
a substantially worse (and growing) noise. 

I probably want to think about a way to rationalize the potential 
offset adjustment. However, at the moment, I think it is better to
leave it so we see if there's a systematic evolution.

Experimenting with different -pi and with ldouble, in 1-d.
I find that large -pi can quiet the potential initial value more than
the level of noise it gets at later time. Also, in this state, making
ldouble true reduces the immediate rise of noise, as it should do by
making the current initially zero. 

Observed a slow downward drift of phi from -0.02 to -0.06 in 500 steps
with all periodic particles and phi. (This with ldouble true, but it
makes no difference.) With particle drifts turned off, this changes
to a slower drift up. 

Tried fiddling with boundary conditions but did not find a way of 
controlling phi offset. Possibly I need an explicit phi-adjust routine.
Is there a way of using the median height? How do you quickly find the
median of a lot of numbers?

24 July 18
Add makemedianzero code that subtracts the median from the potential 
when potential is periodic in all dimensions and there are no objects.

Thinking about the avoiding step limit. nf_step and nf_nsteps are key
parameters that are part of 3dcom.f the flux storage determination
which has nf_maxsteps. Neither nf_step nor (probably) nf_nsteps ought
to be allowed to exceed nf_maxsteps. A way to prevent this might be 
to stop incrementing nf_step when it reaches nf_maxsteps. However 
there are various places where nf_step is used for telling things other
than where to store the flux data. Those would have to be changed.

There are many occurrences, but most are associated with flux.
In particular, cijroutine.f, fluxdata.f, objplot.f, padvnc.f, reduce.f,
and stress.f all appear to be using strictly for flux parameter and 
so need no changes. That leaves: partwriteread.f, mainroutines.f, coptic.f

mainroutines.f only incidentally uses it as the name of an argument.
Changed that to nstep. Same for partwriteread.f. coptic.f is more
mixed.  nf_step gets set internally from readfluxfile. It is passed to
restartread (but not set in it). Then the main iteration increments
it, etc. It is from then on that it needs to be reconsidered.
Completed what I hope will limit the flux writing length, but not the 
diagnostic run length.

A quick check with nf_maxsteps set ridiculously short, shows that 
diagnostic files continue to be written apparently correctly.

Found a problem with the last x-position at first diag output.
It is caused by makemedianzero. It was caused by not adding 1
to the indexcontract since it is the offset. Fixed.

I find that deep holes now work.

25 Jul 18
Added holerad functionality to qinit.f (not tested active).
Tested in a rudimentary way 65x10x10 mesh. Looks sensible.

Changed the identification of holetoplen to be with holepow got rid of
the equivalence and reorganized so that one can override the defaults
with command line.

Found a bug that seems to be in sormpi. It doesn't converge when the
case is 129 129 3 with periodic phi-boundaries. But that's the only 
xy mesh that is unstable!!! Tried changing sormpi.f underrelax. 
No difference. I find that a tiny tweak on the 0.25 coefficient in
Chebychev acceleration stabilizes it. Although it was not obvious
a 65 65 3 case also had nonconvergency issues prior to this tweak.
So it appears that 2^n+1 square is the most likely to be unstable.

26 Jul 18
Further work on bugs.
Looking into reproducing the prior oblique instability. It does not 
easily reproduce. Only if using trapinit.f.

I find that in pinit.f the only place Tperp is used is in maxwellstats.
maxwellstats is used for notseparable=2. That is the case for collisionless.
Verified that the anticipated pinit case reports that. So the initialization
is done from the maxwellstats array and should be anisotropic. 
Find by experiment that trapinit and pinit are giving anisotropic T,
but qinit is not. Fixed that.

Found other problems that I thought were caused by no save, but were 
because initializations were not being done because holepsi was zero.

There are differences between pinit and the two that basically agree now
trapinit and qinit in giving oblique instabilities. Not pursuing pinit
issues at this stage because for the long term we won't use it for these
sorts of applications.

30 Jul 18
Further investigation of the single-wavelength instability observed with
fully periodic (particles and potential) domains.

Since there's a suspicion that this instability might be caused by the
finite grid effects, and COPTIC does not use momentum conserving field
interpolation, one wonders if the instability would be suppressed if it
did. It seems as if it would be fairly easy to implement momentum
conserving field interpolation in a uniform periodic mesh, but probably
rather difficult otherwise.

The issue is that E normal to a face would have to be interpolated to
the grid by taking E_j=(\phi_{j-1}-\phi_{j+1})/2dx and then at positions
between j-1 and (f) j one uses
f*E_{j}+(1-f)*E_{j-1}= f*\phi_{j-1}-f*\phi_{j+1}+(1-f)\phi_{j-2}-(1-f)\phi_{j}
  =(1-f)*\phi_{j-2}+f*\phi_{j-1}- (1-f)*\phi_{j}-f\phi_{j+1}

So it is four-point interpolation. Whereas the standard interpolation is
3 point:  (1-f)E_{j-1}+f*E_j = (1-f)(\phi_{j-1}-\phi_j)+f(\phi_j-\phi_{j+1})/dx
provided f>1/2. And since the domain ends at f=1/2 we can put the guard
just 1/2 a mesh point away from the domain end.

But if (j-1) is a lower guard point, the 4-point interpolation reaches
an additional mesh point past the guard: to j-2, even when f>1/2. So
unless the domain is terminated a full mesh point before the guard cell
interpolation cannot be restricted just to the guard.

A uniform mesh periodic boundary can be implemented in a way that reaches
beyond the guard cell straightforwardly by wrapping further into the
periodic region.

[It is only particle pushing that is affected.  Solution of Poisson's
equation is not affected, so the mpi splitting of the domain and block
boundaries are not affected either.]

Routine getfield already has a shortcut that is used for locally uniform
differences. I think that is all that has to be adjusted for the different
interpolation. So the changes needed appear to be rather localized in the
code.

Did more tests and found that the instability disappears if the mesh
count is bigger than the length, so that the mesh spacing is smaller
than the debye length. Hence not pursuing the above.

Fiddled with the calculation of the Jacobi convergence radius to do 
a better calculation of it. It does not much improve the convergence
and we still need a fudge factor, which I've made .245 instead of .25
to guarantee stability.

2 Aug 18

Looking for the single-wavelength instability cause. Found that it
is present even in runs that previously were stable. Hence it has
been introduced by some change this year. Using psi.2/B.35x64 as the
test case.

Checkout and rerun with
commit 57b70b1964d206949e2011c865b77016aaa9c1c0
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Wed Feb 21 22:15:14 2018 -0500

    Improvements to slicing in accis for putting color bar on contours

No instability.

Checkout and run   B.35x64scr2
commit 1a7253ea997ae6660a08c1502727b5b549c1c0c7
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue May 29 19:04:11 2018 -0700

    Correct NOACCISCHECK

No instability.

Checkout and run    B.35x64scr3
commit f70463672a5609138a2c4ab4c220ad4ef99e6ef5
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue Jul 10 10:19:48 2018 -0700

    Accis alterations (can't remember what)

No instability

Checkout and run    B.35x64scr4
commit 007c5f9f089269f6bde37731b4532b882fb7f4b4
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue Jul 24 09:28:04 2018 -0400

    Remove all SOBOL calls from trapinit.f (ranlen3)
    Allow quieting in short mesh dimensions >1.
    Allow default 20 with -pi call.

No instability

Checkout and run    B.35x64scr5
commit 654f9644210f4cac15bc7c95c5764abefe45e338
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Wed Jul 25 13:11:38 2018 -0400

    Fixed the Chebychev acceleration in sormpi to avoid instability with
    a square mesh 129 129 when periodic, arising from poor SOR convergence.

Crazy big potential (and maybe instability later).
This was after the cmdline defaults were changed which broke the trapinit.
So I need to disentangle that.
git diff cf6d2da4fdc22 a0a030bdf0b7af
Shows differences in various files partcom.f cmdline.f mainroutines.f
qinit.f.
git diff a0a030bdf0b7af  654f96442 has only sormpi changes.
So probably the best way to disentangle is to checkout sormpi.f
from 654f96442 into cf6d2da4f.

So git checkout
commit cf6d2da4fdc22b5036cd88389f95da21a56d4b1f
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Tue Jul 24 15:19:36 2018 -0400

    Change the behavior when asked to run for more steps than there are
    flux storage locations. Now it runs for the steps asked, but stops
    incrementing nf_step the location of the flux storage for this step.
    Thus all the exceeding steps' flux data are written over each other in
    the last place. But it ought to be able to run this long.

git checkout -p 654f96442 src/sormpi.f make and run scr6
slurm-12203002.out
This has strong suspicion of long-wavelength instability at late
times (2000). This has .247 for its chebychev iteration factor.

So run the same executable in psi.2/B.35x128scr
I notice that once the -sb is zeroed (at step 660) convergence is
often incomplete, and it explodes around 800.
This executable has the problem (in spades).

The strong evidence so far is that the single-wavelength instability
is caused by sormpi adjustments.

These started to be made after commit a0a030bdf0 so if I patch the
sormpi.f file back to that and use the current master I ought to
get stable results for old runs. So we try:

git checkout master
git checkout -p a0a030bdf0 src/sormpi.f
a
make
run in psi.2/B.35x128scr2

This proves that the single-wavelength instability was made much
worse by changes to sormpi in this commit on 25th July:
commit 654f9644210f4cac15bc7c95c5764abefe45e338
Author: Ian Hutchinson <ihutch@mit.edu>
Date:   Wed Jul 25 13:11:38 2018 -0400
    Fixed the Chebychev acceleration in sormpi to avoid instability with
    a square mesh 129 129 when periodic, arising from poor SOR convergence.

However, the 129x129 case appears to be on the hairy edge of instability
even with the old sormpi. One can see non-convergences getting quite
frequent, and this seems to induce jittering. By step 1500 it is really
present, and continues at a moderate level till 2000.

In summary, we must not allow nonconvergence of the potential solver
because that seems to make the PIC scheme long-wavelength unstable. 

Evidently there needs to be a different solution to stabilizing the
problems with all-periodic mesh 129x129. Maybe there's actually a
bug in the boundary setting or some other problem. It ought to be
easier to diagnose because it shows up immediately. But even so,
I'm not confident that I will have addressed all problems since
the old case is so close to long-wavelength instability.

Working now on hutchX1.

Using the same test case on hutchX1 quiet start non-periodic.
with -sb0 with the current master
(broken sormpi) we get instability right from the start.

First patch sormpi back to the old settings
git checkout -p a0a030bdf0 src/sormpi.f
Then we find that the potential is converging, and we do not get
such obvious early instability. However as time goes on we begin
to see some bouncing up and down. It's hard to tell if this is 
instability or just lost of quietness initially but eventually one
can tell it is episodically unstable at times when nonconvergence
is occurring. 

Got the hacked sormpi so that I can get the alternative xjac calculation
Increase the max iterations from 2 to 3*default.
Decrease the eps_sor to 0.1e-5 from 1e-5.
Use the new xjac calculation because experiments show that it converges
quicker. (But not quickly enough not to need more max iterations). 
This ought to be enough to suppress instability caused by sormpi
inaccuracy. (But I still haven't figured out the periodic instability).
This suppresses single-wavelength instability so that the prior case
that was obviously unstable at step 2000 (-dt0.5) before is now not 
(at least not obviously). 

I realize there is a potential problem with boundary setting and odd number
of mesh nodes per side. We are doing red-black SOR half iterations. But
it's not clear that will work correctly with odd numbers. Certainly it
breaks the red/black symmetry. That might mean the boundary connections
are not correct. Actually they can't be correct, because the red at one
end corresponds to the black at the other. That might explain why changing
one of the 129 to 128 stabilized things.

Added a test to sormpi to stop if a periodic boundary is asked for an
odd length mesh (other than 3). Turned it off to do some verification.
Very clear. We get short wavelength instability if two dimensions are
both odd, but not if just one is odd. Still it seems we ought to ensure
that neither are odd.

Re run.4nodes the (non-periodic) case psi.2/B.35x128. It is now stable.

Conclusions On Stability Issues
_______________________________

The single-wavelength instability arose from de-optimizing the sor 
convergence as a hack to prevent observed *short wavelength instability*.
The short-wavelength instability which occurred only for all periodic
potential BCs was caused by using *odd dimensions*. That defeats the 
red-black SOR scheme because the periodic boundaries are setting a
red ghost cell from a black periodic position. The resulting error is 
enough to cause instability when two dimensions are odd. It is now 
avoided within sormpi by a test for odd periodic boundaries which throws
a fatal error. 

The single-wavelength instability is (at least greatly) suppressed by 
better convergence of the potential solution. That is achieved by
using a more optimized Jacobi radius, allowing more iterations, and 
reducing the size of the convergence parameter (to 1e-6 from 1e-5).
Hopefully this will suppress the instability fully in low noise cases.

3 Aug 18
Succeeded in running a fully periodic hole with 32 processes.
Had some problems with stability with more processors initially
which is why that run was done. Now did a run.4nodes and the
results are fine. That was with the block configuration limited
by -id4,4,1. The problems occurred for no limits. So ran a case
without limits which does 16 8 1. It runs ok, so that was not the
problem. I think it was that an old executable was being accessed
anyway that problem has gone away. I declare problems fixed for now.

Doing 128^3 tests I see a need for sormpi tolerance relaxation, so 
implement increasing of eps_sor by factor 1.5 after each
new 2 nonconvergences. 

Currently the high-B mover driftparticle moves the particle by a 
timestep times v_parallel plus v_perp= ExB_drift plus v_driftperp,
and sets the total velocity to be v_parallel+v_perp, thus zeroing
the gyromotion (mu). Prior to this drift, the kick is applied to the
whole velocity, although only the increment in v_parallel matters,
since the perp will be reset. I wonder if instead one could retain
finite mu and rotate the particle by its gyroangle increment. 
That rotation is the cyclotronic motion. Why would it not work provided
that we decline to change the particle's mu (perpendicular energy in the
drift frame)?

14 Aug 18

While further diagnosing poor convergence when the domain is smaller
than the mesh count I find a peculiarity of running COPTIC as an
electron pushing code. It is that the cij coefficients are in units of
Debyelengths whereas the q (charge density) is not, it is in whatever
the computational units happen to be. Therefore if you are solving
Poisson's equation to find potential for a certain particle
distribution, the potential obtained depends upon the Debyelength (and
is actually inversely proportional to its square). This does not make
any difference if no -l flag or -l1 is given, because debyelength is
by default 1. But it appears to be a confusion of units in
non-quasineutral cases. The cij are set equal to debyelength^2 times
the finite difference coefficients for \nabla^2; so the sormpi solves
(d^/dx^2+...)\phi = q, where x, y, and z, are measured in units of
debyelength. In fact, the only important place where debyelen enters
the calculation is in setting cij (plus some ancient orbitinj stuff).

This seems a bit inconsistent. However, Poisson's equation is certainly
-\lambda_D^2 \nabla^2(e\phi/Te) = (ni/ninf)-(ne/ninf) (for Z=1). So if
density is normalized to ninfinity, it makes sense. The reason for 
changing \phi with debyelen (for the same rhoinf) is that the effective
charge on each superparticle is \propto 1/debyelen^2. 

I think this confusion probably does not need fixing, but it probably
does need explaining in the RefManual. So wrote an introductory section.

Returning to the convergence problem, I find that 128,128 on a 128x128
domain converges spectacularly well. By contrast anything else 
e.g. 128x126 fails to converge with deltas +.0000198 permanently.
This very strange result makes one suspect rounding errors in the 
SOR calculation. But I can't find them by experimenting with different
operator orders etc. I also don't seem to be able to avoid them by
bumping the relaxation parameter in different directions, although that
does change the endpoint value of delta. These are with all periodic
potential. Problem is there but not so bad with fixed x potential. 
Removing makemedian zero does nothing. 

I think I see what is happening. There is a gradual upward creep of the
whole solution as evidenced by umax and umin in the iterations. That's
why the delta is always the same. It's really converged, but drifting
upward (irrelevantly) at a rate fast enough to fool the convergence.
That is presumably because of there being a slight imbalance in the
total charge. One might have to fix that by a better method. Needs to 
be thought about. The rounding error might be in the background, or
in rhoinf. 

I think I can confirm this by just changing -ri value by a miniscule amount
which causes the convergence to go bad. Seems as though there's a 
coincidence whereby the background cancellation is near perfect for
certain combinations of -ri and domain size. 128x128 -ri1 happens to
be one. A -ri change of 1e-5 is insufficient to break it but 2e-5 is enough.
There are 1.6M particles. 

Broadly speaking, a periodic solution cannot pay attention to any component
that changes the average potential. It ought to subtract it off. I've
tried to fix the problem outside of sormpi by makemedianzero. But that has
not solved the problem of convergence within sormpi. 

Constructed an additional parameter in sorrelaxgen adelta that is the sum
of all the deltas. Implemented mpi_reduce sum, so giving the total sum
of deltas over the entire grid. Divide it by the number of mesh nodes, to
give a (weighted) average increment. Adding it to the debug output, I see
that it is, within a percent or two, half of the delta returned by 
testifconverged. That's because the number of nodes advanced is actually
half of nodecount, because of red/black operations. If I subtract the
corrected adelta from delta, I get far lower residue. Lower than 10^{-6}
and the convergence is then fixed. Works also with 2 cpus.

15 Aug 18

Got wavedisplace going, using a compressive displacement to give obvious
effects in a uniform plasma. There are some puzzles about the -gp -gd
plots that first seem to show a flat density/potential and only the 
second time show the wave. I can't understand why, since wavedisplace
is called immediately after qinit. The weird thing is that with a hole
as well as the wave, the first plots show the hole but not the wave.
These are before any step has been reported. After the reporting of the
first step, the wave is immediately present. Maybe the problem is that
I have not reset the x_part mesh numbers as well as their positions. 
Yes. That was the problem. Fixed using partlocate.
Then generated a kinked hole that looks fine.

Now we need the input interface. Done
Demonstrated and oscillating kinked hole on my laptop. Period
180 steps of -dt.5 for psi=1, Ny=4 in 128x128 domain.

16 Aug 18

Discovered a problem that only happens with mpi (-n 2 or greater) that if
you run 128x128 iuds and ifull, the process hangs after one step. But if
you have 129x129 ifull it doesn't. Run with bounds checking. Got an error
in 3dobjects.f  
Index '0' of dimension 2 of array 'obj_geom' below lower bound of 1
That's because ngeomobj is 0. Fixed that.
Then got 
At line 335 of file mainroutines.f
Index '0' of dimension 1 of array 'scratch' below lower bound of 1
That was an error in offset indexing in the new makemedian zero. Fixed.
Then it seems the hang is gone, but returns after 3 steps without 
any bounds check errors. Not present without mpi. Changing the number
of particles changes the hang. It only happens with -ri exactly 1.
Doesn't happen without shear wave. But it is not a linmesh error in
wavedisplace. It never returns from sormpi. Problem seems to be that
one process thinks the interation is converged but the other doesn't.
Must be something I did in the communications of the testifconverged.
I think there is some inconsistency in the sign hack.

Got to fix the drift convergence problem more consistently. The issue
is that convgd(1) is the maximum absolute change over the whole mesh,
whereas adelta is the average signed delta. You can't be sure the local
delta (which is not reduced) has the effective sign of convgd(1). So 
the subtraction might be inconsistent from node to node. That's a no no.
Maybe the thing to do is to keep track of two deltas, plus and minus.
Another thought is to subtract adelta from delta before reducing it.
But that requires us to have the adelta for the whole mesh before we
do the cnvgd reduce, or do two reduces. Actually I alread do two since
one is max and the other sum. So I reorganize the mpi stuff to be 
lower level mpireducesuminplace etc.

However, on 256x128 we get getpotential errors and 128x128. So we are
not fixed yet.  This is not an mpi error. sormpi did not
converge. It's broken even in serial coptic made with gfortran.
Something has broken sor solution. There are no differences in sorrelaxgen
The differences in sormpi all appear to be in testifconverged. 
This is all irrelevant. The problem arises not from increasing the number
of mesh nodes but from increasing the x-domain size. That's a different
problem. Get a8b08456: has the same problem. Get 7db88fec: same problem.
That was after the Jacobi convergence radius changes. Seems as if this
problem is not new. Get 3e3d37 before. Still the same. This is a new
bug. It transpires the density is crazy if x dimension is larger than
around +-100. But only with qinit. Seems it can't cope with a length
that is greater than 102!! Found a problem with derivphiofx producing
a nan when the cosh argument is too large. Fix. 

Fix another bug which arose in wavedisplace when partlocate was added
because some arguments were arrays that weren't declared as such.

20 Aug 2018
Found and fixed an error in getmedian that meant it did not work 
correctly if iuds!=ifull.

23 Aug 2018

After fruitless fiddling with sormpi, it appears that making it converge
well enough with large domains to avoid a very slow-growing long wavelength
instability is impossible. We appear to be in the bit noise level without
really solving the problems. 

Hence implemented a new solver for 2D allperiodic cases fftsolve. 
It requires libfftw3 to be installed. But works amazingly fast, even
with just one process. It ought to check that the meshes are uniform
but that is not yet implemented. It appears to be working correctly.
Now we wait and see if it fixes the instabilities. Rather surprisingly 
it works even with the guard cells of dimension 3 not set correctly.
That must be fixed.

24 Aug 2018

Fixed the guard cell problem, but also switched to 3D version because
it was no slower and could be made more general.  Also implemented new
logical LNPF that defeats use of fftw and is set true by any face with
nonperiodic potential boundary condition. Implemented -bn switch to set
it regardless (and hence defeat fftw use). coptic main checks for any
dimension that has more than one meshstep, and if so also sets LNPF true.

Found a bug that getpotential goes wrong if a hole depth of -ih.02 is set
but not -ih.04. It happens with either phi-solver. It is an overflow of the
exp function in the phiofx routines. Fixed by limiting the argument
magnitude.

25 Aug 2018

Found that with the fft solver the initial hole constructed by qinit
experiences a sizeable oscillation in the first few steps that throws
off visible waves propagating outward. Prior to its implementation
it does not.

Compare with the sormpi solver by using the -bn switch. With -bn
switch the oscillation does not occur. Thus the oscillation appearance
is caused by the new fft solver. I suspect this is an issue of
interpolation. The fft solution may not be setting the finite-difference
expression of Poisson equivalently, (or the E-field correctly). If so
then it can probably be fixed, but needs some clear thinking.
Certainly halving the x-range with the same mesh number reduces the
initial oscillation. So that's consistent with there being an interpolation
difference.

28 Aug
It was necessary to divide the charge fft by 4sin^2(k/2) rather than k^2
to correctly solve the finite difference equation. But also there was a
bug in the indexing that took some time to track down.

7 Dec
Had a problem with the enhanced diagexamine, that first showed up on 
engaging, but could be made to show up on laptop. It gives errors of the
type "relocation truncated to fit". This turns out to be because of large
static arrays. Actually one cannot convince oneself that the program 
does not fit fine in 2G of memory from a simple count of the space required.
But apparently there is loads of overhead. By default all static allocated
memory and code must do so. But if you add -mcmodel=medium to the 
compiler flags. That allows the storage to exceed the 2G (not code). And 
that seems to stop it. It is said that this is an inefficient thing to
do because longer mov assembly code is involved. But hey. Problems
can also be made to occur with coptic code itself for 1024x1024x3 dimensions.
It is detected only at link time, but presumably needs to be compiled for.
However, just adding the compile flag in the analysis directory seems 
to be sufficient. 

6 Aug 2019

We want to develop a code algorithm that is almost periodic particles, except
that when a particle is relocated to the opposite end of the domain its
velocity on relocation is changed to be a random draw from the background
(outside the domain). The idea is that for a drift orbit case in 2-D 
particles ought never to be moved off the transverse position they start
on. If they are, just by making the particles non-periodic in that direction,
then relocation in the transverse direction occurs with the result that
substantial charge imbalances along different fieldlines arise. We want to
keep the number of charges on a field line constant to avoid this effect,
and yet give substantial attenuation to the magnetized plasma waves that
are generated by hole instability, as if this were on an infinite domain
where the waves just propagate away from the hole, without wrapping around.

ipartperiod might be able to denote this sort of interface. However, there
is code that addresses its upper bits like
chargetomesh.f:         if(ipartperiod(id)/64-(ipartperiod(id)/128)*2.eq.1)then
chargetomesh.f:        if(ipartperiod(id)/128-(ipartperiod(id)/256)*2.eq.1)then
This seems to be intended for cases where the particles are periodic, but
the mesh ends at a node, not between nodes as is the norm for periodic BCs.
However, I can't find anywhere that these are set other than in reading the
copticgeom.dat file, and no examples of 98,... values greater than 4.
Looks as if the facility was never fully implemented or used.

It also looks untrustworthy since there are lots of tests that just
ask if ipartperiod is 4. To work around this:
Changed all ipartperiod.eq.4 to ipartperiod.eq.4.or.ipartperiod.eq.5
So now 4 or 5 ought to work the same. A 2D hole test shows no problems
changing 4 to 5.

Now we can implement a reset of velocity in the particle relocation
code when ipartperiod is 5 (in principle). This is within padvnc.f
partlocate. Before doing that git commit.

Selecting velocity perp to face is done by
      call ranlux(ra,1)
      ra=ra*ncrein
      ir=int(ra)
      fr=ra-ir
      xr(mdims+idrein)=hreins(ir,index,ispecies)*(1-fr)+hreins(ir+1
     $     ,index,ispecies)*fr
where index runs from 1 to 6, and idrein=(index+1)/2 and index is 
documented as +-x,+-y,+-z. This refers to position end/start since
      sg=-(2*mod(index,2)-1)*0.999999
1 gives sg- 2 gives sg+, and position is start for sg+ end for sg-,
1 is end, 2 is start of mesh, (requiring v- and v+ respectively).

We need to initialize hrein. It seems to be done already (when
ipartperiod is printed). So don't bother with additional tests.
Need to pass reinspecies from padvnc to partlocate via creincom.f
Seems to be working.

8 Aug 19
Got this working although with large core count it gave a slight upward
potential curve at the boundary. Not yet understood. However, it failed
to get rid of the streaks. There is not much difference obvious between 
psi.8/B20x512y238 and psi.8/B20x512y238vreset. They both have substantial
growth of instability by 6000 steps, with clear streaks. Hence there's not
so much incentive to thoroughly debug the ipartperiod 5 code.

14 Aug 19
Upgrading phasespace.f analysis tool. Added code to find the maxlog of the
energy occurrence histogram and to subtract it from the energy range.
This has the effect of centering the peak at zero energy, which for a 
hole is what you want. The scaled version of the distribution (which 
must be divided by the v-width of the bins still has a hole at zero
even when fixed; so I don't understand it. The problem is that the 
x-range is too restricted, and cuts off the wings of the hole. Increasing
the range helps a lot. However, I recall now that there's an issue with
the scaling in that obviously if you have a very long x-domain then
the untrapped histogram is going to be out of proportion to the 
trapped. There must be a trapped/untrapped ratio that depends on the
ratio of hole length to domain length. I don't have any compensation
for that.

The phase-space area is dzdv \to d\tau dW, where tau is the time during
the orbit (see JPP2019). Therefore the total phase space area attributable
to an energy W is T(W)dW, where T(W) is the period of the trapped orbit, 
or the transit time of an untrapped orbit. For a sech4 hole the trapped
orbit period is 2\pi/sqrt(-W/2) (at sepx) and the integral over a W
interval is T(W)dW = 2\pi\sqrt(2) 2[sqrt(-W_2)-sqrt(-W_1)]. For untrapped
particles, to lowest order the transit time is T=L/v = L/sqrt(2W) and so
T(W)dW=L[sqrt(2W_2)-sqrt(2W_1)]. Thus the ratio of trapped to untrapped
phase space is 4\pi/L. This seems to give an unequivocal way of
normalizing. [Although the bounce period is sqrt(2) different for deeply
trapped particles.]

Discovered a source of distortion in the phasespace plots, arising from 
finite dt. The velocity and position values are staggered. Therefore we
need to subtract half-dt times the velocity from the position. Fixed.
Probably one ought to do the same correction to the energy histogramming.
Tried using partlocate to set the upper mesh position but did not work.
So put the mesh position calculation right into phasespace.

Also changed default domain to be the whole of the mesh, and because
of the Leapfrog correction, shaved off the ends of mesh along iaxis.

Actually the phasespace.f in src/ needs the same leapfrog correction.
Done, wrapping position as if periodic.

Realized that I should be using idtp from each particle for this correction
instead of dt. Adjusted.

That just about does it for now.

20 Aug 19
Found while trying to verify the initial distribution that there is a
bug in partexamine so that the negative x end of a limited extent box
has seemingly ~twice the distribution level it should.
21 Aug 19
Actually it is only twice the distribution if the total length restriction
cuts off one combined bin's worth of distance. When the restriction is 
less, the amplification is less. And if there's no restriction, no
amplification. Found the error in subaccum bin limit codes. Corrected.

Now I still have the phasespace area puzzle that is a sqrt(2) factor off

27 Aug 19
Replaced the sqrt(2) hack with better justified trapped particle period
including finite W corrections, and also correction to untrapped effective
box length allowing for hole influence. 

Also made phasespace work with multiply-specified files on the command line
(as well as extension-free specification which reads all.)

4 Nov 19

Begin work on ffttrid a version that uses fft only in the tranverse directions 
and uses tridiagonal in the first (parallel) direction. This is to enable
non-periodic BCs in the parallel direction. Which is intended to test what
happens if we use outward propagating boundary conditions instead of 
periodic. Also, it is hoped that this will enable use of shorter parallel
domains without compromising the stability criterion by k-parallel limitations.

First we are going to have to use cscratch differently. So increase its 
allocated dimensions in coptic.f to ifull, with the objective of providing
additional surface 2-D space for scratch storage. There's a headache to 
do with the test subroutine in that it is not consistent with this change.

Worked to sort things out and got the ffttrid to give the same result as
the 3D FFT version when using a correctly aligned sine wave and a BC
corresponding to zero at the domain ends (i=1/2, i=M+1/2). Their maximum
fractional error for mesh length 100 is 2e-6. And their maximum values
are 0.149439E-02 cf 1/k**2 0.149002E-02 which is pretty good, but not
brilliant. They are as good as each other.

6 Nov 2019

I need to diagnose the causes of the potential peak occurring at the
boundary when the particles have v reset on periodic exchange. I
suspect that it is not a bug but a physical effect of the reset. It
means that if an electron is trapped in a local potential peak, then
each time it crosses the periodic boundary, its velocity is reset and
it has a finite chance of being given energy sufficient to escape
trapping. If it does, then it escapes out of the local region (without
encountering the boundary again). A passing particle entering the well
also has a finite chance of becoming trapped.  However, if it does,
then it immediately returns to the boundary and has a chance of being
untrapped. I think that this balance of effects is such that there is
a bias that tends to deplete trapped particles but not passing
particles in the boundary vicinity.

A hand calculation indicates that the above suspicion does not work.
A positive potential ought to enhance total electron density by a
Boltzmann factor even in this vresetting case and cancel itself.
Therefore we must look for some other explanation. 

Set up test case in scratchvreset. It shows the problem quickly
(in 1d). Fiddled around in that with diagnostics in partlocate.
Noticed that the positions of relocated particles for moderate
length domains (-50 to 50) can be substantially beyond the xmesh
boundaries... by more than one cell when there are large numbers
of mesh counts. j

It strikes me that when a particle is relocated significantly inside
the mesh but with a different velocity, there's an inconsistency in the
way that the boundary crossing has been handled. The velocity ought 
to have changed at exactly the crossing, but then it would not have gone
the same distance into the opposite domain. Maybe that's an effect?

7 Nov 2019

It seems clear that the current vreset process will produce a
non-Maxwellian distribution close to the boundary because a flux
source velocity distribution is being used but being applied in a way
that is distributed in space over approximately one space-step,
inconsistently. We know that a flux distribution reinjected a distance
that is a random fraction of the timestep*velocity inside the domain
gives a correct Maxwellian distribution. But the velocity being used
in the current code is the old velocity not the new velocity. We
presumably could use the new velocity if we wished, but that would
locate the particle in a different position. We would then be using a
full scale reinjection (in the normal direction on a specified face at
least). It still does not seem sensible to use the whole reinjection
routine for this, because we don't need all the tangential position
and velocity mechanisms.  But if we use partlocate, it needs to know
what dt(pos) is. Alternatively we could multiply the distance into the
domain by the ratio vnew/vold, which might be easier to implement. 

Working on this. Clean up the linmesh test to a more rational implementation.
Remove the fist fisz variables and tests. Separating ipartperiod 4 and 5.
Not yet completed start at removal of the domain length shift tiny
adjustment removal. 

8 Nov 2019

Worked extensively on partlocate 5 code. Found that correcting the position
for the velocity difference using vnew/vold gets rid of the potential blip
at the boundary. So it seems the diagnosis above of the problem was correct.
Cleaned up the partlocate code and reverified. Seems now to be all in order
and with the vreset working. 

11 Nov 2019

Now getting back to ffttrid version implementation into full coptic
code (not just a ffttest version). Changed the makefile to put into
libcoptic ffttrid.o by default instead of fftsolve.f (but keep fftsolve.f
around for now as the old version). 

Try some 1d tests with ffttrid version 5,4,4. These don't seem to be 
solving for phi consistently with the fftsolve version. Hole height
rises and falls. This has BC potential=0. Perhaps the guard potential
values need to be set differently? Set guards doesn't fix. 

There are also some particle outside meshlen errors. Got myself into a
mess with trying to fix this. Might have to backtrack to older version 
of padvnc.f. Did so and then did smaller correction to get rid of outside
meshlen errors.

We have made little progress beyond getting rid of outside meshlen errors.
the ffttrid version is still not behaving as expected. 

Ahh! Seems to be because I did not copy back the guard cells of
u. Reprogramming seems to fix the hole growth/decay. Without the guard
cells set correctly the periodic boundary values of acceleration are
incorrect. When set things work.

13 Nov 2019

Went through debugging process that seemed eventually to get a working
outward propagation scheme going. There are long-scale oscillations
with the unmodified scheme on a 100x100 grid which eventually
overwhelm the hole. They occur with without vreset, but at a lower
level than with vreset. They can be stabilized by a hack of enhancing
the D(1) an D(M) by about a factor 1.1. Seems to indicate that the 
original scheme is only marginally stable. It is the long-scale
(low k^2) modes that are problematic, which corresponds to large 
G=C dt/dx = (1/k) dt/dx. On a 100 width grid, k minimum is 2pi/yL=0.06.
Which makes G=0.5/0.06=10. Then (1-G)/(1+G)=-9/11. A different way to 
stabilize (which also works) is to limit G=min(5,dt/dx/k). 

14 Nov 2019

Found other programming errors. When corrected, a growing long mode occurs
almost linear in x but sinusoidal in y, occurs. It appears to be driven
by the domain corners. It arises from the B-adjustment.

19 Nov 2019

Implementing a more systematic ffttrid using for non-periodic x boundaries.
When particles and potential are not periodic in x, we currently call the
SOR solver. But the idea is that instead we call ffttrid solver. 
We currently decide to call fft solvers only if LNPF is false, indicating
there are no non-periodic faces. That will have to change. LNPF is set true
either by the -bn switch, or by any non-periodic potential face LPF(i).
It would currently make sense to limit that to non-x cases. Doing that 
to make the fft called, but it is broken, because not using ffttrid.
Change to test LPF(1) to decide whether to call ffttrid or 3D periodic. 
ffttrid call still corrupts, but I think that's because of incompatibilities
with the full-mesh boundary. With fully periodic, it works.

The problem with ffttrid presents as a field corruption in getfield. 
Boxinterp zero weight error. This seems to be happening when an interpolation
is sought with xff(1)=100. Particle is at 50.5 exactly on the upper
x boundary. Inregion is T. That should not be the case. Found that a
modification to the linmesh test had broken it. Fixed.

I now seem to have a completely non-periodic-in-x version with phi=0 at 
boundary working with ffttrid. It does have some potential stripes in the
transverse direction, which I suppose are signs of total density fluctuation
along a field line. But they are barely detectable cf the overall noise level.
Actually one can see them in the averaged diagnostic with -a20 but not in
the step by step potential solve.

The shear wave initialization is overruled by this non-periodic setting.
A nuisance. Fixed that using the mechanisms in wavedisplace. Seems fine.

Now we have a version that is open wrt particles and specifies a logarithmic
derivative match to external k2 decay, limited to a maximum inverse value.
That last point is to restrict the very long transverse length cases not to
acquire large fluctuations from the boundary condition and the total 
charge variation. Current upper limit on -phi/phi'dx is 5. That's sort of
like setting the potential to zero 5dx outside the grid. It is clear that
we must not let r go to infinity because that would amount to setting
phi' to zero, which is impossible (at both ends) if the charge is nonzero
(and k2 is effectively zero). 

Now ipartperiod=5 is obsolete and I'll remove it. Done. 

I probably ought to rewrite ffttrid so that any one of the dimensions can
be taken as the non-periodic one. But that's significant drudgery. Also 
ought to use the phi boundary conditions to set ri. 

Did the general non-periodic dimension drudgery. Not too bad. 
Set the maximum ri to be BF/AF of inp. Ok.

21 Nov 2019

I discover there's a problem with nf_step and nf_maxsteps and
restarting.  The step at which we are restarting is determined from
reading the flux file, where it is stored. However, it is stored as a
value that is limited by nf_maxsteps. Therefore, once we have done
10000=nf_maxsteps-1 it never gets incremented again, so succeeding
restarts all think they are starting at 10000 (and write over prior
diagnostic files).  We need a different way of deciding where/whether
to write flux step data from that determining the step number. nstep
determines the step number. If it is set correctly (not from limited
nf_step), I think that will fix it. Trouble is, nstep is not in
common. That's probably why I used nf_step. So now make writefluxfile
add the nstep value on the end of the file, using an additional
argument, and readfluxfile return the nstep value in the ierr
argument.  restartread already has nstep as an argument (passing
nf_step and setting nstep=nf_step before any adjustment to
nf_step). Seems to work.

25 Nov 2019

Trying to run 3D cases with ffttrid. It is broken when there are more
than 3 points in the z-direction. Also when I swap y/z for 2D cases.
3D fftphisolve seems ok. There's a bug. So attempting to get the 
ffttest code going on ffttrid. Not yet achieved.

26 Nov 2019

Working more on ffttrid. Adjusted ffttest. Realized the BF/AF alteration
was a mistake. 
I think I understand my mistake. I gave chscratch dimensions iuds 
(strictly 0:iuds-1) then populated 1:iuds-2, omitting the guard cells.
But then I told dfft that its dimensions are iuds-2. Then it considers
it to be a full matrix, but it isn't, and indexes it wrong
when the second dimension is >1. I did that because I wanted to use
some of the cscratch for saving the guard cells. But I don't think it
works. Began to fix this, and yes my diagnosis is right the indexing
was broken. 

Basically fixed except the corner nonperiodic points are left zero. 
Now runs 3D with nonperiodic direction x. Does not run with other 
nonperiodic direction because of cmdline test. Remove that and it
does run with any one direction non-periodic and detected through 
ipartperiod. However, at the moment the hole is with x being the
trapping direction. 

7 Jan 2020

Trying to develop a more self-consistent initialization of a hole with
limited holerad. Supposing a separable form of solution is possible
(following Chen and Parks) one can in principle adjust the particle
placement and BGK solution to allow for transverse k^2. Began to
implement starting with findxofran. Got a version that can accommodate
the k^2 in the parallel placement. But then realized that uniform
perpendicular placement can't really be correct for this separable
form. So without a substantial reconsideration of transverse placement
there probably is no way of getting a really self-consistent
solution. For a pure drift case, a separable initial state has
x-integrated density that is non-uniform in the transverse direction. 

8 Jan 2020 

Yesterday I changed my view of the way to approach the extra term in
the BGK solution. It is not necessary to regard k^2 as a uniform
constant.  Instead take it to represent a different value depending on
the radial position r2 since it represents the perpendicular electric
field divergence -k^2\phi=(1/r)d/dr(rd\phi/dr). For the Gaussian
shape, then k^2=4/holerad^2*(1-r2/holerad^2) which changes sign of
course at r2=holerad^2. Implementing this gave a result with very
little oscillation when B is large enough for drift motion to apply, 
although there remains an oscillation for lower B. In hindsight the
assumption that k^2 is a constant is completely spurious, since we
are prescribing the shape of the potential in both directions, and 
should be free to prescribe whatever we like for these separable shapes
provided we couple them to account for the total divergence. 

The oscillation produced by finite larmor radius is a separate
problem.  It appears to be an inconsistency in the transverse motion,
and quite possibly in the transverse velocity. One idea is that it
arises because of just choosing the v_perp from a Maxwellian. Because 
that ignores the ExB drift from the perpendicular localization. It
is perfectly possible to add that drift to the random perpendicular 
velocity, which would be more consistent. Even more consistent perhaps
would be to add the ExB drift corresponding to the gyrocenter position?

Start by moving the transverse velocity setting _after_ the parallel
position setting. This allows us to know E. The problem is that previously
I had moved it before transverse position setting so that the gyrocenter
rhoic could be used in the calculation of r2. These two ideas need to be
rationalized. I think that it is probably better to calculate the 
psiradfac based on the actual particle position (rather than on the 
gyrocenter position as currently done). That seems to have the best
chance of giving consistent initial charge distribution. But then to 
choose the particle transverse velocity adding the drift (possibly at gyro
center) to the transverse velocity. 

Having implemented the ExB drift (as well as rationalized the code a lot)
I find that the highest initial peak is obtained without a gyroradius
correction to the potential position. But the ExB drift does not seem
to do much, whether positive or negative, to the oscillation. 

Playing around with trying to understand the effects, I try lowered Tperp
to see if that helps. However, I run into the fact that cartreinject does
not work correctly for unequal T and Tperp. The transverse injection velocity 
(parallel to the x-face) is given by Ts, not Tperps as it should. This 
distribution is set by
  call cumprob(fvdrein,xw,xc,
     $           ncrein,preins(0,id,ispec),gdummy,myid)
which puts the distribution selection array into preins, based on the 
function fvdrein. fvdrein ignores Tperps and uses Ts universally. 
Fixed that. 

Then it appears that the initial drift velocity is incorrect in that
v2 has antisymmetric variation along the 2-axis and v3 along 3-axis.
That's not right, it should be v2 along 3-axis etc. Turning off the 
ExB correction makes no difference. It is localized to the hole region
and is compressive (moving inward), but present even for the first step
diagnostics. 

13 Jan 2020

Pursuing the drift problem I try to implement a diagsum file write 
prior to the first step. However, I get a strange result with the 
velocity exactly zero for all cells. I fail so far to figure out why.
Ahh! I realize it must be the double velocity setting!!!

14 Jan 2020
Correcting the double velocity setting: (i.e. disabling it when there is
non-zero holerad) seems to reduce the initial hole oscillations, but not
to zero. There are two effects: First the very first step takes the 
transverse velocity profile and rotates it a bit. Then second, as time goes
on, the velocity profile structure rotates back toward toward the opposite
tilt angle. 

I find that the first-step rotation disappears if one uses sthacc for
the velocity rotation in moveparticle. This was an unresolved problem
from 27 Apr 15 where for B oblique to a reinjection face it was found
that using the stheta (etc) gave a strange layer at the edge. However,
the case I am pursuing does not have that problem because the particles 
are periodic transverse to B. I never really sorted out the problem 
anyway. And it ought not to matter as long as the timestep length is
not changing. (Maybe it does change for reinjection). 

The second: rotation backwards still occurs.

15 Jan 2020

I do not now think the rotation just described is a bug. Instead I think
it is an effect of the finite ratio of cyclotron frequency to plasma 
frequency. It does not significantly disappear when the perpendicular
temperature is made very low (e.g. 0.001). Instead what happens is that
the electron perpendicular distribution function becomes significantly
distorted from Maxwellian, having tail(s) that I believe are generated 
by the short time-duration of passage through the hole. During that
passage, which can be short compared with the cyclotron period, there
is perpendicular acceleration generated by the E-field, but the velocity
has not enough time to be rotated by the B-field. Consequently there
is an enhancement  \Delta v_2\propto -x_2, which is the reason for the
rotation of the velocity contours. 

This is not really a polarization (drift) because it is nearer to being
a transverse impulse on a time short c.f. 1/Omegac.

Increasing the B-field, Omegac/omegap, decreases the contour rotation
effect, but decreasing \rho_c (by lowering Tperp) does not. That's
consistent with the impulse idea.  Thus my previously associating it
with finite Larmor radius is a mistake.

There remains a significant breathing oscillation right from the start.
I suspect it is caused by the proximity of the hole edge to the transverse
boundaries, which make the initial state not self-consistent. However a
R15 hole which hardly reaches the boundaries still has this oscillation, so
that's evidence against. 

17 June 2020

Planning for particle initialization based on f-specification rather than
phi-specification, for use with 2D hole studies. Instead of being given
phi and solving BGKint for f0, we suppose f0 to be specified and solve for
phi. Based on tests with this differential method solution, we need to use
non-uniform phi-mesh for the phi(x) solution. So it is no longer true that
the phi grid in qinit can be considered uniform. 

The main routines that need to be changed in qinit are

findxofran, or more specifically its subroutines:
derivphiofx
intphiofx

phiofx

findxofphi, is currently based on bisection and function calls.  Might
be better if rewritten, but if we construct a new function call phiofx
could stay as it is.

f0construct is either obsolete (if f0 is already constructed) or totally
different being constructed from inputs. 

GetDistribAtPhi needs to be checked but looks ok. Does a trapezoidal
integration, which does not assume uniform u-spacing, although u is
actually uniformly spaced, and that does not need to be changed.

There are a few places where it has been assumed that the phi mesh is 
uniform or that the transverse shape is Gaussian. A good place to start
might be to fix those. First we need a test case. It appears that
testing/f0constructtest.f has some of what I want.

At the moment, phi(x) is scaled by a transverse variation factor. 
      phi=psiradfac*phiofx(x_part(id,islot),psi,coshlen,holetoplen)
However, that is not really correct because the transverse divergence
factor changes the x-scale-length, making the assumption of
separability false.  I don't know quite what to do about that. For now
it probably should be ignored, since in principle the f0(x) itself
could change transversely.  But I do need to know how to scale the
delta-f relative to flat-top. Based on the scaling developed in 2015 paper
\tilde f0(0) \propto \Delta v \propto v_s. So \tilde f0 \propto sqrt(psi).
This is all consistent with the multistep hole scaling. 

We might also adjust the phiofx x-scale with transverse position
eventually?

Working to get f0constructtest going. Initially it segfaults. That turns
out to be because of missing argument in new version of f0construct.
Added to f0constructtest and that fixes. 
Change the xmax not to use findxofphi(psi/nphi...
For now just use 12. Which puts it out of the way. 

f0construct itself calls BGKint to put the trapped distribution into f0.
We need different code for that, based on the ftrapped form. du is 
uniform. The untrapped part of f0 is added on with +- an extra nphi.
BGKint has some assumptions of uniform phi array. But since we are 
getting rid of it, we don't really need to fix those.

23 Nov 2020

I want to be able to find the response of the plasma to a fixed localized
charge that is simply specified. Is this the same as the existing point
charge PPPM option? Not quite. I don't want the PIC particles to encounter
too extreme a local electric field, because my main interest is not the
very local behavior near it, but the eventual screening form at distances
of a few debye lengths or greater. The purpose is to show that the gyrokinetic 
anisotropic shielding is a chimera. 

It would be possible to create a subroutine that simply added an extra
charge via achargetomesh, to the mesh locally. The resulting field would
be limited automatically by the size of the grid, since the field locally
is NOT then calculated by the PPPM process (and thereby allowed to become
very large close to the charge). But that requires some additional 
infrastructure for input. 

It is not clear to me whether allowing the PPPM processes to work as
normal creates a problem or not. There are checks in the PPPM code to
limit the size of the applied electric field. One is in getadfield,
where the squared distance from the point charge is not permitted to
be smaller than 1.e-12 (hard coded). This mostly seems to be to
prevent NANs. Another is in padvnc that if the E-field times the
timestep (f1*dtaccel) (an impulse) exceeds dropaccel (-dd, default 10)
at any step the particle is dropped from the simulation (as if
absorbed). Is that sufficient? Its main purpose is to prevent a particle
from gaining a monstrous unphysical momentum by stepping close to the 
PPPM point charge. If one does, then don't shoot it across the domain,
because that's obviously unphysical, just drop it as the least perturbative
option. In effect that models a finite sized object absorbing particles
that come too close. 

The point-charge object charge magnitude is specified as the potential
pp that would be present at its specified radius rp (the end of the
PPPM domain) without shielding. At rp the unshielded field is
pp/rp. The dropaccel is exceeded when dtc*f1 > drop. Which occurs
where (rp/rd)^2*(pp/rp)= pp*rp/rd^2 > drop/dtc. Thus for example if
pp=rp=1 and dtc=1, then rd=sqrt(1/drop) = sqrt(1/10) ~ 1/3. Note that
typical space particle step would be dpx ~ v.dt ~ v ~ 1. We would
probably want to use smaller dt and hence dpx. (And we would have to, in
order to manage the cyclotron orbit.) 

Subcycling by a dt adjustment up to a factor of 8 (or 4) is allowed to 
help the above problem. So presumably it would make the factor sqrt(1/40).
However, I have not used subcycling for ages and I don't trust that it 
is really correct at the moment. So I'll avoid. 

I think that bottom line is that I can use the existing pointcharge code.
If I make sure the point position does not coincide with a mesh point 
that will limit the mesh potential to being not too large. I probably
need mesh spacing somewhat smaller than debyelen. In any case the 
non-linear region where phi.gt.1 anyway does not follow yukawa scaling. 
I therefore propose a PPPM region of radius ~ 2 and magnitude ~ 0.2,
on a mesh of spacing ~ 0.5, and extent roughly +-10. (40^3)
I can play with dropaccel to see if it makes a difference. 
In prior pointcharge runs I have tended to use non-uniform mesh, but 
I'm not sure that's a good choice for this. 

Exploring these possibilities I get strange results with point charges
that seem to be too narrow. I wonder if there's some additional shielding
coming from boltzamp to the pointcharge, but I have trouble finding out. 

I can use a sphere, and once I figure out some puzzles about the fftw
solver (which can't apply to sphere, since it needs the intercepts).
It gives sort of sensible results except that I don't know if I am
removing particles even if I don't set flux accumulation. Checking I 
see that linregion always returns true if ibool_part=0, which it is.
So no I am not removing particles if ibool=0, which it is if there is
no boolean 99 line in the objects file or if it is 99,0. It seems this
is the easiest choice to use. 

One cannot use periodic potential conditions for all boundaries. All zero
works fine, or all logarithmic derivative =-1, even better. This seems
basically to work. And (of course) gives pretty much isotropic result,
down to the noise level. Maybe we need to run some on a substantial 
number of processors. The laptop is beginning to struggle. If anything
the potential is elongated in the x-direction i.e. parallel. Also, 
even an elliptical boundary rapidly circularizes (as expected).

29 Apr 2021

Exploring the idea of using two ion species to represent a two beam
distribution, in the context of an electron hole. Currently nspeciesmax
is 2, but looks as if it could be increased to 3. Initialization is the
main issue to begin with.

qinit does not appear to be correctly set up for more than one species
with an electron hole present. It does not seem to deal differently with
each species. This might be the main immediate problem, and probably 
should be dealt with for 2 species before moving on to 3. Normally 
ions are species 1 and electrons species 2. Other choices are probably
possible. I don't think COPTIC has been used for 2-species hole simulations.

It seems possible that we could use the ions as species 2 (and 3) rather
than 1 (and 2). In that case the electron timescale would be the defining
normalization. But for a classic (fast) electron hole the ions would have
a speed relative to the hole that is some fraction of the electron thermal
speed. 

A periodic domain is possible, but only if the ion (reflected)
distribution is symmetric in the rest frame of the hole, because the
reflection is effectively ion trapping in a periodic domain. It might
be possible to have an asymmetric passing ion population, but the
reflected population would eventually symmetrize itself. Perhaps that
is part of the requirement to be in the f-minimum so that the
reflected population can readily be symmetrized. The noise difficulty with
small amplitude holes in a PIC code is going to be a bit of a headache for
the small amplitude limit, and slow holes.

30 Apr 2021

Turn off Reset normal position unless ispecies.eq.hspecies, this seems
to allow electron hole in initially uniform ion background distribution.
And the hole then accelerates. Got a 2-species case with electrons
first then ions going, after several corrections about species.
Self acceleration occurs. Adding a significant drift to ions reduces
it considerably.

1 May 2021
Trying to use -ni with periodic particles so that I can put half of the
ions into one species and half in another. There's a hack that prevents
this. Bypassing the hack gives Corrupt gradlocalregion errors. Actually
they are Corrupt shortcuts, which are hard to diagnose. 

It's a bit of a mystery why this happens. cij(1+icl*iinc) is zero.
u(1+iinc) is NAN. ixn0 and iinc are (e.g) 2  121857 which is
the third element of ils 121856 plus 1.
xff=   1.58384323       1.96075141       2.42574096 always.

Tracked the problem down eventually to rhoinfcalc making rhoinf zero
and producing a NAN in the q. Changed logic to avoid that.

2 May 2021

Increasing the nspecies to 3 and specifying two ion species with half
the numbers of electrons, v=+-.1, gives a hole that does not immediately
accelerate. However, it does begin to oscillate. As the oscillation grows,
it sheds some trapped electrons. The oscillation displacement still
grows, but not much the velocity excursion. Still that is ~+-.5, much
larger than the ion beam difference. So the hole's velocity does not
remain trapped between the ions' velocities. Eventually, as the ion
fluctuations are spread out, and the hole amplitude drops from initially
0.8 to ~0.4, the hole acquires a velocity in one direction and traverses
the domain in one direction thereafter at speed ~0.5, after about 3000
steps (1500/omegap). Stopped at 4000 steps. 

It looks as if the advancing code is now correct. However, the initial
state of ions is not, and that might be part of the reason for the 
oscillation that leads the hole to fast speeds. 

The ion and electron temperatures are equal, so the ion acoustic speed
is 1/43 times the electron thermal speed. Thus v=-+.1 vte is 4.3 cs. 

Ion initialization seems to be problematic. The ion density is uniform,
but it should not be. There ought to be an ion density increase at the
hole because of ion slowing. That would then suppress inital hole motion. 
Instead, the ions initialized with unperturbed velocity but uniform
density leave the hole with enhanced velocity and this seems to launch
a negative ion fluctuation outward in both directions. Perhaps this 
is a source of greater oscillatory instability. (Overstability.)

Second test with v=+-.02 shows immediate splitting of hole into 2. Each
heads off in the opposite direction, but once it escapes the initial
negative ion density region different things happen. One of the holes
stops and reverses direction following the other. They both move along
at maybe vh=.2, which is higher than either ion stream. Note that the
potential drop (initially 0.8) is enough to accelerate a zero velocity
ion to v=sqrt(2\psi/m) ~= 1.3 c_s. Also v=0.02 is 0.86 c_s. So the 
ions initialized in the hole are very strongly accelerated. Maybe I
should use Ti>>T_e, to reduce this effect. It seems as if the left hand 
hole tries to catch up with the other, but then an ion fluctuation
pushes it back. After 3000 steps still they have not merged.

Instead with v=+-.1 and -t10 much hotter ions: the hole remains at zero
velocity/position till about step 300 (t=150), then heads off in one
direction. There is no oscillation, and the ion perturbation appears
to be concentrated in the near wake of the hole. I'm not clear whether
losing velocity trapping was a noise effect, or genuine instability. 
Trying with mpiexec -n2 to decrease noise. If anything the escape is
quicker. With -t5 instead the oscillatory instability occurs but takes
quite a long time to get going. Then escapes at about step 800.

4 May 2021
On ihutchws using 8 processes. v=+-1. -t5. A couple of oscillations
then lost from trapping by step 700. 

A big part of the startup problem is that it produces a negative ion
density perturbation moving in the direction of the drift velocity
(opposite for two-stream). It takes quite a while for that to be
overcome by the positive density perturbation of the streams that
is expected in steady state. If the hole is sufficiently perturbed
to leave the velocity trapping region before that, it becomes fast.
The amplitude of the oscillatory instability is sufficient to move
the hole out of the ion f-minimum. v+-.1 -t5 with -ih.4 (half amplitude)
still escapes at about step 800. v.05, -t5 -ih.4 escapes in negative
direction by about step 200. 

If we adjusted only the velocity of initialization in the hole so as
to be consistent with conserved energy, then instead of an initial
negative density, there would be (I think) an initial positive ion
density during startup. This would help avoid the initial loss of
the hole that defeats ion distributions with very small vb. It might
be a way to demonstrate sustained slow holes in a mildly double humped
ion distribution.

29 Jul 2021

Imported a new BGKintnew from the asymhole directory that has the facility
to call a function denionfun to determine the ion (density) response,
which by default is just unity. It appears to give the same result
for a test case hole as the old one (once some corrections are applied).

30 Jul 2021 Got denionfun working and BGKintnew.

31 Jul 2021 Change denionfun to have second argument isigma.

6 Aug 2021

Considerable progress on implementing MultiGaussian ion response and
initialization. During this coding I notice that in qinit.f the ExB
drift of a 2-D hole is added only to the electrons (hole species) and
not to the ions. I don't see why that is. Both species should drift
at the same rate. So I change "Add transverse ExB velocity" to apply 
to all species. 

Another issue I discover is that for multiple species holes there are
inconsistencies in the velocity scaling. tisq is decided per species
in initialization according to eoverm. But velocity was being set as
tisq*gasdev(myid)+vds(ispecies)*vdrifts(id,ispecies) 

That means the vds are all in the same units regardless of mass.  But
really ions are way heavier and it makes little sense to do what I
have been doing and giving two species velocity shifts of order
unity. With this convention I would have to specify far smaller
shifts. I change the setting therefore to
tisq*(gasdev(myid)+vds(ispecies)*vdrifts(id,ispecies))
so both velocity width and shift are in thermal units for the species
under consideration. 

A similar problem exists for the MultiGaussian treatment. 
Straightened that out using tisq multiplication. Now the MultiGaussian
ion treatment works fine. 

Issues exist with the phaseread and phasewrite. Only one species is
written to the pps file. It would be better to have the pns for all
species. That would enable postprocessing to do a plot similar to
what is done in real time. That needs editing src/phasespace.f in
several different routines.

7 Aug 2021
Rationalize phasepscont by always accumulating psaccum and psnaccum
for all species. Improve the plots to include all species' phase
spaces.

This shows there is a problem with the approach. The ion distribution
does not seem to behave as expected for energy conservation. My 
assumption is that this is because the ion temperature was used 
in contradiction with the multigaussian settings.

Need instead to use eoverms directly. This raises the question about
whether the eoverms of species 1 is what defines the
non-dimensionalization and does it need to be accounted for here?
Is it eoverms(ispecies)/eoverms(1)? I think not because coptic has
one reference temperature Tr, which defines the Debye length.
According to Refmanual, input specifications are normalized values
for potential velocity time T charge-to-mass ration and B-field, and
defining the Debye length is a free choice, generally unity. However
I seem to be breaking that convention by the multigaussian specification
because -fp<vshift>,<vthermal>,<n> regards the v's as being in ion
units, not reference units. Electrons are in reference units because their 
eoverms is -1. 

9 Aug 2021
Corrected the multgaussian units not to pay attention to -t except for
defining the velocity extent of the phasespace diagnostics. Then on
ihutchws ran some two-maxwellian cases -fp+-vs,1,.5. -ih.4 I find that 

vs=1.3 is unstable early to unidirectional growth.
vs=1.5 is unstable to oscillatory growth and escapes in one direction
fairly soon.
vs=1.7 is unstable to oscillations but not so severely and has not
escaped by s2000 (t=1000).

vs=2.0 shows no instability by s1000 t=500. It also shows the ion
phasespace distribution very steady, unlike the unstable cases which
show a diagonal shaped region of f-enhancement shearing with the 
progress of time. The steadiness is a confirmation that the initialization
is really consistent. By t=700 there is a discernable oscillation of
small amplitude. By t=850 its amplitude is position of peak x=+-2 
(period ~ t=80) and does not grow beyond ~2.5 by end of run t=1000.

Nevertheless, there is an unpredicted oscillatory instability near the
threshold of theoretical instability, that requires explanatation. 

vt2vs3 which scales ion velocity up by 2 from vt1vs1.5 escapes after
only about one oscillation period.

vt2vs4 shows again that increasing the ion velocity spread with the 
same relative shape as vt1vs2 does not stabilize, and actually leads
to earlier escape. 

p.2v1.7 shows little smaller potential does not make things significantly
more stable against oscillations.

v1.7dt.25 takes timesteps a factor of 2 smaller than vs1.7. It is
superficially very similar. Reaches an excursion of ~-9 at time 965
but has not escaped. That's slightly less excursion than the dt.5 case.
As before, the oscillations cause a big change in the ion phase space.

15 Sep 2022

Got errors with MPICH reduce calls because of calling the same routine
with different types of arguments as per the fortran interface
definitions, on ihutchws with ubuntu 22.04, which has gcc version
11.2.0 (Ubuntu 11.2.0-19ubuntu1).

After some struggling, I found that could be fixed by using a compile
flag -fallow-argument-mismatch. However, the 20.04 ubuntu has
gcc version 9.4.0, which has no such flag. AARRGGHH.
It appears that one only needs the makefile to be modified. But I
have not updated it in the distribution. Instead I add makefilefallow
that has the needed modification. 

25 Sep 2024

Restarting for more than 10000 steps seems to be broken. Why?
Well, the number of prior steps is obtained from the .flx file, 
which needs to be present. But the 3dcom.f file defines
 parameter nf_maxsteps=10001, and the dimensions of various things
like ff_rho, nf_npart, ff_dt, ff_address are determined by it. 

Flux reading and writing, for example of ff_rho and ff_dt
is (1,nf_step) so clearly one can't write a flux file with nf_step
greater than nf_maxsteps. 

However, nstep, the writefluxfile argument, is passed from the
datawrite with the actual step number, which could be greater
than nf_step (or maxsteps), and it is what is written to the
flx file, and read back into ierr. What prevents nf_step from 
becoming gt maxsteps? Seems to be in coptic.f main iteration:
         nf_step=min(nf_maxsteps-1,nf_step+1) 
However, I think that just prevents one from writing beyond the allocation
of ff_rho etc. 

It seems to be that there is a contrary ierr=0 at the end of
readfluxfile, which prevents it returning the prior step. Anyway if it
is commented out the step can exceed maxsteps, and repetitive restarts
continue to rachet up the pps output number. So I suspect that is
the problem. 

14 Oct 2024
Trying to sort out the annoyingly confusing -g. switches for phase space.
Currently:

In cmdline
         if(argument(1:3).eq.'-gn')ldistshow=.not.ldistshow
         if(argument(1:3).eq.'-gd'.or.argument(1:3).eq.'-gp')then
            lsliceplot=.true.
            read(argument(4:),*,err=211,end=211)ipstep
         if(argument(1:3).eq.'-gp')lphiplot=.not.lphiplot
         if(argument(1:3).eq.'-gx')then
            read(argument(4:),*,err=201,end=213)ipfset
            call pfset(ipfset)
            goto 240
 213        call pfset(-3)
         endif

In coptic:
   if ipstep or mod(j,ipstep)=0 then
       if lphiplot, then set ldistshow true.
   if ldistshow, then call phasepscont
   At iavesteps
   if .not.ldistshow, but idistp.ne.0 (and 1d) call phasepscont
           idistp 0 by default is set by switch -gp, to 1 or integer
   lsliceplot controls only sliceGweb, nothing in phasespace.

In mainroutines:
   ldistshow does not appear
   lphiplot does not appear
   In phasepscont:
      phasewrite is always called by process myid=nprocs-1
      if lplot (passed from lphiplot) then a plot is done. 

It seems the problem is trying to use pfset(-3) to turn off file
writing.  That's incorrect. It needs to be 0 to prevent file
writing. pfsw -3 (or negative) writes files but does no screen
drawing. In order to turn off display, one needs to call svganodisplay
which sets a C integer flag, but that happens only if pfsw.lt.0. These
two things seem to be incompatible, since in pltinit svga is called 
when pfsw is 0 (no displaying). If we want to do neither displaying
nor writing, what do we do? We need to turn off plotting entirely,
that seems to be lphiplot false. Since we need to use its toggle
switch to set the ipstep, maybe we just call (e.g.) -gp -gp2. 
Yes that seems right. 

15 Oct. No it does not seem to be right. -gp -gp2 by itself does not work
but  -gp -gp2 -gn does. Fix the help. 
